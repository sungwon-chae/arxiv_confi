#!/usr/bin/env python3
"""
Data Ingestion Pipeline - ê³ ë„í™”ëœ ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ
processors ê¸°ë°˜ì˜ í†µí•© íŒŒì´í”„ë¼ì¸ CLI
"""

import asyncio
import argparse
import logging
import sys
from pathlib import Path
from typing import List, Optional
import json

from pipeline_engine import DataIngestionPipeline, PipelineConfig

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

def setup_argument_parser():
    """CLI ì¸ìˆ˜ íŒŒì„œ ì„¤ì •"""
    parser = argparse.ArgumentParser(
        description="ê³ ë„í™”ëœ ë°ì´í„° ì¸ì œìŠ¤ì…˜ íŒŒì´í”„ë¼ì¸",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # CSV íŒŒì¼ ê³ ê¸‰ ì²˜ë¦¬
  python main.py process-csv data.csv --text-dir texts/ --build-kg
  
  # ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì²˜ë¦¬
  python main.py process-documents *.pdf --extract-tables --build-kg
  
  # ì§€ì‹ ê·¸ë˜í”„ë§Œ êµ¬ì¶•
  python main.py build-kg processed_data/ --output kg.json
  
  # ê¸°ì¡´ ë°©ì‹ í˜¸í™˜ (ê°„ë‹¨í•œ CSV ì²˜ë¦¬)
  python main.py csv-legacy data.csv --text-dir texts/
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ë“¤')
    
    # CSV ê³ ê¸‰ ì²˜ë¦¬
    csv_parser = subparsers.add_parser('process-csv', help='CSV íŒŒì¼ ê³ ê¸‰ ì²˜ë¦¬')
    csv_parser.add_argument('csv_path', help='CSV íŒŒì¼ ê²½ë¡œ')
    csv_parser.add_argument('--text-dir', help='ì™¸ë¶€ í…ìŠ¤íŠ¸ íŒŒì¼ ë””ë ‰í† ë¦¬')
    csv_parser.add_argument('--text-path-field', help='í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ í•„ë“œëª…')
    csv_parser.add_argument('--content-field', default='content', help='ì½˜í…ì¸  í•„ë“œëª…')
    csv_parser.add_argument('--class-name', help='Weaviate í´ë˜ìŠ¤ëª…')
    csv_parser.add_argument('--db-name', help='ë°ì´í„°ë² ì´ìŠ¤ëª…')
    csv_parser.add_argument('--vectorize', nargs='+', help='ë²¡í„°í™”í•  í•„ë“œë“¤')
    csv_parser.add_argument('--build-kg', action='store_true', help='ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶•')
    csv_parser.add_argument('--chunk-strategy', choices=['semantic', 'sentence', 'fixed'], 
                           default='semantic', help='ì²­í‚¹ ì „ëµ')
    csv_parser.add_argument('--output-dir', help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    
    # ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì²˜ë¦¬  
    docs_parser = subparsers.add_parser('process-documents', help='ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì²˜ë¦¬')
    docs_parser.add_argument('file_paths', nargs='+', help='ì²˜ë¦¬í•  ë¬¸ì„œ íŒŒì¼ë“¤')
    docs_parser.add_argument('--batch-size', type=int, default=10, help='ë°°ì¹˜ í¬ê¸°')
    docs_parser.add_argument('--no-images', action='store_true', help='ì´ë¯¸ì§€ ì¶”ì¶œ ë¹„í™œì„±í™”')
    docs_parser.add_argument('--no-tables', action='store_true', help='í…Œì´ë¸” ì¶”ì¶œ ë¹„í™œì„±í™”')
    docs_parser.add_argument('--build-kg', action='store_true', help='ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶•')
    docs_parser.add_argument('--db-name', help='ë°ì´í„°ë² ì´ìŠ¤ëª…')
    docs_parser.add_argument('--output-dir', help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    docs_parser.add_argument('--output-format', choices=['structured_json', 'markdown', 'elements'],
                           default='structured_json', help='ì¶œë ¥ í˜•ì‹')
    
    # ì§€ì‹ ê·¸ë˜í”„ ë…ë¦½ êµ¬ì¶•
    kg_parser = subparsers.add_parser('build-kg', help='ì§€ì‹ ê·¸ë˜í”„ ë…ë¦½ êµ¬ì¶•')
    kg_parser.add_argument('data_path', help='ì²˜ë¦¬ëœ ë°ì´í„° ê²½ë¡œ (ë””ë ‰í† ë¦¬ ë˜ëŠ” JSON íŒŒì¼)')
    kg_parser.add_argument('--output', required=True, help='ì§€ì‹ ê·¸ë˜í”„ ì¶œë ¥ íŒŒì¼')
    kg_parser.add_argument('--min-confidence', type=float, default=0.5, help='ìµœì†Œ ì—”í‹°í‹° ì‹ ë¢°ë„')
    kg_parser.add_argument('--similarity-threshold', type=float, default=0.8, help='ìœ ì‚¬ë„ ì„ê³„ê°’')
    
    # ê¸°ì¡´ ë°©ì‹ í˜¸í™˜ (legacy)
    legacy_parser = subparsers.add_parser('csv-legacy', help='ê¸°ì¡´ ë°©ì‹ CSV ì²˜ë¦¬ (í˜¸í™˜ì„±)')
    legacy_parser.add_argument('csv_path', help='CSV íŒŒì¼ ê²½ë¡œ')
    legacy_parser.add_argument('--text-dir', help='ì™¸ë¶€ í…ìŠ¤íŠ¸ íŒŒì¼ ë””ë ‰í† ë¦¬')
    legacy_parser.add_argument('--text-path-field', help='í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ í•„ë“œëª…')
    legacy_parser.add_argument('--class-name', help='Weaviate í´ë˜ìŠ¤ëª…')
    legacy_parser.add_argument('--db-name', help='ë°ì´í„°ë² ì´ìŠ¤ëª…')
    legacy_parser.add_argument('--vectorize', nargs='+', help='ë²¡í„°í™”í•  í•„ë“œë“¤')
    legacy_parser.add_argument('--no-verification', action='store_true', help='ì¸ì œìŠ¤ì…˜ ê²€ì¦ í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°')
    
    return parser

async def cmd_process_csv(args):
    """CSV ê³ ê¸‰ ì²˜ë¦¬ ëª…ë ¹"""
    logger.info(f"CSV ê³ ê¸‰ ì²˜ë¦¬ ì‹œì‘: {args.csv_path}")
    
    config = PipelineConfig(
        db_name=args.db_name,
        class_name=args.class_name,
        vectorize_fields=args.vectorize,
        chunk_strategy=args.chunk_strategy,
        build_knowledge_graph=args.build_kg,
        output_dir=Path(args.output_dir) if args.output_dir else None
    )
    
    pipeline = DataIngestionPipeline(config)
    
    try:
        result = await pipeline.process_csv_enhanced(
            csv_path=args.csv_path,
            text_dir=args.text_dir,
            text_path_field=args.text_path_field,
            content_field=args.content_field
        )
        
        if result["success"]:
            logger.info("âœ… CSV ì²˜ë¦¬ ì™„ë£Œ!")
            logger.info(f"ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤: {result['database_name']}")
            logger.info(f"ğŸ“„ ì´ ë¬¸ì„œ ìˆ˜: {result['total_documents']}")
            
            if result.get("knowledge_graph_stats"):
                kg_stats = result["knowledge_graph_stats"]
                logger.info(f"ğŸ§  ì§€ì‹ ê·¸ë˜í”„: {kg_stats['basic_stats']['total_entities']}ê°œ ì—”í‹°í‹°, "
                           f"{kg_stats['basic_stats']['total_relationships']}ê°œ ê´€ê³„")
        else:
            logger.error(f"âŒ CSV ì²˜ë¦¬ ì‹¤íŒ¨: {result.get('error')}")
            return 1
            
    finally:
        pipeline.close()
    
    return 0

async def cmd_process_documents(args):
    """ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì²˜ë¦¬ ëª…ë ¹"""
    # íŒŒì¼ ê²½ë¡œ í™•ì¥ (glob íŒ¨í„´ ì§€ì›)
    file_paths = []
    for pattern in args.file_paths:
        if '*' in pattern or '?' in pattern:
            from glob import glob
            file_paths.extend(glob(pattern))
        else:
            file_paths.append(pattern)
    
    logger.info(f"ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {len(file_paths)}ê°œ íŒŒì¼")
    
    config = PipelineConfig(
        db_name=args.db_name,
        extract_images=not args.no_images,
        extract_tables=not args.no_tables,
        build_knowledge_graph=args.build_kg,
        output_format=args.output_format,
        output_dir=Path(args.output_dir) if args.output_dir else None
    )
    
    pipeline = DataIngestionPipeline(config)
    
    try:
        result = await pipeline.process_documents_multimodal(
            file_paths=file_paths,
            batch_size=args.batch_size
        )
        
        if result["success"]:
            logger.info("âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ!")
            logger.info(f"ğŸ“Š ì„±ê³µ: {result['successful_files']}/{result['total_files']} íŒŒì¼")
            logger.info(f"ğŸ“„ ì´ ìš”ì†Œ: {result['processing_stats']['total_elements']}ê°œ")
            
            if result["failed_files"] > 0:
                logger.warning(f"âš ï¸ ì‹¤íŒ¨í•œ íŒŒì¼: {result['failed_files']}ê°œ")
            
            if result.get("knowledge_graph"):
                kg_stats = result["knowledge_graph"]["stats"]
                logger.info(f"ğŸ§  ì§€ì‹ ê·¸ë˜í”„: {kg_stats['basic_stats']['total_entities']}ê°œ ì—”í‹°í‹°")
        else:
            logger.error("âŒ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨")
            return 1
            
    finally:
        pipeline.close()
    
    return 0

async def cmd_build_kg(args):
    """ì§€ì‹ ê·¸ë˜í”„ ë…ë¦½ êµ¬ì¶• ëª…ë ¹"""
    logger.info(f"ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶• ì‹œì‘: {args.data_path}")
    
    # ë°ì´í„° ë¡œë“œ
    data_path = Path(args.data_path)
    processed_documents = []
    
    if data_path.is_file() and data_path.suffix == '.json':
        # JSON íŒŒì¼ì—ì„œ ë¡œë“œ
        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            if isinstance(data, list):
                processed_documents = data
            elif isinstance(data, dict) and 'processed_documents' in data:
                processed_documents = data['processed_documents']
    elif data_path.is_dir():
        # ë””ë ‰í† ë¦¬ì—ì„œ JSON íŒŒì¼ë“¤ ë¡œë“œ
        for json_file in data_path.glob('*.json'):
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                processed_documents.append(data)
    else:
        logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° í˜•ì‹: {data_path}")
        return 1
    
    if not processed_documents:
        logger.error("âŒ ì²˜ë¦¬í•  ë¬¸ì„œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        return 1
    
    config = PipelineConfig(
        min_entity_confidence=args.min_confidence,
        similarity_threshold=args.similarity_threshold,
        build_knowledge_graph=True
    )
    
    pipeline = DataIngestionPipeline(config)
    
    try:
        result = await pipeline.build_knowledge_graph_standalone(
            processed_documents=processed_documents,
            output_path=args.output
        )
        
        if result["success"]:
            logger.info("âœ… ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶• ì™„ë£Œ!")
            logger.info(f"ğŸ“ ì €ì¥ ê²½ë¡œ: {args.output}")
            
            stats = result["stats"]
            logger.info(f"ğŸ§  ì—”í‹°í‹°: {stats['basic_stats']['total_entities']}ê°œ")
            logger.info(f"ğŸ”— ê´€ê³„: {stats['basic_stats']['total_relationships']}ê°œ")
        else:
            logger.error(f"âŒ ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶• ì‹¤íŒ¨: {result.get('error')}")
            return 1
            
    finally:
        pipeline.close()
    
    return 0

async def cmd_csv_legacy(args):
    """ê¸°ì¡´ ë°©ì‹ CSV ì²˜ë¦¬ (í˜¸í™˜ì„±)"""
    logger.info(f"ê¸°ì¡´ ë°©ì‹ CSV ì²˜ë¦¬: {args.csv_path}")
    
    # ê¸°ì¡´ ìŠ¤í¬ë¦½íŠ¸ì˜ async í•¨ìˆ˜ë¥¼ ì§ì ‘ í˜¸ì¶œ
    try:
        from simple_manager import MultipleVectorDBManager
        from scripts.generate_schema import generate_schema_from_csv
        from scripts.setup_database import (
            create_database_schema, load_csv_data, test_database_ingestion,
            normalize_date_to_rfc3339
        )
        from pathlib import Path
        
        logger.info("ğŸš€ ê¸°ì¡´ ë°©ì‹ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘")
        
        manager = None
        
        try:
            # 1. VectorDB ë§¤ë‹ˆì € ì´ˆê¸°í™”
            logger.info("1ï¸âƒ£ VectorDB ë§¤ë‹ˆì € ì´ˆê¸°í™”...")
            manager = MultipleVectorDBManager()
            logger.info("âœ… ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
            
            # 2. ìŠ¤í‚¤ë§ˆ ìƒì„±
            logger.info("2ï¸âƒ£ ìŠ¤í‚¤ë§ˆ ìƒì„±...")
            csv_path = Path(args.csv_path)
            
            schema_success, schema, db_name = await create_database_schema(
                manager=manager,
                csv_path=csv_path,
                class_name=args.class_name,
                db_name=args.db_name,
                vectorize_fields=args.vectorize
            )
            
            if not schema_success:
                logger.error("âŒ ìŠ¤í‚¤ë§ˆ ìƒì„± ì‹¤íŒ¨ë¡œ ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ")
                return 1
            
            # 3. ë°ì´í„° ë¡œë”©
            logger.info("3ï¸âƒ£ ë°ì´í„° ë¡œë”©...")
            
            # í…ìŠ¤íŠ¸ íŒŒì¼ ë§¤í•‘ ì„¤ì •
            text_field_mapping = None
            if args.text_dir and args.text_path_field:
                text_field_mapping = {
                    'path_field': args.text_path_field,
                    'content_field': 'content'
                }
            
            text_dir = Path(args.text_dir) if args.text_dir else None
            
            data_success = await load_csv_data(
                manager=manager,
                csv_path=csv_path,
                db_name=db_name,
                schema=schema,
                text_dir=text_dir,
                text_field_mapping=text_field_mapping
            )
            
            if not data_success:
                logger.error("âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
                return 1
            
            # 4. ì¸ì œìŠ¤ì…˜ ê²€ì¦ í…ŒìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
            if not args.no_verification:
                logger.info("4ï¸âƒ£ ì¸ì œìŠ¤ì…˜ ê²€ì¦...")
                await test_database_ingestion(manager, db_name)
            
            # 5. ìµœì¢… ìƒíƒœ í™•ì¸
            logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ì™„ë£Œ!")
            logger.info(f"ğŸ“Š ìµœì¢… ìƒíƒœ:")
            logger.info(f"   í´ë˜ìŠ¤ëª…: {schema['class']}")
            logger.info(f"   DBëª…: {db_name}")
            logger.info(f"   í™œì„± DB ìˆ˜: {len(manager.list_databases())}")
            logger.info(f"   í™œì„± DB ëª©ë¡: {manager.list_databases()}")
            logger.info(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ìŠ¤í‚¤ë§ˆ: {len(manager.list_schemas())}ê°œ")
            
            return 0
            
        except Exception as e:
            logger.error(f"âŒ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return 1
        
        finally:
            # ì •ë¦¬
            if manager:
                manager.close()
                logger.info("ğŸ”Œ VectorDB ì—°ê²° ì¢…ë£Œ")
                
    except ImportError as e:
        logger.error(f"âŒ í•„ìš”í•œ ëª¨ë“ˆì„ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return 1

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = setup_argument_parser()
    
    if len(sys.argv) == 1:
        parser.print_help()
        return 0
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return 0
    
    try:
        if args.command == 'process-csv':
            return await cmd_process_csv(args)
        elif args.command == 'process-documents':
            return await cmd_process_documents(args)
        elif args.command == 'build-kg':
            return await cmd_build_kg(args)
        elif args.command == 'csv-legacy':
            return await cmd_csv_legacy(args)
        else:
            logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹: {args.command}")
            return 1
            
    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        return 130
    except Exception as e:
        logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
