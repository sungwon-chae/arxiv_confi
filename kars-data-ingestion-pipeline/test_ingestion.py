#!/usr/bin/env python3
"""
ë°ì´í„° ì¸ì œìŠ¤ì…˜ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
CSV ë°ì´í„° ë¡œë”©, ìŠ¤í‚¤ë§ˆ ìƒì„±, ë²¡í„° DB ì ì¬ ê²€ì¦ì— ì´ˆì ì„ ë§ì¶˜ í…ŒìŠ¤íŠ¸
"""

import asyncio
import csv
import json
import logging
import os
import tempfile
from pathlib import Path
from typing import Dict, List, Any
import pytest

from simple_manager import MultipleVectorDBManager
from scripts.generate_schema import generate_schema_from_csv
from scripts.setup_database import (
    create_database_schema, 
    load_csv_data, 
    test_database_ingestion,
    normalize_date_to_rfc3339
)
from pipeline_engine import DataIngestionPipeline, PipelineConfig

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class IngestionTestSuite:
    """ë°ì´í„° ì¸ì œìŠ¤ì…˜ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸"""
    
    def __init__(self):
        self.test_data_dir = Path("test_data")
        self.test_data_dir.mkdir(exist_ok=True)
        self.manager = None
        
    async def setup_manager(self):
        """VectorDB ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        self.manager = MultipleVectorDBManager()
        return self.manager
        
    async def cleanup_manager(self):
        """VectorDB ë§¤ë‹ˆì € ì •ë¦¬"""
        if self.manager:
            self.manager.close()
            self.manager = None
    
    def create_test_csv(self, filename: str, data: List[Dict[str, Any]]) -> Path:
        """í…ŒìŠ¤íŠ¸ìš© CSV íŒŒì¼ ìƒì„±"""
        csv_path = self.test_data_dir / filename
        
        if data:
            with open(csv_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=data[0].keys())
                writer.writeheader()
                writer.writerows(data)
        
        return csv_path
    
    def create_test_text_files(self, text_data: Dict[str, str]) -> Path:
        """í…ŒìŠ¤íŠ¸ìš© í…ìŠ¤íŠ¸ íŒŒì¼ë“¤ ìƒì„±"""
        text_dir = self.test_data_dir / "texts"
        text_dir.mkdir(exist_ok=True)
        
        for filename, content in text_data.items():
            with open(text_dir / filename, 'w', encoding='utf-8') as f:
                f.write(content)
        
        return text_dir
    
    async def test_csv_schema_generation(self):
        """CSV ê¸°ë°˜ ìŠ¤í‚¤ë§ˆ ìƒì„± í…ŒìŠ¤íŠ¸"""
        logger.info("=== CSV ìŠ¤í‚¤ë§ˆ ìƒì„± í…ŒìŠ¤íŠ¸ ===")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_data = [
            {
                "id": "doc_001",
                "title": "í…ŒìŠ¤íŠ¸ ë¬¸ì„œ 1",
                "content": "ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ë¬¸ì„œì…ë‹ˆë‹¤.",
                "created_date": "2024-01-01T00:00:00Z",
                "category": "í…ŒìŠ¤íŠ¸",
                "size": "1024"
            },
            {
                "id": "doc_002", 
                "title": "í…ŒìŠ¤íŠ¸ ë¬¸ì„œ 2",
                "content": "ë‘ ë²ˆì§¸ í…ŒìŠ¤íŠ¸ ë¬¸ì„œì…ë‹ˆë‹¤.",
                "created_date": "2024-01-02T00:00:00Z",
                "category": "ìƒ˜í”Œ",
                "size": "2048"
            }
        ]
        
        csv_path = self.create_test_csv("test_schema.csv", test_data)
        
        try:
            # ìŠ¤í‚¤ë§ˆ ìƒì„± í…ŒìŠ¤íŠ¸
            schema, db_name = generate_schema_from_csv(
                csv_path=csv_path,
                class_name="TestDocument",
                db_name="test_schema_db",
                vectorize_fields=["title", "content"]
            )
            
            # ìŠ¤í‚¤ë§ˆ ê²€ì¦
            assert schema is not None, "ìŠ¤í‚¤ë§ˆê°€ ìƒì„±ë˜ì§€ ì•ŠìŒ"
            assert schema["class"] == "TestDocument", f"í´ë˜ìŠ¤ëª… ë¶ˆì¼ì¹˜: {schema['class']}"
            assert len(schema["properties"]) > 0, "ì†ì„±ì´ ì •ì˜ë˜ì§€ ì•ŠìŒ"
            
            # ë²¡í„°í™” í•„ë“œ í™•ì¸
            vectorized_props = [p for p in schema["properties"] if p.get("vectorizePropertyName", False)]
            assert len(vectorized_props) >= 2, f"ë²¡í„°í™” í•„ë“œ ë¶€ì¡±: {len(vectorized_props)}"
            
            logger.info(f"âœ… ìŠ¤í‚¤ë§ˆ ìƒì„± ì„±ê³µ: {schema['class']}, {len(schema['properties'])}ê°œ ì†ì„±")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ìŠ¤í‚¤ë§ˆ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
        finally:
            # ì •ë¦¬
            if csv_path.exists():
                csv_path.unlink()
    
    async def test_csv_data_loading(self):
        """CSV ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸"""
        logger.info("=== CSV ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸ ===")
        
        await self.setup_manager()
        
        try:
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
            test_data = [
                {
                    "id": "load_001",
                    "title": "ë¡œë”© í…ŒìŠ¤íŠ¸ 1",
                    "content": "ë°ì´í„° ë¡œë”©ì„ ìœ„í•œ í…ŒìŠ¤íŠ¸ ë¬¸ì„œì…ë‹ˆë‹¤.",
                    "created_date": "2024-01-01 10:00",
                    "category": "ë¡œë”©í…ŒìŠ¤íŠ¸",
                    "priority": "1"
                },
                {
                    "id": "load_002",
                    "title": "ë¡œë”© í…ŒìŠ¤íŠ¸ 2", 
                    "content": "ë‘ ë²ˆì§¸ ë¡œë”© í…ŒìŠ¤íŠ¸ ë¬¸ì„œì…ë‹ˆë‹¤.",
                    "created_date": "2024-01-02 11:00",
                    "category": "ë¡œë”©í…ŒìŠ¤íŠ¸",
                    "priority": "2"
                }
            ]
            
            csv_path = self.create_test_csv("test_loading.csv", test_data)
            
            # ìŠ¤í‚¤ë§ˆ ìƒì„±
            schema_success, schema, db_name = await create_database_schema(
                self.manager, csv_path, "LoadingTest", "test_loading_db", ["title", "content"]
            )
            
            assert schema_success, "ìŠ¤í‚¤ë§ˆ ìƒì„± ì‹¤íŒ¨"
            
            # ë°ì´í„° ë¡œë”©
            loading_success = await load_csv_data(
                self.manager, csv_path, db_name, schema
            )
            
            assert loading_success, "ë°ì´í„° ë¡œë”© ì‹¤íŒ¨"
            
            # ì¸ì œìŠ¤ì…˜ ê²€ì¦
            verification_success = await test_database_ingestion(self.manager, db_name)
            assert verification_success, "ì¸ì œìŠ¤ì…˜ ê²€ì¦ ì‹¤íŒ¨"
            
            logger.info("âœ… CSV ë°ì´í„° ë¡œë”© ë° ê²€ì¦ ì„±ê³µ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ CSV ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
        finally:
            await self.cleanup_manager()
            # ì •ë¦¬
            for file in self.test_data_dir.glob("test_loading.*"):
                if file.exists():
                    file.unlink()
    
    async def test_enron_data_ingestion(self):
        """ì‹¤ì œ Enron ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ì™¸ë¶€ í…ìŠ¤íŠ¸ ì—°ë™ í…ŒìŠ¤íŠ¸"""
        logger.info("=== ì‹¤ì œ Enron ë°ì´í„° ì¸ì œìŠ¤ì…˜ í…ŒìŠ¤íŠ¸ ===")
        
        await self.setup_manager()
        
        try:
            # ì‹¤ì œ Enron ìŠ¤í‚¤ë§ˆì™€ í…ìŠ¤íŠ¸ íŒŒì¼ ì‚¬ìš©
            enron_csv_path = Path("config/schemas/enron_schema.csv")
            dummy_texts_dir = Path("dummy_texts")
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not enron_csv_path.exists():
                logger.warning(f"Enron CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {enron_csv_path}")
                return True  # í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ
            
            if not dummy_texts_dir.exists():
                logger.warning(f"Dummy texts ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {dummy_texts_dir}")
                return True  # í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ
            
            logger.info(f"ğŸ“ Enron CSV: {enron_csv_path}")
            logger.info(f"ğŸ“ í…ìŠ¤íŠ¸ ë””ë ‰í† ë¦¬: {dummy_texts_dir}")
            
            # ìŠ¤í‚¤ë§ˆ ìƒì„± (ì‹¤ì œ Enron ë°ì´í„° êµ¬ì¡° ì‚¬ìš©)
            schema_success, schema, db_name = await create_database_schema(
                self.manager, 
                enron_csv_path, 
                "EnronDocument", 
                "enron_test_db", 
                ["Email Subject", "content"]  # ì œëª©ê³¼ ë‚´ìš©ì„ ë²¡í„°í™”
            )
            
            assert schema_success, "Enron ìŠ¤í‚¤ë§ˆ ìƒì„± ì‹¤íŒ¨"
            logger.info(f"âœ… Enron ìŠ¤í‚¤ë§ˆ ìƒì„± ì„±ê³µ: {schema['class']}")
            
            # ì™¸ë¶€ í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ë°ì´í„° ë¡œë”©
            text_field_mapping = {
                'path_field': 'Text Precedence',  # CSVì˜ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ ì»¬ëŸ¼
                'content_field': 'content'
            }
            
            loading_success = await load_csv_data(
                self.manager, enron_csv_path, db_name, schema, dummy_texts_dir, text_field_mapping
            )
            
            assert loading_success, "Enron ë°ì´í„° ë¡œë”© ì‹¤íŒ¨"
            
            # ì¸ì œìŠ¤ì…˜ ê²€ì¦
            verification_success = await test_database_ingestion(self.manager, db_name)
            assert verification_success, "Enron ë°ì´í„° ì¸ì œìŠ¤ì…˜ ê²€ì¦ ì‹¤íŒ¨"
            
            # ì¶”ê°€ ê²€ì¦: ì‹¤ì œ í…ìŠ¤íŠ¸ ë‚´ìš©ì´ ë¡œë”©ë˜ì—ˆëŠ”ì§€ í™•ì¸
            try:
                # ê°„ë‹¨í•œ ê²€ìƒ‰ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë‚´ìš© í¬í•¨ ì—¬ë¶€ í™•ì¸
                results = await self.manager.search([db_name], "í•œêµ­ LNG", limit=1)
                
                if ("results" in results and 
                    db_name in results["results"] and 
                    "results" in results["results"][db_name]):
                    
                    db_results = results["results"][db_name]["results"]
                    if db_results:
                        first_result = db_results[0]
                        if hasattr(first_result, 'to_dict'):
                            result_dict = first_result.to_dict()
                        else:
                            result_dict = first_result
                        
                        properties = result_dict.get('properties', {})
                        content = properties.get('content', '')
                        
                        if content and len(content) > 50:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ ë‚´ìš©ì´ ìˆëŠ”ì§€
                            logger.info(f"âœ… ì™¸ë¶€ í…ìŠ¤íŠ¸ ë‚´ìš© í™•ì¸: {len(content)}ì")
                        else:
                            logger.warning("âš ï¸ ì™¸ë¶€ í…ìŠ¤íŠ¸ ë‚´ìš©ì´ ë¹„ì–´ìˆìŒ")
                            
            except Exception as e:
                logger.warning(f"í…ìŠ¤íŠ¸ ë‚´ìš© ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            
            logger.info("âœ… ì‹¤ì œ Enron ë°ì´í„° ì¸ì œìŠ¤ì…˜ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Enron ë°ì´í„° ì¸ì œìŠ¤ì…˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False
        finally:
            await self.cleanup_manager()
    
    async def test_date_normalization(self):
        """ë‚ ì§œ ì •ê·œí™” í…ŒìŠ¤íŠ¸"""
        logger.info("=== ë‚ ì§œ ì •ê·œí™” í…ŒìŠ¤íŠ¸ ===")
        
        test_cases = [
            ("2024-01-01T00:00:00Z", "2024-01-01T00:00:00Z"),  # ì´ë¯¸ RFC3339
            ("2024-01-01 10:30", "2024-01-01T10:30:00Z"),      # ì‹œê°„ í¬í•¨
            ("2024-01-01", "2024-01-01T00:00:00Z"),            # ë‚ ì§œë§Œ
            ("01/15/2024", "2024-01-15T00:00:00Z"),            # MM/DD/YYYY
            ("", "1970-01-01T00:00:00Z"),                      # ë¹ˆ ê°’
            ("ì˜ëª»ëœë‚ ì§œ", "1970-01-01T00:00:00Z")              # ì˜ëª»ëœ í˜•ì‹
        ]
        
        try:
            for input_date, expected in test_cases:
                result = normalize_date_to_rfc3339(input_date)
                assert result == expected, f"ë‚ ì§œ ì •ê·œí™” ì‹¤íŒ¨: {input_date} -> {result} (ì˜ˆìƒ: {expected})"
            
            logger.info("âœ… ë‚ ì§œ ì •ê·œí™” í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ë‚ ì§œ ì •ê·œí™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False
    
    async def test_pipeline_csv_processing(self):
        """íŒŒì´í”„ë¼ì¸ ì—”ì§„ì„ í†µí•œ CSV ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        logger.info("=== íŒŒì´í”„ë¼ì¸ CSV ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ===")
        
        try:
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
            test_data = [
                {
                    "document_id": "pipe_001",
                    "title": "íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ 1",
                    "description": "íŒŒì´í”„ë¼ì¸ ì—”ì§„ì„ í†µí•œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.",
                    "created_at": "2024-01-01T10:00:00Z",
                    "status": "active",
                    "importance": "3"
                },
                {
                    "document_id": "pipe_002",
                    "title": "íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ 2",
                    "description": "ê³ ê¸‰ ì²˜ë¦¬ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.",
                    "created_at": "2024-01-02T11:00:00Z", 
                    "status": "active",
                    "importance": "5"
                }
            ]
            
            csv_path = self.create_test_csv("test_pipeline.csv", test_data)
            
            # íŒŒì´í”„ë¼ì¸ ì„¤ì •
            config = PipelineConfig(
                db_name="test_pipeline_db",
                class_name="PipelineTest",
                vectorize_fields=["title", "description"],
                build_knowledge_graph=False  # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
            )
            
            pipeline = DataIngestionPipeline(config)
            
            try:
                result = await pipeline.process_csv_enhanced(
                    csv_path=str(csv_path),
                    text_dir=None,
                    text_path_field=None,
                    content_field="description"
                )
                
                assert result["success"], f"íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹¤íŒ¨: {result.get('error')}"
                assert result["total_documents"] == 2, f"ë¬¸ì„œ ìˆ˜ ë¶ˆì¼ì¹˜: {result['total_documents']}"
                
                logger.info("âœ… íŒŒì´í”„ë¼ì¸ CSV ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
                return True
                
            finally:
                pipeline.close()
                
        except Exception as e:
            logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ CSV ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False
        finally:
            # ì •ë¦¬
            for file in self.test_data_dir.glob("test_pipeline.*"):
                if file.exists():
                    file.unlink()
    
    async def run_all_tests(self) -> Dict[str, bool]:
        """ëª¨ë“  ì¸ì œìŠ¤ì…˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸš€ ë°ì´í„° ì¸ì œìŠ¤ì…˜ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹œì‘")
        logger.info("=" * 60)
        
        test_results = {}
        
        tests = [
            ("CSV ìŠ¤í‚¤ë§ˆ ìƒì„±", self.test_csv_schema_generation),
            ("CSV ë°ì´í„° ë¡œë”©", self.test_csv_data_loading), 
            ("ì‹¤ì œ Enron ë°ì´í„° ì¸ì œìŠ¤ì…˜", self.test_enron_data_ingestion),
            ("ë‚ ì§œ ì •ê·œí™”", self.test_date_normalization),
            ("íŒŒì´í”„ë¼ì¸ CSV ì²˜ë¦¬", self.test_pipeline_csv_processing)
        ]
        
        for test_name, test_func in tests:
            try:
                logger.info(f"\nâ–¶ï¸ {test_name} í…ŒìŠ¤íŠ¸ ì‹œì‘...")
                result = await test_func()
                test_results[test_name] = result
                
                if result:
                    logger.info(f"âœ… {test_name} í…ŒìŠ¤íŠ¸ ì„±ê³µ")
                else:
                    logger.error(f"âŒ {test_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                    
            except Exception as e:
                logger.error(f"âŒ {test_name} í…ŒìŠ¤íŠ¸ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
                test_results[test_name] = False
        
        # ê²°ê³¼ ìš”ì•½
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        logger.info("=" * 60)
        
        passed = sum(1 for result in test_results.values() if result)
        total = len(test_results)
        
        for test_name, result in test_results.items():
            status = "âœ… PASS" if result else "âŒ FAIL"
            logger.info(f"{status} {test_name}")
        
        logger.info(f"\nğŸ¯ ì´ {passed}/{total}ê°œ í…ŒìŠ¤íŠ¸ í†µê³¼")
        
        if passed == total:
            logger.info("ğŸ‰ ëª¨ë“  ì¸ì œìŠ¤ì…˜ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        else:
            logger.warning(f"âš ï¸ {total - passed}ê°œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        
        return test_results
    
    def cleanup_test_data(self):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬"""
        if self.test_data_dir.exists():
            for file in self.test_data_dir.rglob("*"):
                if file.is_file():
                    file.unlink()
            
            # ë¹ˆ ë””ë ‰í† ë¦¬ ì œê±°
            for dir_path in sorted(self.test_data_dir.rglob("*"), reverse=True):
                if dir_path.is_dir() and not any(dir_path.iterdir()):
                    dir_path.rmdir()


async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜"""
    test_suite = IngestionTestSuite()
    
    try:
        results = await test_suite.run_all_tests()
        
        # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì¢…ë£Œ ì½”ë“œ 1 ë°˜í™˜
        failed_tests = [name for name, result in results.items() if not result]
        
        if failed_tests:
            logger.error(f"ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸: {', '.join(failed_tests)}")
            return 1
        else:
            logger.info("ëª¨ë“  í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
            return 0
            
    finally:
        test_suite.cleanup_test_data()


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)