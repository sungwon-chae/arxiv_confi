"""
í•˜ì´ë¸Œë¦¬ë“œ RAG í”„ë¡œì„¸ì„œ
KnowledgeGraphBuilderì˜ ê²°ê³¼ë¥¼ LightRAG í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê³  í†µí•© ì²˜ë¦¬
"""

import logging
import json
import asyncio
from typing import Dict, Any, List, Optional, Tuple, Set
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, asdict
import hashlib

# LightRAG ê´€ë ¨ imports
try:
    from lightrag import LightRAG
    from lightrag.llm import gpt_4o_mini_complete
    from lightrag.utils import EmbeddingFunc
    LIGHTRAG_AVAILABLE = True
except ImportError:
    LIGHTRAG_AVAILABLE = False
    logging.warning("LightRAG not available. Install with: pip install lightrag")

# ê¸°ì¡´ ëª¨ë“ˆ imports
import sys
sys.path.append(str(Path(__file__).parent.parent))
from processors.multimodal_processor import MultimodalDocumentProcessor
from processors.knowledge_graph_builder import (
    KnowledgeGraphBuilder, 
    Entity, 
    Relationship, 
    KnowledgeGraph,
    EntityType,
    RelationType
)
from processors.lightrag_adapter import WeaviateVectorAdapter, LightRAGIntegration
from weaviate_db import WeaviateDB
from base import VectorDBConfig

logger = logging.getLogger(__name__)

@dataclass
class HybridProcessingResult:
    """í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ê²°ê³¼"""
    success: bool
    file_path: str
    multimodal_elements: List[Dict[str, Any]]
    entities: List[Entity]
    relationships: List[Relationship]
    lightrag_entities: List[Dict[str, Any]]
    weaviate_chunks: List[str]
    processing_time: float
    metadata: Dict[str, Any]

class HybridRAGProcessor:
    """
    ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì²˜ë¦¬ + ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶• + LightRAG í†µí•©
    """
    
    def __init__(self,
                 weaviate_db: WeaviateDB,
                 lightrag_working_dir: str = "./lightrag_data",
                 use_lightrag: bool = True,
                 llm_model: str = "gpt-4o-mini",
                 embedding_model: str = "text-embedding-3-small"):
        """
        Args:
            weaviate_db: ê¸°ì¡´ Weaviate DB ì¸ìŠ¤í„´ìŠ¤
            lightrag_working_dir: LightRAG ì‘ì—… ë””ë ‰í† ë¦¬
            use_lightrag: LightRAG ì‚¬ìš© ì—¬ë¶€
            llm_model: LLM ëª¨ë¸ëª…
            embedding_model: ì„ë² ë”© ëª¨ë¸ëª…
        """
        self.weaviate_db = weaviate_db
        self.use_lightrag = use_lightrag and LIGHTRAG_AVAILABLE
        
        # ê¸°ì¡´ í”„ë¡œì„¸ì„œë“¤
        self.multimodal_processor = MultimodalDocumentProcessor()
        self.kg_builder = KnowledgeGraphBuilder()
        
        # LightRAG í†µí•©
        if self.use_lightrag:
            self.lightrag_integration = LightRAGIntegration(weaviate_db)
            self.vector_adapter = self.lightrag_integration.vector_adapter
            
            # LightRAG ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            self._init_lightrag(lightrag_working_dir, llm_model, embedding_model)
        else:
            self.lightrag_integration = None
            self.vector_adapter = None
            self.rag = None
            
        self.initialized = False
    
    def _init_lightrag(self, working_dir: str, llm_model: str, embedding_model: str):
        """LightRAG ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            # LightRAG ì„¤ì •
            self.rag = LightRAG(
                working_dir=working_dir,
                llm_model_func=self._create_llm_func(llm_model),
                embedding_func=self._create_embedding_func(embedding_model),
                # ì»¤ìŠ¤í…€ ë²¡í„° ìŠ¤í† ë¦¬ì§€ ì‚¬ìš©
                vector_storage=self.vector_adapter,
                graph_storage="NetworkXStorage",  # ë¡œì»¬ ê·¸ë˜í”„ ìŠ¤í† ë¦¬ì§€
                chunk_size=1200,
                chunk_overlap=200,
                enable_llm_cache=True
            )
            logger.info("âœ… LightRAG ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ LightRAG ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.use_lightrag = False
            self.rag = None
    
    def _create_llm_func(self, model_name: str):
        """LightRAGìš© LLM í•¨ìˆ˜ ìƒì„±"""
        def llm_func(prompt, **kwargs):
            try:
                # ê¸°ì¡´ OpenAI í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
                response = self.weaviate_db.openai_client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {"role": "system", "content": kwargs.get("system_prompt", "")},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=kwargs.get("temperature", 0.7),
                    max_tokens=kwargs.get("max_tokens", 1000)
                )
                return response.choices[0].message.content
            except Exception as e:
                logger.error(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                return ""
        
        return llm_func
    
    def _create_embedding_func(self, model_name: str):
        """LightRAGìš© ì„ë² ë”© í•¨ìˆ˜ ìƒì„±"""
        embedding_dim = 1536 if "text-embedding-3" in model_name else 1024
        
        def embed_func(texts: List[str]) -> List[List[float]]:
            try:
                response = self.weaviate_db.openai_client.embeddings.create(
                    input=texts,
                    model=model_name
                )
                return [data.embedding for data in response.data]
            except Exception as e:
                logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
                return [[0.0] * embedding_dim for _ in texts]
        
        return EmbeddingFunc(
            embedding_dim=embedding_dim,
            max_token_size=8192,
            func=embed_func
        )
    
    async def initialize(self):
        """í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        if not self.initialized:
            try:
                # Weaviate ì´ˆê¸°í™”ëŠ” ì´ë¯¸ ì™„ë£Œë¨
                
                # LightRAG ê´€ë ¨ ì´ˆê¸°í™”
                if self.use_lightrag:
                    await self.lightrag_integration.initialize()
                    if self.rag:
                        # LightRAG ìŠ¤í† ë¦¬ì§€ ì´ˆê¸°í™”
                        await self.rag.ainiitialize_storages()
                
                self.initialized = True
                logger.info("âœ… HybridRAGProcessor ì´ˆê¸°í™” ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                raise
    
    async def process_document(self, 
                             file_path: Union[str, Path],
                             extract_options: Optional[Dict[str, bool]] = None) -> HybridProcessingResult:
        """
        ë¬¸ì„œë¥¼ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
        
        1. ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ íŒŒì‹± (MinerU/magic-pdf)
        2. ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶• (ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ)
        3. LightRAG ì¸ë±ì‹± (ê·¸ë˜í”„ ê¸°ë°˜)
        4. Weaviate ë²¡í„° ì €ì¥ (ìƒì„¸ ì»¨í…ìŠ¤íŠ¸)
        """
        if not self.initialized:
            await self.initialize()
        
        start_time = datetime.now()
        file_path = Path(file_path)
        
        try:
            logger.info(f"ğŸ“„ í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ì‹œì‘: {file_path}")
            
            # 1. ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì²˜ë¦¬
            logger.info("1ï¸âƒ£ ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...")
            doc_data = await self.multimodal_processor.process_document(
                file_path, 
                extract_options, 
                output_format="structured_json"
            )
            
            if not doc_data.get("success", False):
                raise Exception(f"ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {doc_data.get('error')}")
            
            # 2. ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶•
            logger.info("2ï¸âƒ£ ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶• ì¤‘...")
            kg = self.kg_builder.build_knowledge_graph([doc_data])
            
            # 3. LightRAG í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ë° ì¸ë±ì‹±
            lightrag_entities = []
            if self.use_lightrag and self.rag:
                logger.info("3ï¸âƒ£ LightRAG ì¸ë±ì‹± ì¤‘...")
                lightrag_entities = await self._index_to_lightrag(kg, doc_data)
            
            # 4. Weaviateì— ìƒì„¸ ì²­í¬ ì €ì¥
            logger.info("4ï¸âƒ£ Weaviate ë²¡í„° ì €ì¥ ì¤‘...")
            chunks = self.multimodal_processor.extract_for_vectorization(doc_data)
            chunk_ids = await self._save_chunks_to_weaviate(chunks, kg)
            
            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # ê²°ê³¼ ìƒì„±
            result = HybridProcessingResult(
                success=True,
                file_path=str(file_path),
                multimodal_elements=doc_data.get("elements", []),
                entities=list(kg.entities.values()),
                relationships=list(kg.relationships.values()),
                lightrag_entities=lightrag_entities,
                weaviate_chunks=chunk_ids,
                processing_time=processing_time,
                metadata={
                    "total_elements": doc_data.get("total_elements", 0),
                    "element_statistics": doc_data.get("element_statistics", {}),
                    "total_entities": len(kg.entities),
                    "total_relationships": len(kg.relationships),
                    "total_chunks": len(chunk_ids),
                    "lightrag_indexed": len(lightrag_entities)
                }
            )
            
            logger.info(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ì™„ë£Œ: {processing_time:.2f}ì´ˆ")
            return result
            
        except Exception as e:
            logger.error(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return HybridProcessingResult(
                success=False,
                file_path=str(file_path),
                multimodal_elements=[],
                entities=[],
                relationships=[],
                lightrag_entities=[],
                weaviate_chunks=[],
                processing_time=(datetime.now() - start_time).total_seconds(),
                metadata={"error": str(e)}
            )
    
    async def _index_to_lightrag(self, 
                               kg: KnowledgeGraph, 
                               doc_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ì§€ì‹ ê·¸ë˜í”„ë¥¼ LightRAG í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¸ë±ì‹±"""
        indexed_entities = []
        
        try:
            # ë¬¸ì„œ ì „ì²´ë¥¼ LightRAGì— ì¶”ê°€ (ìë™ ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ)
            if self.rag:
                # ì „ì²´ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
                full_text = self._collect_text_from_elements(doc_data.get("elements", []))
                
                if full_text:
                    # LightRAGì˜ insertë¡œ ìë™ ì²˜ë¦¬
                    await self.rag.ainsert(full_text)
                    logger.info("LightRAG ìë™ ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ ì™„ë£Œ")
            
            # ì¶”ê°€ë¡œ ìš°ë¦¬ê°€ ì¶”ì¶œí•œ ì—”í‹°í‹°ë“¤ì„ ëª…ì‹œì ìœ¼ë¡œ ì €ì¥
            for entity_id, entity in kg.entities.items():
                # ì—”í‹°í‹° ì„¤ëª… ìƒì„±
                description = self._generate_entity_description(entity, kg)
                
                # LightRAG ë²¡í„° ìŠ¤í† ë¦¬ì§€ì— ì €ì¥
                lightrag_entity = {
                    "id": entity_id,
                    "entity_name": entity.name,
                    "entity_type": entity.entity_type.value,
                    "description": description,
                    "source_id": doc_data.get("file_path", "unknown"),
                    "confidence": entity.confidence,
                    "metadata": {
                        **entity.metadata,
                        "mentions_count": len(entity.mentions),
                        "source_element_types": list(set(
                            m.get("element_type", "unknown") for m in entity.mentions
                        ))
                    }
                }
                
                # ë²¡í„° ì–´ëŒ‘í„°ë¥¼ í†µí•´ ì €ì¥
                result = await self.vector_adapter.upsert(lightrag_entity)
                if result.get("status") == "success":
                    indexed_entities.append(lightrag_entity)
            
            # ê´€ê³„ ì •ë³´ë„ ì €ì¥ (ê´€ê³„ë¥¼ íŠ¹ë³„í•œ ì—”í‹°í‹°ë¡œ ì €ì¥)
            for rel_id, rel in kg.relationships.items():
                source_entity = kg.entities.get(rel.source_entity_id)
                target_entity = kg.entities.get(rel.target_entity_id)
                
                if source_entity and target_entity:
                    rel_description = (
                        f"{source_entity.name} {rel.relation_type.value} {target_entity.name}. "
                        f"Evidence: {'; '.join(rel.evidence[:3])}"
                    )
                    
                    lightrag_rel = {
                        "id": rel_id,
                        "entity_name": f"{source_entity.name}-{target_entity.name}",
                        "entity_type": f"RELATIONSHIP_{rel.relation_type.value}",
                        "description": rel_description,
                        "source_id": doc_data.get("file_path", "unknown"),
                        "confidence": rel.confidence,
                        "metadata": {
                            "source_entity": source_entity.name,
                            "target_entity": target_entity.name,
                            "relation_type": rel.relation_type.value,
                            **rel.metadata
                        }
                    }
                    
                    result = await self.vector_adapter.upsert(lightrag_rel)
                    if result.get("status") == "success":
                        indexed_entities.append(lightrag_rel)
            
        except Exception as e:
            logger.error(f"LightRAG ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
        
        return indexed_entities
    
    def _generate_entity_description(self, entity: Entity, kg: KnowledgeGraph) -> str:
        """ì—”í‹°í‹°ì— ëŒ€í•œ ì„¤ëª… ìƒì„±"""
        description_parts = []
        
        # ê¸°ë³¸ ì •ë³´
        description_parts.append(f"{entity.name} is a {entity.entity_type.value}")
        
        # ë©˜ì…˜ ì»¨í…ìŠ¤íŠ¸
        if entity.mentions:
            contexts = [m.get("context", "") for m in entity.mentions[:3]]
            if contexts:
                description_parts.append(f"mentioned in contexts: {'; '.join(contexts)}")
        
        # ê´€ê³„ ì •ë³´
        related_entities = []
        for rel in kg.relationships.values():
            if rel.source_entity_id == entity.id:
                target = kg.entities.get(rel.target_entity_id)
                if target:
                    related_entities.append(f"{rel.relation_type.value} {target.name}")
            elif rel.target_entity_id == entity.id:
                source = kg.entities.get(rel.source_entity_id)
                if source:
                    related_entities.append(f"is {rel.relation_type.value} by {source.name}")
        
        if related_entities:
            description_parts.append(f"relationships: {', '.join(related_entities[:5])}")
        
        return ". ".join(description_parts)
    
    def _collect_text_from_elements(self, elements: List[Dict[str, Any]]) -> str:
        """ë©€í‹°ëª¨ë‹¬ ìš”ì†Œë“¤ì—ì„œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘"""
        texts = []
        
        for elem in elements:
            if elem.get("type") == "text":
                texts.append(elem.get("content", ""))
            elif elem.get("type") == "table":
                # í…Œì´ë¸”ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                content = elem.get("content", {})
                if isinstance(content, dict) and "rows" in content:
                    table_text = "\n".join([
                        " | ".join(str(cell) for cell in row)
                        for row in content["rows"]
                    ])
                    texts.append(f"Table:\n{table_text}")
            elif elem.get("type") == "image":
                # OCR í…ìŠ¤íŠ¸ ì¶”ê°€
                ocr_text = elem.get("metadata", {}).get("ocr_text", "")
                if ocr_text:
                    texts.append(f"Image text: {ocr_text}")
        
        return "\n\n".join(texts)
    
    async def _save_chunks_to_weaviate(self, 
                                     chunks: List[Dict[str, Any]], 
                                     kg: KnowledgeGraph) -> List[str]:
        """ì²­í¬ë¥¼ Weaviateì— ì €ì¥í•˜ê³  ì—”í‹°í‹° ì •ë³´ë¡œ ë³´ê°•"""
        enhanced_chunks = []
        
        for chunk in chunks:
            # ì²­í¬ì™€ ê´€ë ¨ëœ ì—”í‹°í‹° ì°¾ê¸°
            related_entities = self._find_entities_in_chunk(chunk["content"], kg)
            
            # ë©”íƒ€ë°ì´í„°ì— ì—”í‹°í‹° ì •ë³´ ì¶”ê°€
            chunk["metadata"]["entities"] = [
                {
                    "name": entity.name,
                    "type": entity.entity_type.value,
                    "confidence": entity.confidence
                }
                for entity in related_entities
            ]
            chunk["metadata"]["entity_count"] = len(related_entities)
            
            # ì—”í‹°í‹° ì´ë¦„ë“¤ì„ ë²¡í„°í™” í…ìŠ¤íŠ¸ì— ì¶”ê°€ (ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ)
            if related_entities:
                entity_names = [e.name for e in related_entities]
                chunk["vectorize_text"] = f"{chunk['content']}\nEntities: {', '.join(entity_names)}"
            
            enhanced_chunks.append(chunk)
        
        # Weaviateì— ì €ì¥
        chunk_ids = await self.weaviate_db.insert_documents(
            enhanced_chunks,
            vectorize_fields=["content", "vectorize_text"] if "vectorize_text" in enhanced_chunks[0] else ["content"]
        )
        
        return chunk_ids
    
    def _find_entities_in_chunk(self, chunk_text: str, kg: KnowledgeGraph) -> List[Entity]:
        """ì²­í¬ í…ìŠ¤íŠ¸ì—ì„œ ì–¸ê¸‰ëœ ì—”í‹°í‹° ì°¾ê¸°"""
        found_entities = []
        chunk_lower = chunk_text.lower()
        
        for entity in kg.entities.values():
            # ì—”í‹°í‹° ì´ë¦„ì´ ì²­í¬ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if entity.name.lower() in chunk_lower:
                found_entities.append(entity)
                continue
            
            # ì—”í‹°í‹°ì˜ ë©˜ì…˜ë“¤ í™•ì¸
            for mention in entity.mentions:
                mention_text = mention.get("text", "").lower()
                if mention_text and mention_text in chunk_lower:
                    found_entities.append(entity)
                    break
        
        return found_entities
    
    async def hybrid_search(self,
                          query: str,
                          search_mode: str = "hybrid",
                          top_k: int = 10,
                          filters: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            search_mode: "hybrid", "graph_only", "vector_only"
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            filters: ì¶”ê°€ í•„í„° ì¡°ê±´
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼
        """
        if not self.initialized:
            await self.initialize()
        
        results = {
            "query": query,
            "mode": search_mode,
            "graph_results": [],
            "vector_results": [],
            "combined_results": [],
            "metadata": {}
        }
        
        try:
            # 1. LightRAG ê·¸ë˜í”„ ê²€ìƒ‰
            if search_mode in ["hybrid", "graph_only"] and self.use_lightrag and self.rag:
                logger.info("ğŸ” LightRAG ê·¸ë˜í”„ ê²€ìƒ‰ ì¤‘...")
                graph_response = await self.rag.aquery(
                    query=query,
                    param={"top_k": top_k * 2}  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ í•„í„°ë§
                )
                
                # ê·¸ë˜í”„ ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±
                results["graph_results"] = self._parse_lightrag_response(graph_response)
            
            # 2. Weaviate ë²¡í„° ê²€ìƒ‰
            if search_mode in ["hybrid", "vector_only"]:
                logger.info("ğŸ” Weaviate ë²¡í„° ê²€ìƒ‰ ì¤‘...")
                
                # ê·¸ë˜í”„ ê²€ìƒ‰ì—ì„œ ì°¾ì€ ì—”í‹°í‹°ë¡œ í•„í„° êµ¬ì„±
                entity_filter = None
                if results["graph_results"] and search_mode == "hybrid":
                    entity_names = [r["entity_name"] for r in results["graph_results"][:5]]
                    entity_filter = {"entities": {"contains": entity_names}}
                
                # í•„í„° ë³‘í•©
                combined_filters = {**(filters or {}), **(entity_filter or {})}
                
                # Weaviate ê²€ìƒ‰
                vector_results = await self.weaviate_db.search(
                    query=query,
                    limit=top_k,
                    filters=combined_filters if combined_filters else None,
                    hybrid=True  # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BM25 + ë²¡í„°)
                )
                
                results["vector_results"] = vector_results
            
            # 3. ê²°ê³¼ í†µí•© ë° ìˆœìœ„ ì¬ì¡°ì •
            if search_mode == "hybrid":
                results["combined_results"] = self._combine_results(
                    results["graph_results"],
                    results["vector_results"],
                    top_k
                )
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            results["metadata"] = {
                "total_graph_results": len(results["graph_results"]),
                "total_vector_results": len(results["vector_results"]),
                "total_combined_results": len(results["combined_results"]),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            results["metadata"]["error"] = str(e)
        
        return results
    
    def _parse_lightrag_response(self, response: str) -> List[Dict[str, Any]]:
        """LightRAG ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”"""
        # LightRAGëŠ” í…ìŠ¤íŠ¸ ì‘ë‹µì„ ë°˜í™˜í•˜ë¯€ë¡œ íŒŒì‹± í•„ìš”
        # ì‹¤ì œ êµ¬í˜„ì€ LightRAGì˜ ì‘ë‹µ í˜•ì‹ì— ë”°ë¼ ì¡°ì •
        parsed_results = []
        
        # ê°„ë‹¨í•œ íŒŒì‹± ì˜ˆì‹œ
        if response:
            # ì—”í‹°í‹°ì™€ ê´€ê³„ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒŒì‹± í•„ìš”)
            lines = response.split('\n')
            for line in lines:
                if '->' in line or 'relates to' in line:
                    # ê´€ê³„ ì •ë³´ íŒŒì‹±
                    parsed_results.append({
                        "type": "relationship",
                        "content": line.strip(),
                        "confidence": 0.8
                    })
        
        return parsed_results
    
    def _combine_results(self, 
                        graph_results: List[Dict[str, Any]], 
                        vector_results: List[Dict[str, Any]], 
                        top_k: int) -> List[Dict[str, Any]]:
        """ê·¸ë˜í”„ì™€ ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ë¥¼ í†µí•©í•˜ì—¬ ìˆœìœ„ ì¬ì¡°ì •"""
        combined = []
        seen_ids = set()
        
        # ì ìˆ˜ ì •ê·œí™” ë° ê°€ì¤‘ì¹˜ ì ìš©
        graph_weight = 0.6  # ê·¸ë˜í”„ ê²€ìƒ‰ ê°€ì¤‘ì¹˜
        vector_weight = 0.4  # ë²¡í„° ê²€ìƒ‰ ê°€ì¤‘ì¹˜
        
        # ê·¸ë˜í”„ ê²°ê³¼ ì¶”ê°€
        for result in graph_results:
            result_id = result.get("id", str(hash(result.get("content", ""))))
            if result_id not in seen_ids:
                combined.append({
                    **result,
                    "source": "graph",
                    "combined_score": result.get("score", 0.5) * graph_weight
                })
                seen_ids.add(result_id)
        
        # ë²¡í„° ê²°ê³¼ ì¶”ê°€
        for result in vector_results:
            result_id = result.get("id")
            if result_id not in seen_ids:
                combined.append({
                    **result,
                    "source": "vector",
                    "combined_score": result.get("score", 0.5) * vector_weight
                })
                seen_ids.add(result_id)
            else:
                # ì´ë¯¸ ìˆëŠ” ê²°ê³¼ë¼ë©´ ì ìˆ˜ ì—…ë°ì´íŠ¸
                for item in combined:
                    if item.get("id") == result_id:
                        item["combined_score"] += result.get("score", 0.5) * vector_weight
                        item["source"] = "both"
                        break
        
        # í†µí•© ì ìˆ˜ë¡œ ì •ë ¬
        combined.sort(key=lambda x: x["combined_score"], reverse=True)
        
        return combined[:top_k]
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.vector_adapter:
            self.vector_adapter.close()

# ì‚¬ìš© ì˜ˆì‹œ
async def example_usage():
    """HybridRAGProcessor ì‚¬ìš© ì˜ˆì‹œ"""
    
    # Weaviate DB ì„¤ì •
    config = VectorDBConfig(
        db_type="weaviate",
        connection_params={
            "url": "http://localhost:8084",
            "openai_api_key": "your-key",
            "openai_base_url": "http://localhost:8125"
        },
        embedding_model="BAAI/bge-m3",
        default_class="Documents"
    )
    
    weaviate_db = WeaviateDB(config)
    await weaviate_db.initialize()
    
    # í•˜ì´ë¸Œë¦¬ë“œ í”„ë¡œì„¸ì„œ ìƒì„±
    processor = HybridRAGProcessor(
        weaviate_db=weaviate_db,
        use_lightrag=True
    )
    await processor.initialize()
    
    # ë¬¸ì„œ ì²˜ë¦¬
    result = await processor.process_document(
        "path/to/document.pdf",
        extract_options={
            "extract_text": True,
            "extract_images": True,
            "extract_tables": True
        }
    )
    
    if result.success:
        print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"- ë©€í‹°ëª¨ë‹¬ ìš”ì†Œ: {len(result.multimodal_elements)}ê°œ")
        print(f"- ì¶”ì¶œëœ ì—”í‹°í‹°: {len(result.entities)}ê°œ")
        print(f"- ì¶”ì¶œëœ ê´€ê³„: {len(result.relationships)}ê°œ")
        print(f"- LightRAG ì¸ë±ì‹±: {len(result.lightrag_entities)}ê°œ")
        print(f"- Weaviate ì²­í¬: {len(result.weaviate_chunks)}ê°œ")
        print(f"- ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.2f}ì´ˆ")
    
    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    search_results = await processor.hybrid_search(
        query="Find information about machine learning models",
        search_mode="hybrid",
        top_k=5
    )
    
    print(f"\nğŸ” ê²€ìƒ‰ ê²°ê³¼:")
    print(f"- ê·¸ë˜í”„ ê²°ê³¼: {len(search_results['graph_results'])}ê°œ")
    print(f"- ë²¡í„° ê²°ê³¼: {len(search_results['vector_results'])}ê°œ")
    print(f"- í†µí•© ê²°ê³¼: {len(search_results['combined_results'])}ê°œ")
    
    processor.close()
    weaviate_db.close()

if __name__ == "__main__":
    asyncio.run(example_usage())