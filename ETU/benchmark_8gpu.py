#!/usr/bin/env python3
"""
8ëŒ€ H200 GPU ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸
ë©”ëª¨ë¦¬ ëŒ€ì—­í­, ì—°ì‚° ì„±ëŠ¥, ë³‘ë ¬ ì²˜ë¦¬ íš¨ìœ¨ì„± ì¸¡ì •
"""

import os
import time
import torch
import torch.distributed as dist
import torch.nn as nn
import numpy as np
from datetime import datetime

class H200Benchmark:
    def __init__(self):
        self.gpu_count = torch.cuda.device_count()
        self.device_names = []
        self.device_memories = []
        self.results = {}
        
        # GPU ì •ë³´ ìˆ˜ì§‘
        self._collect_gpu_info()
        
    def _collect_gpu_info(self):
        """8ëŒ€ H200 GPU ì •ë³´ ìˆ˜ì§‘"""
        print("ğŸš€ 8ëŒ€ H200 GPU ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        print("=" * 60)
        
        for i in range(self.gpu_count):
            props = torch.cuda.get_device_properties(i)
            name = props.name
            memory_gb = props.total_memory / 1024**3
            
            self.device_names.append(name)
            self.device_memories.append(memory_gb)
            
            print(f"GPU {i}: {name} ({memory_gb:.1f} GB)")
        
        total_memory = sum(self.device_memories)
        print(f"ğŸ’¾ ì´ GPU ë©”ëª¨ë¦¬: {total_memory:.1f} GB")
        print("=" * 60)
        
    def benchmark_memory_bandwidth(self):
        """ë©”ëª¨ë¦¬ ëŒ€ì—­í­ ë²¤ì¹˜ë§ˆí¬"""
        print("ğŸ“Š ë©”ëª¨ë¦¬ ëŒ€ì—­í­ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        
        results = {}
        for i in range(self.gpu_count):
            torch.cuda.set_device(i)
            
            # ëŒ€ìš©ëŸ‰ í…ì„œ ìƒì„± (1GB)
            size = 1024 * 1024 * 1024 // 4  # float32 ê¸°ì¤€
            x = torch.randn(size, device=f'cuda:{i}')
            y = torch.randn(size, device=f'cuda:{i}')
            
            # ë©”ëª¨ë¦¬ ë³µì‚¬ ë²¤ì¹˜ë§ˆí¬
            start_time = time.time()
            for _ in range(100):
                z = x + y
                torch.cuda.synchronize()
            end_time = time.time()
            
            # ëŒ€ì—­í­ ê³„ì‚° (GB/s)
            total_bytes = size * 4 * 100 * 2  # ì½ê¸° + ì“°ê¸°
            bandwidth_gbps = (total_bytes / 1024**3) / (end_time - start_time)
            
            results[f'gpu_{i}'] = bandwidth_gbps
            print(f"GPU {i} ë©”ëª¨ë¦¬ ëŒ€ì—­í­: {bandwidth_gbps:.2f} GB/s")
        
        self.results['memory_bandwidth'] = results
        return results
    
    def benchmark_compute_performance(self):
        """ì—°ì‚° ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        print("ğŸ”¢ ì—°ì‚° ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        
        results = {}
        for i in range(self.gpu_count):
            torch.cuda.set_device(i)
            
            # í–‰ë ¬ ê³±ì…ˆ ë²¤ì¹˜ë§ˆí¬
            size = 4096
            a = torch.randn(size, size, device=f'cuda:{i}')
            b = torch.randn(size, size, device=f'cuda:{i}')
            
            # warmup
            for _ in range(10):
                _ = torch.mm(a, b)
                torch.cuda.synchronize()
            
            # ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬
            start_time = time.time()
            for _ in range(100):
                c = torch.mm(a, b)
                torch.cuda.synchronize()
            end_time = time.time()
            
            # FLOPS ê³„ì‚°
            flops = 2 * size**3 * 100  # í–‰ë ¬ ê³±ì…ˆ FLOPS
            gflops = flops / (end_time - start_time) / 1e9
            
            results[f'gpu_{i}'] = gflops
            print(f"GPU {i} ì—°ì‚° ì„±ëŠ¥: {gflops:.2f} GFLOPS")
        
        self.results['compute_performance'] = results
        return results
    
    def benchmark_multi_gpu_scaling(self):
        """ë©€í‹° GPU ìŠ¤ì¼€ì¼ë§ ë²¤ì¹˜ë§ˆí¬"""
        print("ğŸ”„ ë©€í‹° GPU ìŠ¤ì¼€ì¼ë§ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        
        results = {}
        
        # ë‹¨ì¼ GPUë¶€í„° 8ëŒ€ GPUê¹Œì§€ ì ì§„ì  í…ŒìŠ¤íŠ¸
        for gpu_count in range(1, min(9, self.gpu_count + 1)):
            print(f"  {gpu_count}ëŒ€ GPU í…ŒìŠ¤íŠ¸...")
            
            # ë°°ì¹˜ í¬ê¸° ì¡°ì •
            batch_size = 32 * gpu_count
            size = 2048
            
            # ë°ì´í„° ìƒì„±
            inputs = []
            for i in range(gpu_count):
                torch.cuda.set_device(i)
                x = torch.randn(batch_size // gpu_count, size, device=f'cuda:{i}')
                inputs.append(x)
            
            # ìˆœì°¨ ì²˜ë¦¬
            start_time = time.time()
            outputs = []
            for i, x in enumerate(inputs):
                torch.cuda.set_device(i)
                y = torch.mm(x, x.t())
                outputs.append(y)
                torch.cuda.synchronize()
            seq_time = time.time() - start_time
            
            # ë³‘ë ¬ ì²˜ë¦¬ (ì‹œë®¬ë ˆì´ì…˜)
            start_time = time.time()
            for i, x in enumerate(inputs):
                torch.cuda.set_device(i)
                y = torch.mm(x, x.t())
                torch.cuda.synchronize()
            par_time = time.time() - start_time
            
            # ìŠ¤ì¼€ì¼ë§ íš¨ìœ¨ì„±
            efficiency = seq_time / par_time / gpu_count
            results[f'{gpu_count}gpu'] = {
                'sequential_time': seq_time,
                'parallel_time': par_time,
                'efficiency': efficiency
            }
            
            print(f"    {gpu_count}ëŒ€ GPU íš¨ìœ¨ì„±: {efficiency:.3f}")
        
        self.results['multi_gpu_scaling'] = results
        return results
    
    def benchmark_memory_utilization(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë²¤ì¹˜ë§ˆí¬"""
        print("ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        
        results = {}
        for i in range(self.gpu_count):
            torch.cuda.set_device(i)
            
            # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
            torch.cuda.empty_cache()
            initial_memory = torch.cuda.memory_allocated(i)
            
            # ì ì§„ì  ë©”ëª¨ë¦¬ í• ë‹¹
            tensors = []
            memory_usage = []
            
            for size_mb in [100, 200, 500, 1000, 2000, 5000, 10000]:
                try:
                    # MB ë‹¨ìœ„ë¡œ í…ì„œ ìƒì„±
                    size = size_mb * 1024 * 1024 // 4
                    x = torch.randn(size, device=f'cuda:{i}')
                    tensors.append(x)
                    
                    current_memory = torch.cuda.memory_allocated(i)
                    memory_usage.append({
                        'size_mb': size_mb,
                        'allocated_gb': current_memory / 1024**3,
                        'reserved_gb': torch.cuda.memory_reserved(i) / 1024**3
                    })
                    
                except torch.cuda.OutOfMemoryError:
                    print(f"GPU {i}: {size_mb}MBì—ì„œ OOM ë°œìƒ")
                    break
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del tensors
            torch.cuda.empty_cache()
            
            results[f'gpu_{i}'] = memory_usage
            print(f"GPU {i} ìµœëŒ€ ë©”ëª¨ë¦¬: {memory_usage[-1]['allocated_gb']:.2f} GB")
        
        self.results['memory_utilization'] = results
        return results
    
    def run_all_benchmarks(self):
        """ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("ğŸš€ ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        print()
        
        # 1. ë©”ëª¨ë¦¬ ëŒ€ì—­í­
        self.benchmark_memory_bandwidth()
        print()
        
        # 2. ì—°ì‚° ì„±ëŠ¥
        self.benchmark_compute_performance()
        print()
        
        # 3. ë©€í‹° GPU ìŠ¤ì¼€ì¼ë§
        self.benchmark_multi_gpu_scaling()
        print()
        
        # 4. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        self.benchmark_memory_utilization()
        print()
        
        # ê²°ê³¼ ìš”ì•½
        self._print_summary()
        
        # ê²°ê³¼ ì €ì¥
        self._save_results()
    
    def _print_summary(self):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½"""
        print("=" * 60)
        print("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        
        # ë©”ëª¨ë¦¬ ëŒ€ì—­í­ í‰ê· 
        if 'memory_bandwidth' in self.results:
            avg_bandwidth = np.mean(list(self.results['memory_bandwidth'].values()))
            print(f"í‰ê·  ë©”ëª¨ë¦¬ ëŒ€ì—­í­: {avg_bandwidth:.2f} GB/s")
        
        # ì—°ì‚° ì„±ëŠ¥ í‰ê· 
        if 'compute_performance' in self.results:
            avg_gflops = np.mean(list(self.results['compute_performance'].values()))
            print(f"í‰ê·  ì—°ì‚° ì„±ëŠ¥: {avg_gflops:.2f} GFLOPS")
        
        # ë©€í‹° GPU íš¨ìœ¨ì„±
        if 'multi_gpu_scaling' in self.results:
            efficiencies = [v['efficiency'] for v in self.results['multi_gpu_scaling'].values()]
            avg_efficiency = np.mean(efficiencies)
            print(f"í‰ê·  ë©€í‹° GPU íš¨ìœ¨ì„±: {avg_efficiency:.3f}")
        
        print("=" * 60)
    
    def _save_results(self):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"h200_benchmark_{timestamp}.json"
        
        import json
        with open(filename, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)
        
        print(f"ğŸ“ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥ë¨: {filename}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=== ETU 8ëŒ€ H200 GPU ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ===")
    
    # ë²¤ì¹˜ë§ˆí¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    benchmark = H200Benchmark()
    
    # ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark.run_all_benchmarks()

if __name__ == "__main__":
    main() 