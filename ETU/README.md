# ETU: Exponential-Tilted Unlearning

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

**ETU (Exponential-Tilted Unlearning)** is a state-of-the-art machine unlearning framework for large language models, providing theoretical guarantees and practical efficiency through exponential-tilted distributions and adaptive Î» control.

## ğŸš€ **Quick Overview**

**ETU** formulates unlearning as a KL I-projection with mass constraint, yielding:
- **Closed-form solution**: Direct Îµ-Î» mapping for precise control
- **Provable guarantees**: Ï€_Î¸'(S) â‰¤ Îµ + âˆš(Î´/2) under bounded training error
- **No critics needed**: Works without preference pairs or reference models
- **Parameter-efficient**: Compatible with LoRA and other PEFT methods
- **Memory efficient**: Direct tensor slicing without boolean masks

### **One-Line Usage**
```bash
python -m etu.unlearn --forget_corpora "cais/wmdp-corpora:cyber-forget-corpus" --retain_corpora "cais/wmdp-corpora:bio-retain-corpus" --epsilon 0.05 --use_lora --frozen_on_cpu
```

---

## ğŸ“‹ **Table of Contents**

- [Installation](#installation)
- [Quick Start](#quick-start)
- [Advanced Usage](#advanced-usage)
- [Experiments](#experiments)
- [API Reference](#api-reference)
- [Contributing](#contributing)
- [Citation](#citation)

## ğŸ› ï¸ **Installation**

### Prerequisites
- Python 3.9+
- PyTorch 2.0+
- CUDA-compatible GPU (recommended)

### Install Dependencies
```bash
git clone https://github.com/sungwon-chae/ETU.git
cd ETU
pip install -r requirements.txt
```

## ğŸš€ **Quick Start**

### Basic Unlearning
```bash
python -m etu.unlearn \
  --forget_corpora "cais/wmdp-corpora:cyber-forget-corpus" \
  --retain_corpora "cais/wmdp-corpora:bio-retain-corpus" \
  --epsilon 0.05 \
  --use_lora \
  --frozen_on_cpu \
  --verbose
```

### Quick Debug Mode
```bash
python -m etu.unlearn \
  --forget_corpora wikitext \
  --retain_corpora wikitext \
  --epsilon 0.1 \
  --max_num_batches 10 \
  --frozen_on_cpu \
  --log_every 5
```

## ğŸ”¬ **Advanced Usage**

### PMI-based V_S Refinement
```bash
python -m etu.unlearn \
  --forget_corpora "cais/wmdp-corpora:cyber-forget-corpus" \
  --retain_corpora "cais/wmdp-corpora:bio-retain-corpus" \
  --use_pmi_vs \
  --pmi_top_k 2000 \
  --pmi_min_count 20 \
  --pmi_smoothing 1.0 \
  --epsilon 0.05 \
  --use_lora \
  --frozen_on_cpu
```

### Preference Learning Integration
```bash
# NPO (Neural Preference Optimization)
python -m etu.unlearn \
  --forget_corpora "cais/wmdp-corpora:cyber-forget-corpus" \
  --retain_corpora "cais/wmdp-corpora:bio-retain-corpus" \
  --preference_weight 0.1 \
  --pref_format npo \
  --pref_every 5 \
  --pref_margin 0.0 \
  --epsilon 0.05 \
  --use_lora \
  --frozen_on_cpu

# DPO (Direct Preference Optimization)
python -m etu.unlearn \
  --forget_corpora "cais/wmdp-corpora:cyber-forget-corpus" \
  --retain_corpora "cais/wmdp-corpora:bio-retain-corpus" \
  --preference_weight 0.1 \
  --pref_format dpo \
  --pref_beta 0.1 \
  --pref_every 5 \
  --epsilon 0.05 \
  --use_lora \
  --frozen_on_cpu
```

## ğŸ“Š **Experiments**

### Paper Reproduction
```bash
# Run complete paper experiments
./run_paper_experiments.sh

# Hyperparameter sweep
./run_hyperparameter_sweep.sh
```

### Performance Optimization
```bash
# LoRA rank comparison
for rank in 64 128 256 512; do
  python -m etu.unlearn \
    --forget_corpora "cais/wmdp-corpora:cyber-forget-corpus" \
    --retain_corpora "cais/wmdp-corpora:bio-retain-corpus" \
    --epsilon 0.05 \
    --use_lora \
    --lora_r $rank \
    --frozen_on_cpu \
    --max_num_batches 40
done
```

## ğŸ”§ **Configuration Options**

### Core Parameters
- `--epsilon`: Target suppression threshold (default: 0.05)
- `--lambda_max`: Maximum Î» value (default: 12.0)
- `--lambda_update_freq`: Î» update frequency (default: 25)
- `--lambda_eta`: Î» update step size (default: 0.25)

### V_S Configuration
- `--use_pmi_vs`: Enable PMI-based refinement (default: True)
- `--pmi_top_k`: Top-K tokens by PMI (default: 2000)
- `--pmi_min_count`: Minimum frequency for PMI (default: 20)
- `--vocab_top_k`: Top-K tokens by frequency (default: None)

### LoRA Integration
- `--use_lora`: Enable LoRA (default: False)
- `--lora_r`: LoRA rank (default: 256)
- `--lora_alpha`: LoRA scaling factor (default: 512)
- `--lora_dropout`: LoRA dropout rate (default: 0.1)

### Performance Options
- `--frozen_on_cpu`: Keep frozen model on CPU (default: False)
- `--batch_size`: Training batch size (default: 4)
- `--max_num_batches`: Maximum batches per epoch (default: 80)
- `--log_every`: Log frequency for non-verbose mode (default: 50)
- `--deterministic`: Enable deterministic algorithms (may impact performance)

## ğŸ“ˆ **Results & Evaluation**

### Output Files
- `V_S.ids.json`: Forbidden token set for reproducibility
- `args.json`: Complete experiment configuration
- `metrics.json`: Training metrics and final results
- `suppression_report.json`: Detailed suppression analysis with Wilson bounds

### Key Metrics
- **Ï€Î¸(S)**: Current probability mass on forbidden set
- **Î»**: Current exponential tilting parameter
- **KL divergence**: Distance to target distribution
- **Wilson upper bound**: Statistical confidence interval
- **Target achieved**: Whether Îµ threshold is met

## ğŸ§ª **Testing & Validation**

### Mathematical Validation
```bash
python test_math.py
```

### Import Testing
```bash
python -c "
from etu.unlearn import run_etu, get_args
from etu.utils import load_model, get_data
print('âœ… All imports successful')
"
```

## ğŸ“š **API Reference**

### Core Functions
- `run_etu()`: Main ETU training function
- `get_args()`: Command-line argument parser
- `load_model()`: Model and tokenizer loader
- `get_data()`: Dataset loader for WMDP and WikiText

### Utility Functions
- `build_forbidden_token_ids()`: V_S construction
- `build_forbidden_token_ids_pmi()`: PMI-based V_S refinement
- `wilson_upper()`: Wilson confidence interval
- `create_suppression_report()`: Evaluation report generator

## ğŸ¤ **Contributing**

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

### Development Setup
```bash
git clone https://github.com/sungwon-chae/ETU.git
cd ETU
pip install -r requirements.txt
pip install -e .
```

### Code Style
- Follow PEP 8 guidelines
- Use type hints where appropriate
- Add docstrings for all functions
- Include tests for new features

## ğŸ“– **Citation**

If you use ETU in your research, please cite:

```bibtex
@article{etu2026,
  title={ETU: Exponential-Tilted Unlearning for Large Language Models},
  author={Chae, ...},
  journal={iclr2026},
  year={2026}
}
```

## ğŸ“„ **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ **Acknowledgments**

- Built on top of [Hugging Face Transformers](https://github.com/huggingface/transformers)
- LoRA integration with [PEFT](https://github.com/huggingface/peft)
- Dataset support from [Hugging Face Datasets](https://github.com/huggingface/datasets)

## ğŸ“ **Contact**

- **Author**: Sungwon Chae
- **Email**: csw0815@snu.ac.kr
- **GitHub**: [@sungwon-chae](https://github.com/sungwon-chae)
- **Institution**: Seoul National University

---

## ğŸ”¬ **Detailed Technical Documentation**

### Theory

ETU formulates unlearning as the KL I-projection of the base model Ï€_base onto the convex set defined by the mass constraint:

```
min_q KL(q(Â·|x) || Ï€_base(Â·|x))  s.t.  Î£_{yâˆˆS} q(y|x) â‰¤ Îµ
```

The solution takes the exponential-tilted form:

```
q_Î»(y|x) = Ï€_base(y|x) * exp(-Î» * 1{yâˆˆS}) / Z(Î»)
```

where Î» is computed via the closed-form mapping:

```
Î» = logit(p_S) - logit(Îµ)
```

**Theoretical Guarantee**: Ï€_learn(S) â‰¤ Îµ + âˆš(Î´/2) under bounded training error Î´.

### Critical Implementation Details

#### 1. Proper S (Forbidden Set) Handling
- **V_S construction**: Builds forbidden token set from forget data with frequency filtering
- **Token-level approximation**: Uses token frequencies to approximate sequence-level S
- **Configurable size**: `--vocab_top_k` to limit V_S size for stability
- **Empty V_S protection**: Runtime checks prevent training with empty forbidden sets

#### 2. Correct p_S Estimation
- **Mass-based**: Computes Î£_{yâˆˆV_S} Ï€_base(y|x) over positions and batches
- **Base model only**: Uses frozen Ï€_base, not updated model
- **Numerically stable**: Proper clipping and normalization
- **Confidence intervals**: 95% CI reporting for statistical significance

#### 3. Adaptive Î» Control
- **Real-time adjustment**: Updates Î» based on current Ï€_Î¸(S) vs target Îµ
- **Pinsker margin**: Uses Îµ + âˆš(Î´/2) as adjustment threshold
- **EMA smoothing**: Exponential moving average reduces noise
- **Configurable frequency**: `--lambda_update_freq` controls update rate

#### 4. Memory and Performance Optimizations
- **Direct slicing**: No boolean masks, efficient tensor operations
- **Mixed precision**: AMP support for bf16/fp16 training
- **Gradient clipping**: Stable training with norm clipping
- **Learning rate scheduling**: Linear warmup with linear decay for better convergence

#### 5. Amplification Mode

ETU supports amplification mode where Î» < 0 to increase probability mass on S:
- **Use case**: Amplification useful for expanding model capabilities on specific topics
- **Control**: Î» updates also allow negative values when `--allow_negative_lambda` is set
- **Example**: When Îµ > p_S, Î» becomes negative to amplify Ï€_Î¸(S) above Îµ

#### 6. Retain-Free Operation

ETU can operate without retain data, focusing purely on suppression:
- **Core mechanism**: ETU's objective function controls S mass relative to base distribution Ï€_base
- **Retain purpose**: Language/domain utility monitoring + weak regularization
- **Suppression guarantee**: Ï€_Î¸(S) â‰¤ Îµ + âˆš(Î´/2) holds regardless of retain data
- **Usage**: Set `--retain_corpora ""` and `--retain_weight 0` for suppression-only experiments

#### 7. Preference-Based Refinement

ETU supports optional preference-based refinement for fine-grained control:
- **NPO (default)**: Hinge loss encouraging retain > forget logprobs
- **DPO (optional)**: Logistic loss with reference model (frozen) for stronger ranking signals
- **Integration**: Combines global suppression (Î»-tilt) with local preference (posâ†‘, negâ†“)
- **Safety**: Only active when retain data is available and `--preference_weight > 0`
- **Performance**: Controlled frequency with `--pref_every` to balance speed and effectiveness

#### 8. Retain Data Management

ETU provides flexible retain data handling:
- **Split alignment**: forgetì™€ retain split ìˆ˜ë¥¼ ë§ì¶°ì•¼ í•¨ (ì˜ˆ: bio:forget,cyber:forget + wikitext,wikitext)
- **Broadcast mode**: `--retain_broadcast`ë¡œ ë‹¨ì¼ retain corpusë¥¼ ëª¨ë“  forget splitì— ì¬ì‚¬ìš©
- **Automatic fallback**: retain ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ retain loss ìƒëµí•˜ê³  ETU ë‹¨ë…ìœ¼ë¡œ ë™ì‘

#### 9. Quality Control Checklist

##### **ë¹ ë¥¸ í’ˆì§ˆ ì²´í¬ë¦¬ìŠ¤íŠ¸**
- **V_S í¬ê¸° ë¡œê·¸**: |V_S|/Vê°€ 5~15% ì •ë„ë©´ ë³´í†µ ì•ˆì •ì . 50%â†‘ ê²½ê³  ëœ¨ë©´ `--pmi_top_k` ì¤„ì´ê¸°
- **retain ì—†ì´ ETUë§Œ**: `--retain_weight 0` ì´ê±°ë‚˜ retain ë¹„ì–´ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ retain-loss ìŠ¤í‚µë¨
- **Î» ë¡œê¹…**: [Î»-update]ì—ì„œ EMA Ï€Î¸(S)ê°€ Îµ ì•„ë˜ë¡œ ì•ˆì •ë˜ë©´ OK
- **ìµœì¢… ì‚°ì¶œë¬¼**: V_S.ids.json, args.json, metrics.json ì €ì¥ í™•ì¸
- **Target achieved**: create_suppression_report()ì—ì„œ âœ“ ë‚˜ì˜¤ëŠ”ì§€ ì²´í¬

##### **ì¶”ê°€ ë¯¸ì„¸ íŒ**
- **sample_size**: `estimate_p_S_over_VS(..., sample_size=512)`ëŠ” ë¬´ê±°ì›€. ê°œë°œ/ë””ë²„ê·¸ ë•ŒëŠ” 128, ì‹¤í—˜ ê³ ì •ë³¸ì—ì„œ 512
- **LoRA ì—†ì´**: `--use_lora` ì œê±° + `--layer_ids`ë¡œ ì¢í˜€(ì¤‘ê°„ 3~4ê°œ ë ˆì´ì–´) ì•ˆì •ì„±ê³¼ ì†ë„ ì¡ê¸°
- **DPO**: `reference_model=frozen`ì„ ë„˜ê¸°ëŠ” ê²Œ ìˆ˜ë ´ì´ ì˜ ë¨ (ìê¸°ì°¸ì¡° self-DPOëŠ” ê°€ë” í”ë“¤ë¦¼)

### Theoretical Guarantees

ETU provides principled guarantees:
- **Suppression**: Ï€_Î¸(S) â‰¤ Îµ + âˆš(Î´/2) where Î´ is KL(Ï€_Î¸ || q_Î») estimated from evaluation
- **Control**: Wilson upper bounds provide conservative confidence intervals
- **Audit**: Comprehensive reporting with both point estimates and statistical bounds
- **Î´ measurement**: Î´ëŠ” KL(Ï€Î¸â€–qÎ»)ì„ í‰ê°€ ì‹œ ì¸¡ì •í•˜ë©°, ë¦¬í¬íŠ¸ì—ì„œ Wilson ìƒí•œê³¼ í•¨ê»˜ ë³´ê³ í•œë‹¤

### Advanced Configuration

#### Wilson Upper Bound Control
- `--wilson_max_n 2048`: Wilson ìƒí•œì—ì„œ ì‚¬ìš©í•  n_eff ìƒí•œ (ê¸°ë³¸ê°’: 2048)

#### V_S Filtering Parameters
- `--vs_freq_rate 0.01`: V_S ë¹ˆë„ ì»· ë¹„ìœ¨ (ê¸°ë³¸ê°’: 1%)
- `--vs_abs_cap 20000`: V_S ë¹ˆë„ ì ˆëŒ€ ìƒí•œ (ê¸°ë³¸ê°’: 20,000)

#### Held-out Mini Evaluation
- **Concept**: Î» ì—…ë°ì´íŠ¸ ì‹œ ì‘ì€ held-out ë°ì´í„°(4-8 ë°°ì¹˜)ë¡œ Ï€_Î¸(S)ì™€ Î´ë¥¼ ì¬ì¸¡ì •í•˜ì—¬ ë°°ì¹˜ í¸í–¥ ì™„í™”
- **Benefits**: ë” ì •í™•í•œ Pinsker ë§ˆì§„ ê³„ì‚°, ì•ˆì •ì ì¸ Î» ì œì–´

#### Experimental Features
- `--span_masking`: BPE ì—°ì† ì¡°ê° ìŠ¤íŒ¬ ë‹¨ìœ„ V_S í™•ì¥ (ì‹¤í—˜ì , í˜„ì¬ëŠ” flagë§Œ ë…¸ì¶œ/ë¹„í™œì„±)
- `--span_ngram_max 3`: ìŠ¤íŒ¬ ë§ˆìŠ¤í‚¹ì˜ ìµœëŒ€ n-gram í¬ê¸°

#### Activation Analysis (Optional)
- **ERASER-inspired**: Activation extraction functions for analysis/debugging
- **Note**: ETU's core mechanism relies on probability mass estimation over V_S, not activation-based unlearning
- **Purpose**: Useful for understanding model behavior and computing layer statistics
- **Usage**: Available in `utils.py` but not used in core ETU training
- **Flag**: `--analyze_activations` to enable activation analysis (disabled by default)
- **Use cases**: Layer selection validation, stability monitoring, LoRA debugging, ablation studies
- **Performance**: Only use when needed - hooks add overhead and memory usage

#### Activation Analysis Usage
```bash
# Enable activation analysis for debugging
python run_etu_lora.py \
    --model_name_or_path HuggingFaceH4/zephyr-7b-beta \
    --forget_corpora bio-forget-corpus,cyber-forget-corpus \
    --retain_corpora wikitext,wikitext \
    --epsilon 0.05 --use_lora \
    --analyze_activations --verbose
```

#### Parameter Selection
- `--name_keywords q_proj,k_proj,v_proj,o_proj`: íŒŒë¼ë¯¸í„° ì´ë¦„ í‚¤ì›Œë“œ (ê¸°ë³¸ê°’: attention projections)
- `--param_ids`: íŒŒë¼ë¯¸í„° ì¸ë±ìŠ¤ (name_keywordsì™€ í•¨ê»˜ ì‚¬ìš©í•˜ì§€ ë§ ê²ƒ)
- `--module_str`: ëª¨ë“ˆ ê²½ë¡œ í…œí”Œë¦¿ (ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜ ì§€ì›)

### PEFT Integration

LoRA/DoRA ë“± PEFTë¡œì˜ í™•ì¥ ê°€ëŠ¥:
- íŒŒë¼ë¯¸í„° ì„ íƒ ê³„ì¸µì„ LoRA ì–´ëŒ‘í„°ë¡œ ëŒ€ì²´
- `--name_keywords`ë¥¼ LoRA íŒŒë¼ë¯¸í„° ì´ë¦„ìœ¼ë¡œ ì„¤ì •
- ì˜ˆ: `--name_keywords lora_A,lora_B` (LoRA), `--name_keywords dora_A,dora_B` (DoRA)

### Comprehensive Evaluation
- **Suppression metrics**: Ï€_Î¸(S) measurement with confidence intervals
- **Utility preservation**: Perplexity on retain data
- **Trade-off analysis**: Suppression vs utility ratios
- **Automated reporting**: JSON output with all metrics

### Algorithm

1. **Build V_S**: Collect forbidden token set from forget data with frequency filtering
2. **Estimate p_S**: Compute Ï€_base(S) over V_S using frozen model (512 samples)
3. **Compute Î»**: Use closed-form mapping from Îµ and p_S
4. **Create tilted distribution**: Apply exponential tilting with direct slicing
5. **Train**: Minimize KL divergence to tilted distribution with AMP
6. **Adapt Î»**: Periodically adjust based on EMA of current suppression
7. **Evaluate**: Measure suppression and utility preservation
8. **Optional refinement**: Add NPO/DPO preference-based pairwise loss

### Performance Optimizations

#### Memory Efficiency
- **Direct tensor slicing**: `tilted_logits[..., V_S] -= lambda_val` instead of boolean masks
- **Gradient checkpointing**: Compatible with large models
- **Mixed precision**: Automatic mixed precision (AMP) support

#### Training Stability
- **Gradient clipping**: Norm clipping at 1.0
- **Learning rate scheduling**: Linear warmup with linear decay
- **EMA smoothing**: 20-step moving average for Î» updates
- **Confidence intervals**: Statistical significance reporting

#### Numerical Stability
- **Proper clamping**: Min/max bounds on probabilities
- **Log-space operations**: Stable log-softmax computations
- **Error handling**: Graceful handling of edge cases

### Evaluation and Monitoring

#### Key Metrics
- `Ï€_Î¸(S)`: Current mass on forbidden set with 95% CI
- `E[q_Î»(S)]`: Expected mass from theoretical Î»
- `KL(Ï€_Î¸ || q_Î»)`: Training error bound (KL divergence from updated model to tilted distribution)
- `Î»`: Current tilting parameter
- `Perplexity`: Utility preservation on retain data

#### Automated Reporting
```json
{
  "training_success": true,
  "training_duration_seconds": 120.5,
  "epsilon": 0.05,
  "V_S_size": 3247,
  "report": {
    "base_p_S": 0.1234,
    "updated_p_S": 0.0432,
    "suppression_ratio": 0.35,
    "perplexity_ratio": 1.12,
    "target_achieved": true
  }
}
```

### Comparison with Other Methods

| Method | Global Mass Control | Closed-form Solution | Critic/Pairs Needed | Adaptive Control | Memory Efficient |
|--------|-------------------|---------------------|-------------------|------------------|------------------|
| GA | âŒ | âŒ | âŒ | âŒ | âŒ |
| NPO | âŒ | âŒ | âœ… | âŒ | âŒ |
| RMU | âŒ | âŒ | âŒ | âŒ | âŒ |
| **ETU** | **âœ…** | **âœ…** | **âŒ** | **âœ…** | **âœ…** |

### Troubleshooting

#### Common Issues

1. **Empty V_S**: Check forget data or reduce filtering
   ```bash
   --vocab_top_k 10000  # Increase token limit
   ```

2. **Unstable training**: Reduce Î»_max and increase update frequency
   ```bash
   --lambda_max 8.0 --lambda_update_freq 50
   ```

3. **Poor suppression**: Increase lambda_eta and check V_S size
   ```bash
   --lambda_eta 0.5 --vocab_top_k 2000
   ```

4. **Utility degradation**: Increase retain_weight
   ```bash
   --retain_weight 2.0
   ```

#### Performance Tips

- Use `--verbose` for detailed monitoring
- Start with conservative `--epsilon` (0.05-0.1)
- Monitor perplexity ratio (should stay < 1.5)
- Check confidence intervals for statistical significance

## ğŸš€ **H200 GPU í™˜ê²½ ìµœì í™”**

### **ğŸ’ H200 GPU íŠ¹í™” ê¸°ëŠ¥**

ETUëŠ” NVIDIA H200 GPU í™˜ê²½ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

#### **ìë™ í™˜ê²½ ê°ì§€**
- **H200 ê°ì§€**: ìë™ìœ¼ë¡œ H200 GPU í™˜ê²½ì„ ê°ì§€í•˜ê³  ìµœì í™”ëœ ì„¤ì • ì ìš©
- **ë™ì  ì„¤ì •**: GPU í™˜ê²½ì— ë”°ë¼ batch_size, LoRA rank, frozen_on_cpu ìë™ ì¡°ì •
- **ì„±ëŠ¥ ìµœì í™”**: 143GB VRAMì„ í™œìš©í•œ ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬

#### **H200 ìµœì í™”ëœ ê¸°ë³¸ê°’**
```bash
# H200 í™˜ê²½ì—ì„œ ìë™ ì ìš©ë˜ëŠ” ì„¤ì •
--batch_size 8              # ì¼ë°˜ GPU: 4
--lora_r 512               # ì¼ë°˜ GPU: 256
--lora_alpha 1024          # ì¼ë°˜ GPU: 512
--max_num_batches 100      # ì¼ë°˜ GPU: 80
--frozen_on_cpu false      # ì¼ë°˜ GPU: true
```

### **ğŸ¯ H200 ì „ìš© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸**

#### **A. ê¸°ë³¸ ì‹¤í–‰ (ìë™ ìµœì í™”)**
```bash
# H200 í™˜ê²½ ìë™ ê°ì§€ ë° ìµœì í™”
python run_etu_h200.py

# ë˜ëŠ” ê¸°ì¡´ ë°©ì‹ (ìë™ ìµœì í™”)
python run_etu.py
```

#### **B. ê³ ê¸‰ ì‹¤í–‰ (ìˆ˜ë™ ì œì–´)**
```bash
# ë‹¨ì¼ GPU ì‚¬ìš©
python run_etu_h200.py --gpu_id 0

# ë©€í‹° GPU ì‚¬ìš©
python run_etu_h200.py --multi_gpu

# ì»¤ìŠ¤í…€ ì„¤ì •
python run_etu_h200.py --batch_size 16 --lora_r 1024
```

### **ğŸ“Š H200 í™˜ê²½ ëª¨ë‹ˆí„°ë§**

#### **ì‹¤ì‹œê°„ GPU ëª¨ë‹ˆí„°ë§**
```bash
# H200 ì „ìš© ëª¨ë‹ˆí„°ë§
python monitor_h200.py

# ì»¤ìŠ¤í…€ ê°„ê²©
python monitor_h200.py --interval 3 --log custom_monitor.log
```

#### **GPU ìƒíƒœ í™•ì¸**
```bash
# ê¸°ë³¸ GPU ì •ë³´
nvidia-smi

# ìƒì„¸ ëª¨ë‹ˆí„°ë§
nvidia-smi dmon -s pucvmet -d 1

# H200 ì „ìš© ì •ë³´
nvidia-smi --query-gpu=name,memory.total,memory.used,utilization.gpu --format=csv
```

### **ğŸ”¥ H200 ì„±ëŠ¥ ìµœì í™” íŒ**

#### **1. ë©”ëª¨ë¦¬ í™œìš© ìµœì í™”**
```bash
# H200ì˜ 143GB VRAM í™œìš©
--batch_size 8          # í° ë°°ì¹˜ í¬ê¸°
--frozen_on_cpu false   # frozen ëª¨ë¸ë„ GPUì—
--max_num_batches 100   # ë” ë§ì€ ë°°ì¹˜ ì²˜ë¦¬
```

#### **2. LoRA ì„±ëŠ¥ ìµœì í™”**
```bash
# H200 ìµœì  LoRA ì„¤ì •
--lora_r 512           # ë†’ì€ rank (ë©”ëª¨ë¦¬ ì—¬ìœ )
--lora_alpha 1024      # ë†’ì€ alpha
--use_lora true        # LoRA í™œì„±í™”
```

#### **3. ë©€í‹° GPU í™œìš©**
```bash
# GPU 0,1 ì‚¬ìš©
export CUDA_VISIBLE_DEVICES=0,1

# ë³‘ë ¬ ì²˜ë¦¬
python run_etu_h200.py --multi_gpu
```

### **ğŸ“ˆ H200 vs ì¼ë°˜ GPU ì„±ëŠ¥ ë¹„êµ**

| ì„¤ì • | H200 (143GB) | ì¼ë°˜ GPU (24GB) | ì„±ëŠ¥ í–¥ìƒ |
|------|---------------|-----------------|-----------|
| **batch_size** | 8 | 4 | 2x |
| **lora_r** | 512 | 256 | 2x |
| **max_batches** | 100 | 80 | 1.25x |
| **frozen_on_cpu** | false | true | 1.5x |
| **ì´ ì„±ëŠ¥ í–¥ìƒ** | - | - | **3-4x** |

### **ğŸ”§ H200 í™˜ê²½ ë¬¸ì œ í•´ê²°**

#### **ì¼ë°˜ì ì¸ ë¬¸ì œ**
```bash
# 1. CUDA out of memory (H200ì—ì„œëŠ” ë“œë¬¼ìŒ)
# í•´ê²°: --batch_size ì¤„ì´ê¸° (16 â†’ 8)

# 2. GPU ê°ì§€ ì‹¤íŒ¨
# í•´ê²°: nvidia-smi í™•ì¸, CUDA_VISIBLE_DEVICES ì„¤ì •

# 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ
# í•´ê²°: --frozen_on_cpu true (í•„ìš”ì‹œ)
```

#### **H200 íŠ¹í™” ë¬¸ì œ**
```bash
# 1. ê³¼ë„í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©
# í•´ê²°: --batch_size 16 ì´ìƒ ì‚¬ìš© ì‹œ ì£¼ì˜

# 2. LoRA rank ë†’ìŒ
# í•´ê²°: --lora_r 1024 ì´ìƒ ì‹œ ì•ˆì •ì„± í™•ì¸

# 3. ë©€í‹° GPU ë™ê¸°í™”
# í•´ê²°: ë‹¨ì¼ GPUë¡œ ì‹œì‘ í›„ ì ì§„ì  í™•ì¥
```
