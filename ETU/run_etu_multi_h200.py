#!/usr/bin/env python3
"""
ETU 8ëŒ€ H200 GPU ë©€í‹° ë…¸ë“œ ë³‘ë ¬ ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
ëŒ€ìš©ëŸ‰ ëª¨ë¸ (70B+) ì‹¤í—˜ ì§€ì›
"""

import os
import sys
import torch
import argparse
import subprocess
from pathlib import Path

# 8ëŒ€ H200 GPU í™˜ê²½ ìµœì í™”
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

class MultiH200ETU:
    def __init__(self):
        self.gpu_count = torch.cuda.device_count()
        self.device_names = []
        self.device_memories = []
        
        # GPU ì •ë³´ ìˆ˜ì§‘
        self._collect_gpu_info()
        
    def _collect_gpu_info(self):
        """8ëŒ€ H200 GPU ì •ë³´ ìˆ˜ì§‘"""
        print("ğŸš€ 8ëŒ€ H200 GPU í™˜ê²½ ë¶„ì„ ì¤‘...")
        
        for i in range(self.gpu_count):
            props = torch.cuda.get_device_properties(i)
            name = props.name
            memory_gb = props.total_memory / 1024**3
            
            self.device_names.append(name)
            self.device_memories.append(memory_gb)
            
            print(f"GPU {i}: {name} ({memory_gb:.1f} GB)")
        
        # H200 GPU í™•ì¸
        h200_count = sum(1 for name in self.device_names if "H200" in name)
        if h200_count != 8:
            print(f"âš ï¸  H200 GPU {h200_count}ê°œ ê°ì§€ë¨ (ì˜ˆìƒ: 8ê°œ)")
        else:
            print(f"âœ… 8ëŒ€ H200 GPU ëª¨ë‘ ê°ì§€ë¨!")
        
        total_memory = sum(self.device_memories)
        print(f"ğŸ’¾ ì´ GPU ë©”ëª¨ë¦¬: {total_memory:.1f} GB")
        
    def setup_multi_gpu_environment(self, strategy="ddp"):
        """ë©€í‹° GPU í™˜ê²½ ì„¤ì •"""
        print(f"ğŸ”§ ë©€í‹° GPU í™˜ê²½ ì„¤ì •: {strategy}")
        
        if strategy == "ddp":
            # Distributed Data Parallel
            os.environ["MASTER_ADDR"] = "localhost"
            os.environ["MASTER_PORT"] = "12355"
            os.environ["WORLD_SIZE"] = str(self.gpu_count)
            os.environ["RANK"] = "0"
            
        elif strategy == "fsdp":
            # Fully Sharded Data Parallel
            os.environ["FSDP_CONFIG"] = "true"
            os.environ["FSDP_CPU_OFFLOAD"] = "false"
            
        elif strategy == "tensor_parallel":
            # Tensor Parallelism
            os.environ["TP_CONFIG"] = "true"
            os.environ["TP_SIZE"] = str(self.gpu_count)
            
        print(f"âœ… {strategy.upper()} í™˜ê²½ ì„¤ì • ì™„ë£Œ")
        
    def get_multi_gpu_optimized_args(self):
        """8ëŒ€ H200 GPUì— ìµœì í™”ëœ ì¸ì"""
        parser = argparse.ArgumentParser(description="ETU 8ëŒ€ H200 GPU ë©€í‹° ë…¸ë“œ ì‹¤í–‰")
        
        # GPU ì „ëµ
        parser.add_argument("--strategy", type=str, default="ddp",
                          choices=["ddp", "fsdp", "tensor_parallel"],
                          help="ë©€í‹° GPU ì „ëµ (ê¸°ë³¸ê°’: ddp)")
        
        # 8ëŒ€ GPU ìµœì í™” ì„¤ì •
        parser.add_argument("--batch_size", type=int, default=64,
                          help="ì „ì²´ ë°°ì¹˜ í¬ê¸° (8ëŒ€ GPU ë¶„ì‚°, ê¸°ë³¸ê°’: 64)")
        parser.add_argument("--batch_size_per_gpu", type=int, default=8,
                          help="GPUë‹¹ ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 8)")
        parser.add_argument("--max_num_batches", type=int, default=200,
                          help="ìµœëŒ€ ë°°ì¹˜ ìˆ˜ (8ëŒ€ GPU í™œìš©, ê¸°ë³¸ê°’: 200)")
        
        # ëŒ€ìš©ëŸ‰ ëª¨ë¸ ì§€ì›
        parser.add_argument("--model_name_or_path", type=str,
                          default="meta-llama/Llama-2-70b-hf",
                          help="ëŒ€ìš©ëŸ‰ ëª¨ë¸ ê²½ë¡œ (ê¸°ë³¸ê°’: Llama-2-70b)")
        parser.add_argument("--use_lora", action="store_true", default=True,
                          help="LoRA ì‚¬ìš© (ëŒ€ìš©ëŸ‰ ëª¨ë¸ í•„ìˆ˜)")
        parser.add_argument("--lora_r", type=int, default=1024,
                          help="LoRA rank (8ëŒ€ GPU í™œìš©, ê¸°ë³¸ê°’: 1024)")
        parser.add_argument("--lora_alpha", type=int, default=2048,
                          help="LoRA alpha (ê¸°ë³¸ê°’: 2048)")
        
        # ETU í•µì‹¬ íŒŒë¼ë¯¸í„°
        parser.add_argument("--epsilon", type=float, default=0.05,
                          help="ì–µì œ ëª©í‘œ Îµ (ê¸°ë³¸ê°’: 0.05)")
        parser.add_argument("--lambda_max", type=float, default=15.0,
                          help="ìµœëŒ€ Î» ê°’ (8ëŒ€ GPU í™œìš©, ê¸°ë³¸ê°’: 15.0)")
        parser.add_argument("--lambda_update_freq", type=int, default=50,
                          help="Î» ì—…ë°ì´íŠ¸ ë¹ˆë„ (ê¸°ë³¸ê°’: 50)")
        
        # ë°ì´í„° ì„¤ì •
        parser.add_argument("--forget_corpora", type=str,
                          default="cais/wmdp-corpora:cyber-forget-corpus",
                          help="forgetí•  ë„ë©”ì¸ (ë³„ì¹­: bio:forget, cyber:forget, ë˜ëŠ” ì‹¤ì œ ê²½ë¡œ)")
        parser.add_argument("--retain_corpora", type=str,
                          default="cais/wmdp-corpora:bio-retain-corpus",
                          help="retainí•  ë„ë©”ì¸ (ë³„ì¹­: bio:retain, cyber:retain, wikitext, ë˜ëŠ” ì‹¤ì œ ê²½ë¡œ)")
        
        # ì„±ëŠ¥ ìµœì í™”
        parser.add_argument("--gradient_accumulation_steps", type=int, default=4,
                          help="ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í… (ê¸°ë³¸ê°’: 4)")
        parser.add_argument("--mixed_precision", type=str, default="bf16",
                          choices=["fp16", "bf16", "fp32"],
                          help="í˜¼í•© ì •ë°€ë„ (ê¸°ë³¸ê°’: bf16)")
        parser.add_argument("--deterministic", action="store_true",
                          help="ê²°ì •ì  ì‹¤í–‰")
        parser.add_argument("--verbose", action="store_true", default=True,
                          help="ìƒì„¸ ë¡œê¹…")
        
        return parser.parse_args()
    
    def calculate_optimal_batch_size(self, model_size_gb):
        """ëª¨ë¸ í¬ê¸°ì— ë”°ë¥¸ ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
        total_gpu_memory = sum(self.device_memories)
        available_memory = total_gpu_memory * 0.8  # 80% ì‚¬ìš©
        
        # ëª¨ë¸ í¬ê¸°ë³„ ìµœì  ë°°ì¹˜ í¬ê¸°
        if model_size_gb <= 7:
            optimal_batch = 128  # 7B ëª¨ë¸
        elif model_size_gb <= 13:
            optimal_batch = 64   # 13B ëª¨ë¸
        elif model_size_gb <= 30:
            optimal_batch = 32   # 30B ëª¨ë¸
        elif model_size_gb <= 70:
            optimal_batch = 16   # 70B ëª¨ë¸
        else:
            optimal_batch = 8    # 70B+ ëª¨ë¸
        
        # GPU ë©”ëª¨ë¦¬ ì œì•½ ê³ ë ¤
        memory_constrained_batch = int(available_memory / (model_size_gb * 2))
        final_batch = min(optimal_batch, memory_constrained_batch)
        
        return final_batch
    
    def run_multi_gpu_etu(self):
        """8ëŒ€ H200 GPUë¡œ ETU ì‹¤í–‰"""
        try:
            # ì¸ì íŒŒì‹±
            args = self.get_multi_gpu_optimized_args()
            
            # ë©€í‹° GPU í™˜ê²½ ì„¤ì •
            self.setup_multi_gpu_environment(args.strategy)
            
            # ëª¨ë¸ í¬ê¸° ì¶”ì • (ëŒ€ëµì )
            if "70b" in args.model_name_or_path.lower():
                estimated_size = 70
            elif "30b" in args.model_name_or_path.lower():
                estimated_size = 30
            elif "13b" in args.model_name_or_path.lower():
                estimated_size = 13
            else:
                estimated_size = 7
            
            # ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°
            optimal_batch = self.calculate_optimal_batch_size(estimated_size)
            args.batch_size = optimal_batch
            args.batch_size_per_gpu = optimal_batch // self.gpu_count
            
            print(f"ğŸ“Š 8ëŒ€ H200 GPU ìµœì í™” ì„¤ì •:")
            print(f"   - ì „ëµ: {args.strategy.upper()}")
            print(f"   - ì „ì²´ ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
            print(f"   - GPUë‹¹ ë°°ì¹˜ í¬ê¸°: {args.batch_size_per_gpu}")
            print(f"   - LoRA rank: {args.lora_r}")
            print(f"   - ëª¨ë¸ í¬ê¸°: ~{estimated_size}B")
            print(f"   - ì´ GPU ë©”ëª¨ë¦¬: {sum(self.device_memories):.1f} GB")
            print()
            
            # ETU ì‹¤í–‰
            from etu.unlearn import run_etu
            
            print("ğŸš€ 8ëŒ€ H200 GPUë¡œ ETU ì‹¤í–‰ ì‹œì‘...")
            run_etu(args)
            
            print("âœ… 8ëŒ€ H200 GPU ETU ì‹¤í–‰ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=== ETU 8ëŒ€ H200 GPU ë©€í‹° ë…¸ë“œ ì‹¤í–‰ ===")
    
    # ë©€í‹° H200 ETU ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    multi_etu = MultiH200ETU()
    
    # 8ëŒ€ GPU ETU ì‹¤í–‰
    multi_etu.run_multi_gpu_etu()

if __name__ == "__main__":
    main() 