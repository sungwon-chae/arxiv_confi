#!/usr/bin/env bash
# ν•μ΄νΌνλΌλ―Έν„° μ¤μ• μλ™ν™” (H200 GPU μµμ ν™”, Zephyr-7B)
set -euo pipefail

echo "=== Hyperparameter Sweep (H200 GPU μµμ ν™”) ==="
date

# ----- GPU μ”μ•½ -----
GPU_NAMES=$(nvidia-smi --query-gpu=name --format=csv,noheader,nounits || true)
GPU_COUNT=$(echo "${GPU_NAMES:-}" | wc -l | awk '{print $1}')
H200_COUNT=$(echo "${GPU_NAMES:-}" | grep -c "H200" || true)
echo "GPU Summary:"
echo " - μ΄ GPU: ${GPU_COUNT}"
echo " - H200 GPU: ${H200_COUNT}"
echo " - μ²« GPU: $(echo "${GPU_NAMES:-}" | head -1)"

# ----- κΈ°λ³Έ νλΌλ―Έν„° (H200 μµμ ν™”) -----
if [[ "${H200_COUNT}" -ge 1 ]]; then
  echo "π€ H200 GPU ν™κ²½ κ°μ§€λ¨ - μµμ ν™”λ μ„¤μ • μ‚¬μ©"
  BATCH_SIZE=64
  LORA_R=512
  LORA_ALPHA=1024
  MAX_BATCHES=500
  FROZEN_ON_CPU=true       # λ©”λ¨λ¦¬ μ μ•½μ„ μ„ν•΄ true
  STRATEGY="ddp"
else
  echo "β οΈ  H200 GPUκ°€ μ•„λ‹ - λ³΄μμ  μ„¤μ • μ‚¬μ©"
  BATCH_SIZE=8
  LORA_R=256
  LORA_ALPHA=512
  MAX_BATCHES=80
  FROZEN_ON_CPU=true
  STRATEGY="ddp"
fi

echo "π“ κΈ°λ³Έ μ„¤μ •:"
echo " - strategy: ${STRATEGY}"
echo " - batch_size: ${BATCH_SIZE}"
echo " - lora_r: ${LORA_R}"
echo " - lora_alpha: ${LORA_ALPHA}"
echo " - max_num_batches: ${MAX_BATCHES}"
echo " - frozen_on_cpu: ${FROZEN_ON_CPU}"
echo ""

# ----- κ³µν†µ κ²½λ΅/λ°μ² -----
MODEL_ID="HuggingFaceH4/zephyr-7b-beta"
FORGET_DIR="./datasets/cyber-forget"
RETAIN_DIR="./datasets/bio-retain"
OUT_ROOT="sweep_results/zephyr_7b"
LOG_ROOT="${OUT_ROOT}/logs"
mkdir -p "${OUT_ROOT}" "${LOG_ROOT}"

# λ°μ΄ν„°μ…‹ ν΄λ°± (μ—†μΌλ©΄ HF λ ν¬λ΅ μ „ν™)
if [[ ! -d "${FORGET_DIR}" ]] || [[ ! -d "${RETAIN_DIR}" ]]; then
  echo "β„ΉοΈ λ΅μ»¬ λ°μ΄ν„°μ…‹μ΄ μ—†μ–΄μ„ HF λ°μ΄ν„°μ…‹μΌλ΅ λ€μ²΄ν•©λ‹λ‹¤."
  FORGET_DIR="cais/wmdp-corpora:cyber-forget-corpus"
  RETAIN_DIR="cais/wmdp-corpora:bio-retain-corpus"
fi

# λ°μ² (λ©€ν‹° GPUλ©΄ torchrun)
if [[ "${GPU_COUNT}" -ge 2 ]]; then
  LAUNCHER=(torchrun --standalone --nproc_per_node="${GPU_COUNT}")
else
  LAUNCHER=(python)
fi

# λ°νƒ€μ„ κ¶μ¥ env
export NCCL_P2P_DISABLE=0
export NCCL_IB_DISABLE=0
export TORCH_NCCL_BLOCKING_WAIT=1
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256
export HF_HOME="${HF_HOME:-$HOME/.cache/huggingface}"

# ----- κ³µν†µ μΈμ λ°°μ—΄ (λ°λ“μ‹ λ¨λ“  μ¤μ•μ— ν¬ν•¨) -----
COMMON_ARGS=(
  --model_name_or_path "${MODEL_ID}"
  --forget_corpora "${FORGET_DIR}"
  --retain_corpora "${RETAIN_DIR}"
  --batch_size "${BATCH_SIZE}"
  --max_num_batches "${MAX_BATCHES}"
  --use_lora
  --lora_r "${LORA_R}"
  --lora_alpha "${LORA_ALPHA}"
  --strategy "${STRATEGY}"
  --frozen_on_cpu
  --seed 42
  --verbose
)

# ----- 0) λ² μ΄μ¤ λ° -----
RUN_ID="base_$(date +%Y%m%d_%H%M%S)"
OUT_DIR="${OUT_ROOT}/${RUN_ID}"
mkdir -p "${OUT_DIR}"
echo "=== Base Run μ‹μ‘ ==="
"${LAUNCHER[@]}" run_etu_h200.py \
  "${COMMON_ARGS[@]}" \
  --epsilon 0.05 \
  --lambda_max 30.0 \
  --output_dir "${OUT_DIR}" 2>&1 | tee "${LOG_ROOT}/${RUN_ID}.log"

# ----- 1) Epsilon μ¤μ• -----
echo "=== Epsilon μ¤μ• μ‹μ‘ ==="
for epsilon in 0.01 0.05 0.1 0.2; do
  RUN_ID="eps_${epsilon}_$(date +%H%M%S)"
  OUT_DIR="${OUT_ROOT}/${RUN_ID}"
  mkdir -p "${OUT_DIR}"
  echo "  -> epsilon=${epsilon}"
  "${LAUNCHER[@]}" run_etu_h200.py \
    "${COMMON_ARGS[@]}" \
    --epsilon "${epsilon}" \
    --lambda_max 30.0 \
    --output_dir "${OUT_DIR}" 2>&1 | tee "${LOG_ROOT}/${RUN_ID}.log"
done

# ----- 2) lambda_max μ¤μ• -----
echo "=== lambda_max μ¤μ• μ‹μ‘ ==="
for lambda_max in 8.0 12.0 15.0 20.0; do
  RUN_ID="lmax_${lambda_max}_$(date +%H%M%S)"
  OUT_DIR="${OUT_ROOT}/${RUN_ID}"
  mkdir -p "${OUT_DIR}"
  echo "  -> lambda_max=${lambda_max}"
  "${LAUNCHER[@]}" run_etu_h200.py \
    "${COMMON_ARGS[@]}" \
    --epsilon 0.05 \
    --lambda_max "${lambda_max}" \
    --output_dir "${OUT_DIR}" 2>&1 | tee "${LOG_ROOT}/${RUN_ID}.log"
done

# ----- 3) lambda_eta μ¤μ• (μ§€μ› μ‹λ§) -----
echo "=== lambda_eta μ¤μ• μ‹μ‘ ==="
for lambda_eta in 0.1 0.25 0.5 1.0; do
  RUN_ID="leta_${lambda_eta}_$(date +%H%M%S)"
  OUT_DIR="${OUT_ROOT}/${RUN_ID}"
  mkdir -p "${OUT_DIR}"
  echo "  -> lambda_eta=${lambda_eta}"
  "${LAUNCHER[@]}" run_etu_h200.py \
    "${COMMON_ARGS[@]}" \
    --epsilon 0.05 \
    --lambda_max 30.0 \
    --lambda_eta "${lambda_eta}" \
    --output_dir "${OUT_DIR}" 2>&1 | tee "${LOG_ROOT}/${RUN_ID}.log"
done

# ----- 4) LoRA rank μ¤μ• (H200 ν™κ²½μ—μ„λ§) -----
if [[ "${FROZEN_ON_CPU}" == "false" ]]; then
  echo "=== LoRA rank μ¤μ• μ‹μ‘ (H200) ==="
  for rank in 128 256 512 1024; do
    RUN_ID="lora_r_${rank}_$(date +%H%M%S)"
    OUT_DIR="${OUT_ROOT}/${RUN_ID}"
    mkdir -p "${OUT_DIR}"
    echo "  -> lora_r=${rank}, lora_alpha=$((rank * 2))"
    "${LAUNCHER[@]}" run_etu_h200.py \
      "${COMMON_ARGS[@]}" \
      --epsilon 0.05 \
      --lambda_max 30.0 \
      --lora_r "${rank}" \
      --lora_alpha "$((rank * 2))" \
      --output_dir "${OUT_DIR}" 2>&1 | tee "${LOG_ROOT}/${RUN_ID}.log"
  done
fi

# ----- 5) Batch size μ¤μ• (H200 ν™κ²½μ—μ„λ§) -----
if [[ "${FROZEN_ON_CPU}" == "false" ]]; then
  echo "=== Batch size μ¤μ• μ‹μ‘ (H200) ==="
  for bs in 4 8 16 32; do
    RUN_ID="bs_${bs}_$(date +%H%M%S)"
    OUT_DIR="${OUT_ROOT}/${RUN_ID}"
    mkdir -p "${OUT_DIR}"
    echo "  -> batch_size=${bs}"
    "${LAUNCHER[@]}" run_etu_h200.py \
      "${COMMON_ARGS[@]}" \
      --epsilon 0.05 \
      --lambda_max 30.0 \
      --batch_size "${bs}" \
      --output_dir "${OUT_DIR}" 2>&1 | tee "${LOG_ROOT}/${RUN_ID}.log"
  done
fi

echo "=== λ¨λ“  ν•μ΄νΌνλΌλ―Έν„° μ¤μ• μ™„λ£ ==="
date
echo "κ²°κ³Ό: ${OUT_ROOT}/  (λ΅κ·Έ: ${LOG_ROOT}/)"
