#!/usr/bin/env bash
# ETU ë…¼ë¬¸ ì‹¤í—˜ ìë™í™” ìŠ¤í¬ë¦½íŠ¸ (H200 GPU ìµœì í™”, Zephyr-7B)
set -euo pipefail

echo "=== ETU Paper Experiments (H200 GPU ìµœì í™”) ==="
date

# -------- GPU í™˜ê²½ ìš”ì•½ --------
GPU_NAMES=$(nvidia-smi --query-gpu=name --format=csv,noheader,nounits || true)
GPU_COUNT=$(echo "${GPU_NAMES:-}" | wc -l | awk '{print $1}')
H200_COUNT=$(echo "${GPU_NAMES:-}" | grep -c "H200" || true)
echo "GPU Summary:"
echo " - ì´ GPU: ${GPU_COUNT}"
echo " - H200 GPU: ${H200_COUNT}"
echo " - ì²« GPU: $(echo "${GPU_NAMES:-}" | head -1)"

# -------- H200 ìµœì í™” íŒŒë¼ë¯¸í„° --------
if [[ "${H200_COUNT}" -ge 1 ]]; then
  echo "ğŸš€ H200 GPU í™˜ê²½ ê°ì§€ë¨ - ìµœì í™”ëœ ì„¤ì • ì‚¬ìš©"
  BATCH_SIZE=64            # H200 ê¸°ì¤€ ëŒ€ê·œëª¨ ì‹¤í—˜ (7B + LoRA)
  LORA_R=512
  LORA_ALPHA=1024
  MAX_BATCHES=500
  FROZEN_ON_CPU=true       # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ true
  STRATEGY="ddp"           # run_etu_h200.pyê°€ ì§€ì›í•˜ë©´ ì „ë‹¬
else
  echo "âš ï¸  H200 GPUê°€ ì•„ë‹˜ - ë³´ìˆ˜ì  ì„¤ì • ì‚¬ìš©"
  BATCH_SIZE=8
  LORA_R=256
  LORA_ALPHA=512
  MAX_BATCHES=80
  FROZEN_ON_CPU=true
  STRATEGY="ddp"
fi

echo "ğŸ“Š ìµœì í™” ì„¤ì •:"
echo " - strategy: ${STRATEGY}"
echo " - batch_size: ${BATCH_SIZE}"
echo " - lora_r: ${LORA_R}"
echo " - lora_alpha: ${LORA_ALPHA}"
echo " - max_num_batches: ${MAX_BATCHES}"
echo " - frozen_on_cpu: ${FROZEN_ON_CPU}"
echo ""

# -------- ê²½ë¡œ/ì¶œë ¥ ì¤€ë¹„ --------
MODEL_ID="HuggingFaceH4/zephyr-7b-beta"
FORGET_DIR="./datasets/cyber-forget"
RETAIN_DIR="./datasets/bio-retain"
OUT_DIR="paper_results/zephyr_7b"
LOG_DIR="${OUT_DIR}/logs"
mkdir -p "${OUT_DIR}" "${LOG_DIR}"

# ë°ì´í„°ì…‹ ì¡´ì¬ í™•ì¸ (ì—†ìœ¼ë©´ HF IDë¡œ ëŒ€ì²´ ì˜ˆì‹œ)
if [[ ! -d "${FORGET_DIR}" ]] || [[ ! -d "${RETAIN_DIR}" ]]; then
  echo "â„¹ï¸ ë¡œì»¬ ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ê°€ ì—†ì–´ HF ë°ì´í„°ì…‹ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤."
  FORGET_DIR="cais/wmdp-corpora:cyber-forget-corpus"
  RETAIN_DIR="cais/wmdp-corpora:bio-retain-corpus"
fi

# -------- ëŸ°íƒ€ì„ í™˜ê²½ ë³€ìˆ˜(ê¶Œì¥) --------
export NCCL_P2P_DISABLE=0
export NCCL_IB_DISABLE=0
export TORCH_NCCL_BLOCKING_WAIT=1
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256
export HF_HOME="${HF_HOME:-$HOME/.cache/huggingface}"

# ë‹¨ì¼ GPU ì‹¤í–‰
LAUNCHER="python3"
export CUDA_VISIBLE_DEVICES=1 #1ë²ˆë§Œ ì¼ë‹¨ ì‚¬ìš©

# -------- ì‹¤í—˜ ì‹¤í–‰ --------
echo "=== Zephyr-7B ETU ì‹¤í—˜ ì‹œì‘ ==="
${LAUNCHER} run_etu_h200.py \
  --model_name_or_path "${MODEL_ID}" \
  --epsilon 0.05 \
  --lambda_max 30.0 \
  --batch_size "${BATCH_SIZE}" \
  --max_num_batches "${MAX_BATCHES}" \
  --lora_r "${LORA_R}" \
  --lora_alpha "${LORA_ALPHA}" \
  --forget_corpora "${FORGET_DIR}" \
  --retain_corpora "${RETAIN_DIR}" \
  --strategy "${STRATEGY}" \
  --frozen_on_cpu \
  --output_dir "${OUT_DIR}" \
  --verbose 2>&1 | tee "${LOG_DIR}/zephyr_7b_$(date +%Y%m%d_%H%M%S).log"

echo "All experiments completed at: $(date)"
echo "Results saved in ${OUT_DIR}/ and models/ (ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì €ì¥ ìœ„ì¹˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ êµ¬í˜„ì— ë”°ë¦„)"
