#!/usr/bin/env python3
"""
ETU H200 GPU ì „ìš© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
NVIDIA H200 143GB VRAM í™˜ê²½ì— ìµœì í™”
"""

import os
import sys
import torch
import argparse

# H200 GPU í™˜ê²½ ìµœì í™”
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"  # ë””ë²„ê¹…ìš©

# 8ëŒ€ H200 GPU í™˜ê²½ ìµœì í™” (run_etu_multi_h200.pyì—ì„œ ê°€ì ¸ì˜´)
os.environ["NCCL_P2P_DISABLE"] = "0"
os.environ["NCCL_IB_DISABLE"] = "0"
os.environ["TORCH_NCCL_BLOCKING_WAIT"] = "1"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:256,expandable_segments:True"

def setup_h200_environment():
    """H200 GPU í™˜ê²½ ì„¤ì • ë° ê²€ì¦"""
    print("ğŸš€ H200 GPU í™˜ê²½ ì„¤ì • ì¤‘...")
    
    if not torch.cuda.is_available():
        raise RuntimeError("CUDA GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    gpu_count = torch.cuda.device_count()
    if gpu_count == 0:
        raise RuntimeError("ì‚¬ìš© ê°€ëŠ¥í•œ GPUê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # GPU ì •ë³´ ì¶œë ¥
    for i in range(gpu_count):
        props = torch.cuda.get_device_properties(i)
        memory_gb = props.total_memory / 1024**3
        print(f"GPU {i}: {props.name} ({memory_gb:.1f} GB)")
    
    # H200 GPU í™•ì¸
    h200_gpus = []
    for i in range(gpu_count):
        if "H200" in torch.cuda.get_device_name(i):
            h200_gpus.append(i)
    
    if not h200_gpus:
        print("âš ï¸  H200 GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ GPU ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return False
    
    print(f"âœ… H200 GPU {len(h200_gpus)}ê°œ ê°ì§€ë¨")
    return True

def setup_multi_gpu_environment(strategy="ddp"):
    """ë©€í‹° GPU í™˜ê²½ ì„¤ì • (run_etu_multi_h200.pyì—ì„œ ê°€ì ¸ì˜´)"""
    print(f"ğŸ”§ ë©€í‹° GPU í™˜ê²½ ì„¤ì •: {strategy}")
    
    if strategy == "ddp":
        # Distributed Data Parallel
        os.environ["MASTER_ADDR"] = "localhost"
        os.environ["MASTER_PORT"] = "12355"
        os.environ["WORLD_SIZE"] = str(torch.cuda.device_count())
        os.environ["RANK"] = "0"
        
    elif strategy == "fsdp":
        # Fully Sharded Data Parallel
        os.environ["FSDP_CONFIG"] = "true"
        os.environ["FSDP_CPU_OFFLOAD"] = "false"
        
    elif strategy == "tensor_parallel":
        # Tensor Parallelism
        os.environ["TP_CONFIG"] = "true"
        os.environ["TP_SIZE"] = str(torch.cuda.device_count())
        
    print(f"âœ… {strategy.upper()} í™˜ê²½ ì„¤ì • ì™„ë£Œ")

def get_dataset_mapping():
    """ë°ì´í„°ì…‹ ë³„ì¹­ì„ ì‹¤ì œ ê²½ë¡œë¡œ ë§¤í•‘"""
    return {
        # HuggingFace ê²½ë¡œ
        "bio:forget": "cais/wmdp-bio-forget-corpus",
        "bio:retain": "cais/wmdp-corpora:bio-retain-corpus", 
        "cyber:forget": "cais/wmdp-corpora:cyber-forget-corpus",
        "cyber:retain": "cais/wmdp-corpora:cyber-retain-corpus",
        "wikitext": "wikitext",
        
        # ë¡œì»¬ ê²½ë¡œ
        "local:cyber-forget": "./datasets/cyber-forget",
        "local:cyber-retain": "./datasets/cyber-retain",
        "local:bio-forget": "./datasets/bio-forget",
        "local:bio-retain": "./datasets/bio-retain",
        "local:wikitext": "./datasets/wikitext",
    }

def calculate_optimal_batch_size(model_name_or_path):
    """ëª¨ë¸ í¬ê¸°ì— ë”°ë¥¸ ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚° (run_etu_multi_h200.pyì—ì„œ ê°€ì ¸ì˜´)"""
    gpu_count = torch.cuda.device_count()
    if gpu_count == 0:
        return 8  # ê¸°ë³¸ê°’
    
    # GPU ë©”ëª¨ë¦¬ ì •ë³´ ìˆ˜ì§‘
    device_memories = []
    for i in range(gpu_count):
        props = torch.cuda.get_device_properties(i)
        memory_gb = props.total_memory / 1024**3
        device_memories.append(memory_gb)
    
    total_gpu_memory = sum(device_memories)
    available_memory = total_gpu_memory * 0.8  # 80% ì‚¬ìš©
    
    # ëª¨ë¸ í¬ê¸°ë³„ ìµœì  ë°°ì¹˜ í¬ê¸°
    if "70b" in model_name_or_path.lower():
        estimated_size = 70
        optimal_batch = 16   # 70B ëª¨ë¸
    elif "30b" in model_name_or_path.lower():
        estimated_size = 30
        optimal_batch = 32   # 30B ëª¨ë¸
    elif "13b" in model_name_or_path.lower():
        estimated_size = 13
        optimal_batch = 64   # 13B ëª¨ë¸
    elif "7b" in model_name_or_path.lower():
        estimated_size = 7
        optimal_batch = 128  # 7B ëª¨ë¸
    else:
        estimated_size = 7
        optimal_batch = 128  # ê¸°ë³¸ê°’
    
    # GPU ë©”ëª¨ë¦¬ ì œì•½ ê³ ë ¤
    memory_constrained_batch = int(available_memory / (estimated_size * 2))
    final_batch = min(optimal_batch, memory_constrained_batch)
    
    print(f"[batch heuristic] optimal={optimal_batch}, mem_clamp={memory_constrained_batch}, final={final_batch}")
    return final_batch

def resolve_dataset_paths(forget_corpora, retain_corpora):
    """ë°ì´í„°ì…‹ ë³„ì¹­ì„ ì‹¤ì œ ê²½ë¡œë¡œ ë³€í™˜"""
    mapping = get_dataset_mapping()
    
    def resolve_corpus(corpus):
        if corpus in mapping:
            return mapping[corpus]
        return corpus
    
    # ë‹¨ì¼ ë¬¸ìì—´ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    if isinstance(forget_corpora, str):
        if "," in forget_corpora:
            # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì—¬ëŸ¬ ë°ì´í„°ì…‹
            forget_paths = [resolve_corpus(c.strip()) for c in forget_corpora.split(",")]
        else:
            # ë‹¨ì¼ ë°ì´í„°ì…‹
            forget_paths = [resolve_corpus(forget_corpora.strip())]
    else:
        # ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
        forget_paths = [resolve_corpus(c.strip()) for c in forget_corpora]
    
    if isinstance(retain_corpora, str):
        if "," in retain_corpora:
            # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì—¬ëŸ¬ ë°ì´í„°ì…‹
            retain_paths = [resolve_corpus(c.strip()) for c in retain_corpora.split(",")]
        else:
            # ë‹¨ì¼ ë°ì´í„°ì…‹
            retain_paths = [resolve_corpus(retain_corpora.strip())]
    else:
        # ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
        retain_paths = [resolve_corpus(c.strip()) for c in retain_corpora]
    
    return forget_paths, retain_paths  # ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

def get_h200_optimized_args():
    """H200 í™˜ê²½ì— ìµœì í™”ëœ ê¸°ë³¸ ì¸ì"""
    parser = argparse.ArgumentParser(description="ETU H200 GPU ìµœì í™” ì‹¤í–‰")
    
    # GPU ì„ íƒ ë° ì „ëµ
    parser.add_argument("--gpu_id", type=int, default=0, 
                       help="ì‚¬ìš©í•  GPU ID (ê¸°ë³¸ê°’: 0)")
    parser.add_argument("--multi_gpu", action="store_true",
                       help="ì—¬ëŸ¬ GPU ì‚¬ìš© (ë³‘ë ¬ ì²˜ë¦¬)")
    parser.add_argument("--strategy", type=str, default="ddp",
                       choices=["ddp", "fsdp", "tensor_parallel"],
                       help="ë©€í‹° GPU ì „ëµ (ê¸°ë³¸ê°’: ddp)")
    parser.add_argument("--batch_size_per_gpu", type=int, default=8,
                       help="GPUë‹¹ ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 8)")
    
    # H200 ìµœì í™” ì„¤ì •
    parser.add_argument("--batch_size", type=int, default=64,
                       help="ë°°ì¹˜ í¬ê¸° (H200 ê¶Œì¥: 64, ëŒ€ê·œëª¨ ì‹¤í—˜)")
    parser.add_argument("--max_num_batches", type=int, default=500,
                       help="ìµœëŒ€ ë°°ì¹˜ ìˆ˜ (H200 ê¶Œì¥: 500, ëŒ€ê·œëª¨ ì‹¤í—˜)")
    parser.add_argument("--frozen_on_cpu", action="store_true", default=False,
                       help="frozen ëª¨ë¸ì„ CPUì— (H200 ë©”ëª¨ë¦¬ ë„‰ë„‰í•˜ë©´ False ê¶Œì¥)")
    
    # LoRA ìµœì í™”
    parser.add_argument("--use_lora", action="store_true", default=True,
                       help="LoRA ì‚¬ìš© (H200 ê¶Œì¥: true)")
    parser.add_argument("--lora_r", type=int, default=512,
                       help="LoRA rank (H200 ê¶Œì¥: 512)")
    parser.add_argument("--lora_alpha", type=int, default=1024,
                       help="LoRA alpha (H200 ê¶Œì¥: 1024)")
    
    # ETU í•µì‹¬ íŒŒë¼ë¯¸í„°
    parser.add_argument("--epsilon", type=float, default=0.05,
                       help="ì–µì œ ëª©í‘œ Îµ (ê¸°ë³¸ê°’: 0.05)")
    parser.add_argument("--lambda_max", type=float, default=30.0,
                       help="ìµœëŒ€ Î» ê°’ (ê¸°ë³¸ê°’: 30.0, ê°•í•œ ì–µì œë¥¼ ìœ„í•´)")
    parser.add_argument("--lambda_update_freq", type=int, default=1,
                       help="Î» ì—…ë°ì´íŠ¸ ë¹ˆë„ (ê¸°ë³¸ê°’: 1: ë§¤ ìŠ¤í…)")
    
    # ë°ì´í„° ì„¤ì •
    parser.add_argument("--forget_corpora", type=str, 
                       default="./datasets/cyber-forget",
                       help="forgetí•  ë„ë©”ì¸ (ë³„ì¹­: bio:forget, cyber:forget, ë˜ëŠ” ì‹¤ì œ ê²½ë¡œ)")
    parser.add_argument("--retain_corpora", type=str,
                       default="./datasets/bio-retain",
                       help="retainí•  ë„ë©”ì¸ (ë³„ì¹­: bio:retain, cyber:retain, wikitext, ë˜ëŠ” ì‹¤ì œ ê²½ë¡œ)")
    
    # ëª¨ë¸ ì„¤ì •
    parser.add_argument("--model_name_or_path", type=str,
                       default="HuggingFaceH4/zephyr-7b-beta",
                       help="ì‚¬ìš©í•  ëª¨ë¸")
    
    # ì„±ëŠ¥ ìµœì í™”
    parser.add_argument("--deterministic", action="store_true",
                       help="ê²°ì •ì  ì‹¤í–‰ (ì„±ëŠ¥ ì•½ê°„ í•˜ë½)")
    parser.add_argument("--verbose", action="store_true", default=True,
                       help="ìƒì„¸ ë¡œê¹…")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=4,
                       help="ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í… (ê¸°ë³¸ê°’: 4)")
    parser.add_argument("--mixed_precision", type=str, default="bf16",
                       choices=["fp16", "bf16", "fp32"],
                       help="í˜¼í•© ì •ë°€ë„ (ê¸°ë³¸ê°’: bf16)")
    parser.add_argument("--trust_remote_code", action="store_true",
                       help="ì›ê²© ì½”ë“œ ì‹ ë¢° (ëŒ€ìš©ëŸ‰ ëª¨ë¸ìš©)")
    
    # ì¶”ê°€ ETU ì¸ìë“¤
    parser.add_argument("--lr", type=float, default=1e-5,
                       help="í•™ìŠµë¥  (ê¸°ë³¸ê°’: 1e-5)")
    parser.add_argument("--num_epochs", type=int, default=3,
                       help="ì—í¬í¬ ìˆ˜ (ê¸°ë³¸ê°’: 3, ìˆ˜ë ´ì„ ìœ„í•´)")
    parser.add_argument("--min_len", type=int, default=10,
                       help="ìµœì†Œ ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸ê°’: 10)")
    parser.add_argument("--max_len", type=int, default=512,
                       help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸ê°’: 512)")
    
    # LoRA ê´€ë ¨ ì¸ìë“¤
    parser.add_argument("--layer_id", type=int, default=None,
                       help="ë‹¨ì¼ ë ˆì´ì–´ ID (ì§€ì • ì‹œ layer_idsë¥¼ ëŒ€ì²´)")
    parser.add_argument("--layer_ids", type=str, default="5,6,7",
                       help="ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë ˆì´ì–´ IDs")
    parser.add_argument("--param_ids", type=str, default="",
                       help="LoRA ì ìš©í•  íŒŒë¼ë¯¸í„° ID (ì‰¼í‘œë¡œ êµ¬ë¶„)")
    parser.add_argument("--name_keywords", type=str, default="q_proj,k_proj,v_proj,o_proj",
                       help="LoRA ì ìš©í•  ëª¨ë“ˆ ì´ë¦„ í‚¤ì›Œë“œ (ê¸°ë³¸ê°’: q_proj,k_proj,v_proj,o_proj)")
    parser.add_argument("--module_str", type=str, default="{model_name}.model.layers[{layer_id}]",
                       help="LoRA ì ìš©í•  ëª¨ë“ˆ ë¬¸ìì—´ (ê¸°ë³¸ê°’: {model_name}.model.layers[{layer_id}])")
    
    # LoRA options
    parser.add_argument("--use_lora", action=argparse.BooleanOptionalAction, default=True,
                       help="Use LoRA adapters (default: enabled). Use --no-use-lora to disable.")
    parser.add_argument("--lora_r", type=int, default=256, help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=512, help="LoRA scaling factor")
    parser.add_argument("--lora_dropout", type=float, default=0.1, help="LoRA dropout rate")
    parser.add_argument("--lora_target_modules", type=str, 
                       default="q_proj,k_proj,v_proj,o_proj", 
                       help="Target modules for LoRA (comma-separated)")
    
    # V_S ê´€ë ¨ ì¸ìë“¤
    parser.add_argument("--use_pmi_vs", action="store_true", default=True,
                       help="PMI ê¸°ë°˜ V_S ì‚¬ìš© (ê¸°ë³¸ True)")
    parser.add_argument("--vocab_top_k", type=int, default=1000,
                       help="V_Sì— í¬í•¨í•  ìƒìœ„ í† í° ìˆ˜ (ê¸°ë³¸ê°’: 1000)")
    parser.add_argument("--vs_freq_rate", type=float, default=0.1,
                       help="V_S ë¹ˆë„ ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.1)")
    parser.add_argument("--vs_abs_cap", type=int, default=1000,
                       help="V_S ì ˆëŒ€ ìƒí•œ (ê¸°ë³¸ê°’: 1000)")
    parser.add_argument("--pmi_top_k", type=int, default=1000,
                       help="PMI ìƒìœ„ k í† í° (ê¸°ë³¸ê°’: 1000)")
    parser.add_argument("--pmi_min_count", type=int, default=10,
                       help="PMI ìµœì†Œ ì¹´ìš´íŠ¸ (ê¸°ë³¸ê°’: 10)")
    parser.add_argument("--pmi_smoothing", type=float, default=0.1,
                       help="PMI ìŠ¤ë¬´ë”© (ê¸°ë³¸ê°’: 0.1)")
    parser.add_argument("--pmi_max_batches", type=int, default=500,
                       help="PMI ìµœëŒ€ ë°°ì¹˜ ìˆ˜ (ê¸°ë³¸ê°’: 500, ëŒ€ê·œëª¨ ì‹¤í—˜)")
    parser.add_argument("--vs_preview_k", type=int, default=10,
                       help="V_S ë¯¸ë¦¬ë³´ê¸° í† í° ìˆ˜ (ê¸°ë³¸ê°’: 10)")
    
    # Span ë§ˆìŠ¤í‚¹ ê´€ë ¨ ì¸ìë“¤
    parser.add_argument("--span_masking", action="store_true", default=False,
                       help="V_S í† í°ë“¤ì˜ n-gram ê²°í•© ë³´ê°• (ê¸°ë³¸ê°’: False)")
    parser.add_argument("--span_ngram_max", type=int, default=3,
                       help="Span n-gram ìµœëŒ€ ì°¨ìˆ˜ (ê¸°ë³¸ê°’: 3)")
    
    # Lambda ê´€ë ¨ ì¸ìë“¤
    parser.add_argument("--allow_negative_lambda", action="store_true", default=False,
                       help="ìŒìˆ˜ lambda í—ˆìš©")
    parser.add_argument("--lambda_eta", type=float, default=0.1,
                       help="Lambda í•™ìŠµë¥  (ê¸°ë³¸ê°’: 0.1)")
    parser.add_argument("--pinsker_cap", type=float, default=0.10,
                       help="Pinsker margin absolute cap (ê¸°ë³¸ê°’: 0.10)")
    parser.add_argument("--use_upper_for_lambda", action=argparse.BooleanOptionalAction, default=True,
                       help="Î» ì œì–´ì— Wilson ìƒí•œ ì‚¬ìš©(True) / EMA ì‚¬ìš©(False)")
    
    # Wilson ê´€ë ¨ ì¸ìë“¤
    parser.add_argument("--wilson_max_n", type=int, default=1000,
                       help="Wilson ìµœëŒ€ n (ê¸°ë³¸ê°’: 1000)")
    
    # ë¡œê¹… ê´€ë ¨ ì¸ìë“¤
    parser.add_argument("--log_every", type=int, default=10,
                       help="ë¡œê·¸ ì¶œë ¥ ë¹ˆë„ (ê¸°ë³¸ê°’: 10)")
    
    # ì¶œë ¥ ê´€ë ¨ ì¸ìë“¤
    parser.add_argument("--output_dir", type=str, default="",
                       help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    
    # ì‹œë“œ ì„¤ì •
    parser.add_argument("--seed", type=int, default=None,
                       help="ëœë¤ ì‹œë“œ")
    
    # Retain ê´€ë ¨ ì¸ìë“¤
    parser.add_argument("--retain_weight", type=float, default=0.0,
                       help="Retain ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 0.0)")
    parser.add_argument("--retain_broadcast", action="store_true", default=False,
                       help="Retain ë¸Œë¡œë“œìºìŠ¤íŠ¸")
    
    # Preference ê´€ë ¨ ì¸ìë“¤
    parser.add_argument("--preference_weight", type=float, default=0.0,
                       help="Preference ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 0.0)")
    parser.add_argument("--pref_every", type=int, default=10,
                       help="Preference ì—…ë°ì´íŠ¸ ë¹ˆë„ (ê¸°ë³¸ê°’: 10)")
    parser.add_argument("--pref_format", type=str, default="dpo",
                       help="Preference í˜•ì‹ (ê¸°ë³¸ê°’: dpo)")
    parser.add_argument("--pref_beta", type=float, default=0.1,
                       help="Preference beta (ê¸°ë³¸ê°’: 0.1)")
    parser.add_argument("--pref_margin", type=float, default=0.1,
                       help="Preference margin (ê¸°ë³¸ê°’: 0.1)")
    parser.add_argument("--pref_max_len", type=int, default=512,
                       help="Preference ìµœëŒ€ ê¸¸ì´ (ê¸°ë³¸ê°’: 512)")
    
    return parser.parse_args()

def run_h200_optimized_etu():
    """H200 ìµœì í™”ëœ ETU ì‹¤í–‰"""
    try:
        # H200 í™˜ê²½ ì„¤ì •
        is_h200 = setup_h200_environment()
        
        # ì¸ì íŒŒì‹±
        args = get_h200_optimized_args()
        
        # ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚° (ëª¨ë¸ í¬ê¸° ê¸°ë°˜)
        optimal_batch = calculate_optimal_batch_size(args.model_name_or_path)
        if args.batch_size == 64:  # ê¸°ë³¸ê°’ì¸ ê²½ìš°ì—ë§Œ ìë™ ì¡°ì •
            args.batch_size = optimal_batch
            print(f"ğŸ”§ ëª¨ë¸ í¬ê¸° ê¸°ë°˜ ìµœì  ë°°ì¹˜ í¬ê¸°: {optimal_batch}")
        else:
            print(f"ğŸ”§ ì‚¬ìš©ì ì§€ì • ë°°ì¹˜ í¬ê¸° ì‚¬ìš©: {args.batch_size} (heuristic={optimal_batch})")
        
        # GPU ì„¤ì •
        if args.multi_gpu:
            gpu_ids = list(range(torch.cuda.device_count()))
            os.environ["CUDA_VISIBLE_DEVICES"] = ",".join(map(str, gpu_ids))
            print(f"ğŸ”„ ë©€í‹° GPU ëª¨ë“œ: GPU {gpu_ids}")
            print("[note] --multi_gpuëŠ” í™˜ê²½ë§Œ ì„¤ì •í•©ë‹ˆë‹¤. ì‹¤ì œ ë¶„ì‚° ì‹¤í–‰ì€ torchrunìœ¼ë¡œ ì‹¤í–‰í•˜ì„¸ìš”. ì˜ˆ: \n"
                  "torchrun --nproc_per_node=<num_gpus> ETU/run_etu_h200.py --multi_gpu --strategy ddp ...")
            
            # ë©€í‹° GPU í™˜ê²½ ì„¤ì •
            setup_multi_gpu_environment(args.strategy)
        else:
            os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu_id)
            print(f"ğŸ¯ ë‹¨ì¼ GPU ëª¨ë“œ: GPU {args.gpu_id}")
        
        # H200 ìµœì í™” ì„¤ì • ì ìš©
        if is_h200:
            print("ğŸ”§ H200 ìµœì í™” ì„¤ì • ì ìš©:")
            strat = "ddp" if args.multi_gpu else "single"
            print(f"   - strategy: {strat}")
            print(f"   - batch_size: {args.batch_size}")
            print(f"   - batch_size_per_gpu: {args.batch_size_per_gpu}")
            print(f"   - frozen_on_cpu: {args.frozen_on_cpu}")
            print(f"   - lora_r: {args.lora_r}")
            print(f"   - lora_alpha: {args.lora_alpha}")
            print(f"   - max_num_batches: {args.max_num_batches}")
            print(f"   - mixed_precision: {args.mixed_precision}")
            print(f"   - gradient_accumulation_steps: {args.gradient_accumulation_steps}")
        
        # ETU ì‹¤í–‰ - ì˜¬ë°”ë¥¸ ë°©ì‹ìœ¼ë¡œ í˜¸ì¶œ
        from etu.unlearn import run_etu
        from etu.utils import load_model, get_data
        
        print("ğŸš€ ETU ì‹¤í–‰ ì‹œì‘...")
        
        print("ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘...")
        # ì„œë¡œ ë‹¤ë¥¸ ì¸ìŠ¤í„´ìŠ¤ë¡œ ë¡œë“œí•´ì•¼ í•¨ (ë™ì¼ ì°¸ì¡° ê¸ˆì§€)
        frozen_model, tokenizer = load_model(
            args.model_name_or_path, train=False, infer_on_cpu=args.frozen_on_cpu
        )
        updated_model, _ = load_model(
            args.model_name_or_path, train=True, infer_on_cpu=False
        )
        if args.frozen_on_cpu:
            print("ğŸ”§ Frozen ëª¨ë¸ì„ CPUì— ìœ ì§€ (ë©”ëª¨ë¦¬ ì ˆì•½)")
        else:
            print("ğŸ”§ Frozen ëª¨ë¸ì„ GPUì— ë¡œë“œ")
        
        # ë°ì´í„° ë¡œë”©
        print("ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...")
        
        # ë°ì´í„°ì…‹ ë³„ì¹­ í•´ê²°
        forget_paths, retain_paths = resolve_dataset_paths(
            args.forget_corpora, args.retain_corpora
        )
        print(f"ğŸ” Forget ë°ì´í„°ì…‹: {forget_paths}")
        print(f"ğŸ” Retain ë°ì´í„°ì…‹: {retain_paths}")
        
        # layer_idsë¥¼ layer_idì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
        if args.layer_id is not None:
            args.layer_ids = str(args.layer_id)
            print(f"ğŸ”§ Layer ì„¤ì •: layer_id={args.layer_id}, layer_ids={args.layer_ids}")
        
        # ë¬¸ìì—´ë¡œ ë“¤ì–´ì˜¤ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if isinstance(args.layer_ids, str):
            args.layer_ids = [int(x.strip()) for x in args.layer_ids.split(",") if x.strip()]
        
        if isinstance(args.lora_target_modules, str):
            args.lora_target_modules = [s.strip() for s in args.lora_target_modules.split(",") if s.strip()]
        
        print(f"ğŸ”§ Final layer_ids: {args.layer_ids}")
        print(f"ğŸ”§ Final lora_target_modules: {args.lora_target_modules}")
        
        # param_ids: nargs="+" ì´ë¯€ë¡œ ì´ë¯¸ List[int] ë˜ëŠ” None
        if args.param_ids is not None and len(args.param_ids) == 0:
            args.param_ids = None
        
        forget_data_list, retain_data_list = get_data(
            forget_paths,
            retain_paths,
            min_len=args.min_len,
            max_len=args.max_len,
            batch_size=args.batch_size,
        )
        
        # ETU ì‹¤í–‰
        run_etu(
            updated_model,
            frozen_model,
            tokenizer,
            forget_data_list,
            retain_data_list,
            args,
        )
        
        print("âœ… ETU ì‹¤í–‰ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=== ETU H200 GPU ìµœì í™” ì‹¤í–‰ ===")
    
    # H200 ìµœì í™”ëœ ETU ì‹¤í–‰
    run_h200_optimized_etu()

if __name__ == "__main__":
    main() 
