(LLM_EvalPipeline_test) aiuser3@ai-smartlaw:~/ETU$ python3 run_etu_h200.py \
  --multi_gpu \
  --strategy ddp \
  --forget_corpora "./datasets/cyber-forget" \
  --retain_corpora "./datasets/bio-retain" \
  --batch_size 32 \
  --max_num_batches 50 \
  --layer_id 7 \
  --epsilon 0.05 \
  --lambda_max 12.0 \
  --verbose
=== ETU H200 GPU ìµœì í™” ì‹¤í–‰ ===
ğŸš€ H200 GPU í™˜ê²½ ì„¤ì • ì¤‘...
GPU 0: NVIDIA H200 (139.8 GB)
GPU 1: NVIDIA H200 (139.8 GB)
GPU 2: NVIDIA H200 (139.8 GB)
GPU 3: NVIDIA H200 (139.8 GB)
GPU 4: NVIDIA H200 (139.8 GB)
GPU 5: NVIDIA H200 (139.8 GB)
GPU 6: NVIDIA H200 (139.8 GB)
GPU 7: NVIDIA H200 (139.8 GB)
âœ… H200 GPU 8ê°œ ê°ì§€ë¨
ğŸ”„ ë©€í‹° GPU ëª¨ë“œ: GPU [0, 1, 2, 3, 4, 5, 6, 7]
ğŸ”§ ë©€í‹° GPU í™˜ê²½ ì„¤ì •: ddp
âœ… DDP í™˜ê²½ ì„¤ì • ì™„ë£Œ
ğŸ”§ H200 ìµœì í™” ì„¤ì • ì ìš©:
   - strategy: ddp
   - batch_size: 32
   - batch_size_per_gpu: 8
   - frozen_on_cpu: True
   - lora_r: 512
   - lora_alpha: 1024
   - max_num_batches: 50
   - mixed_precision: bf16
   - gradient_accumulation_steps: 4
ğŸš€ ETU ì‹¤í–‰ ì‹œì‘...
ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘...
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 369.62it/s]
âŒ ì˜¤ë¥˜ ë°œìƒ: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 139.81 GiB of which 24.12 MiB is free. Process 1117159 has 127.15 GiB memory in use. Including non-PyTorch memory, this process has 12.62 GiB memory in use. Of the allocated memory 12.10 GiB is allocated by PyTorch, and 7.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/data/aiuser3/ETU/run_etu_h200.py", line 367, in run_h200_optimized_etu
    base_model, tokenizer = load_model(args.model_name_or_path, train=True)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/aiuser3/ETU/etu/utils.py", line 512, in load_model
    model.to("cuda")
  File "/data/aiuser3/LLM_EvalPipeline_test/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4333, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/aiuser3/LLM_EvalPipeline_test/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1371, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/data/aiuser3/LLM_EvalPipeline_test/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/data/aiuser3/LLM_EvalPipeline_test/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/data/aiuser3/LLM_EvalPipeline_test/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/data/aiuser3/LLM_EvalPipeline_test/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 957, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/data/aiuser3/LLM_EvalPipeline_test/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1357, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 139.81 GiB of which 24.12 MiB is free. Process 1117159 has 127.15 GiB memory in use. Including non-PyTorch memory, this process has 12.62 GiB memory in use. Of the allocated memory 12.10 GiB is allocated by PyTorch, and 7.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
