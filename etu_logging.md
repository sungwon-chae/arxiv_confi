(LLM_EvalPipeline_test) aiuser3@ai-smartlaw:~/ETU$ export CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,7

python3 run_etu_h200.py \
  --multi_gpu \
  --strategy ddp \
  --forget_corpora "./datasets/cyber-forget" \
  --retain_corpora "./datasets/bio-retain" \
  --batch_size 32 \
  --max_num_batches 50 \
  --layer_id 7 \
  --epsilon 0.05 \
  --lambda_max 12.0 \
  --verbose
=== ETU H200 GPU ìµœì í™” ì‹¤í–‰ ===
ğŸš€ H200 GPU í™˜ê²½ ì„¤ì • ì¤‘...
GPU 0: NVIDIA H200 (139.8 GB)
GPU 1: NVIDIA H200 (139.8 GB)
GPU 2: NVIDIA H200 (139.8 GB)
GPU 3: NVIDIA H200 (139.8 GB)
GPU 4: NVIDIA H200 (139.8 GB)
GPU 5: NVIDIA H200 (139.8 GB)
GPU 6: NVIDIA H200 (139.8 GB)
âœ… H200 GPU 7ê°œ ê°ì§€ë¨
ğŸ”„ ë©€í‹° GPU ëª¨ë“œ: GPU [0, 1, 2, 3, 4, 5, 6]
ğŸ”§ ë©€í‹° GPU í™˜ê²½ ì„¤ì •: ddp
âœ… DDP í™˜ê²½ ì„¤ì • ì™„ë£Œ
ğŸ”§ H200 ìµœì í™” ì„¤ì • ì ìš©:
   - strategy: ddp
   - batch_size: 32
   - batch_size_per_gpu: 8
   - frozen_on_cpu: True
   - lora_r: 512
   - lora_alpha: 1024
   - max_num_batches: 50
   - mixed_precision: bf16
   - gradient_accumulation_steps: 4
ğŸš€ ETU ì‹¤í–‰ ì‹œì‘...
ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘...
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 377.50it/s]
ğŸ”§ Frozen ëª¨ë¸ì„ CPUì— ìœ ì§€ (ë©”ëª¨ë¦¬ ì ˆì•½)
ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...
ğŸ” Forget ë°ì´í„°ì…‹: ['./datasets/cyber-forget']
ğŸ” Retain ë°ì´í„°ì…‹: ['./datasets/bio-retain']
ğŸ”§ Layer ì„¤ì •: layer_id=7, layer_ids=7
Processing corpus spec: './datasets/cyber-forget'
Loading local dataset folder: ./datasets/cyber-forget
Loading from actual path: ./datasets/cyber-forget/cyber-forget-corpus
Loaded 12 items from local dataset folder
Processing corpus spec: './datasets/bio-retain'
Loading local dataset folder: ./datasets/bio-retain
Loading from actual path: ./datasets/bio-retain/bio-retain-corpus
Loaded 4106 items from local dataset folder
Data loading complete: 1 forget batches, 129 retain batches
====ETU Config====
gpu_id=0
multi_gpu=True
strategy=ddp
batch_size_per_gpu=8
batch_size=32
max_num_batches=50
frozen_on_cpu=True
use_lora=True
lora_r=512
lora_alpha=1024
epsilon=0.05
lambda_max=12.0
lambda_update_freq=25
forget_corpora=./datasets/cyber-forget
retain_corpora=./datasets/bio-retain
model_name_or_path=HuggingFaceH4/zephyr-7b-beta
deterministic=False
verbose=True
gradient_accumulation_steps=4
mixed_precision=bf16
trust_remote_code=False
lr=1e-05
num_epochs=1
min_len=10
max_len=512
layer_id=7
layer_ids=7
param_ids=
name_keywords=q_proj,k_proj,v_proj,o_proj
module_str={model_name}.model.layers[{layer_id}]
use_pmi_vs=False
vocab_top_k=1000
vs_freq_rate=0.1
vs_abs_cap=1000
pmi_top_k=1000
pmi_min_count=10
pmi_smoothing=0.1
pmi_max_batches=100
vs_preview_k=10
allow_negative_lambda=False
lambda_eta=0.1
wilson_max_n=1000
log_every=10
output_dir=
seed=None
retain_weight=0.0
retain_broadcast=False
preference_weight=0.0
pref_every=10
pref_format=dpo
pref_beta=0.1
pref_margin=0.1
pref_max_len=512
=====
Applying LoRA for efficient parameter updates...
Applying LoRA to layers: [7]
trainable params: 13,631,488 || all params: 7,255,363,584 || trainable%: 0.1879
/data/aiuser3/ETU/etu/unlearn.py:99: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(use_cuda and not use_bf16))
Building forbidden token set V_S...
V_S size: 258 tokens (0.8% of vocab)
Estimating base probability mass p_S over V_S...
Estimated p_S (Ï€_base over V_S): 0.2761
[info] |V_S|/V = 0.8%, Ï€_base(S)=0.2761, Îµ=0.0500
V_S preview: ['er', 'â–a', 'on', 're', 'â–the', 'â–w', 'it', 'al', 'ed', 'ing']
Initial Î»: 1.9807 â†’ expected qÎ»(S)â‰ˆ0.0500
======= Epoch 0 =======
  0%|                                                                              | 0/12 [00:00<?, ?it/s]/data/aiuser3/ETU/etu/unlearn.py:220: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp,
[HIGH] Ï€Î¸(S)=0.3062 [95% normal 0.2251,0.3874 | Wilsonâ†‘ 0.3922] | E[qÎ»(S)]=0.0500 | Îµ=0.0500 | KL=11.66 | Î»=1.981
  8%|â–ˆâ–ˆâ–ˆâ–                                  | 1/12 [00:00<00:04,  2.50it/s, loss=11.7, Ï€Î¸(S)=0.306, Î»=1.98][HIGH] Ï€Î¸(S)=0.2037 [95% normal 0.0683,0.3390 | Wilsonâ†‘ 0.2731] | E[qÎ»(S)]=0.0500 | Îµ=0.0500 | KL=2.007 | Î»=1.981
 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 2/12 [00:00<00:02,  3.94it/s, loss=2.01, Ï€Î¸(S)=0.204, Î»=1.98][HIGH] Ï€Î¸(S)=0.2818 [95% normal 0.1908,0.3727 | Wilsonâ†‘ 0.3402] | E[qÎ»(S)]=0.0500 | Îµ=0.0500 | KL=7.462 | Î»=1.981
 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 3/12 [00:00<00:01,  4.90it/s, loss=7.46, Ï€Î¸(S)=0.282, Î»=1.98][HIGH] Ï€Î¸(S)=0.1965 [95% normal 0.0649,0.3282 | Wilsonâ†‘ 0.2464] | E[qÎ»(S)]=0.0500 | Îµ=0.0500 | KL=2.692 | Î»=1.981
 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 4/12 [00:00<00:01,  5.76it/s, loss=2.69, Ï€Î¸(S)=0.197, Î»=1.98][HIGH] Ï€Î¸(S)=0.3376 [95% normal 0.2578,0.4174 | Wilsonâ†‘ 0.3840] | E[qÎ»(S)]=0.0500 | Îµ=0.0500 | KL=19.78 | Î»=1.981
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                      | 5/12 [00:00<00:01,  6.09it/s, loss=19.8, Ï€Î¸(S)=0.338, Î»=1.98][HIGH] Ï€Î¸(S)=0.1812 [95% normal 0.0517,0.3107 | Wilsonâ†‘ 0.2192] | E[qÎ»(S)]=0.0500 | Îµ=0.0500 | KL=1.889 | Î»=1.981
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   | 6/12 [00:01<00:00,  6.77it/s, loss=1.89, Ï€Î¸(S)=0.181, Î»=1.98][HIGH] Ï€Î¸(S)=0.4188 [95% normal 0.3333,0.5043 | Wilsonâ†‘ 0.4592] | E[qÎ»(S)]=0.0500 | Îµ=0.0500 | KL=13.94 | Î»=1.981
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 7/12 [00:01<00:00,  6.86it/s, loss=13.9, Ï€Î¸(S)=0.419, Î»=1.98][HIGH] Ï€Î¸(S)=0.1731 [95% normal 0.0460,0.3003 | Wilsonâ†‘ 0.2050] | E[qÎ»(S)]=0.0500 | Îµ=0.0500 | KL=1.905 | Î»=1.981
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 8/12 [00:01<00:00,  7.36it/s, loss=1.9, Ï€Î¸(S)=0.173, Î»=1.98][HIGH] Ï€Î¸(S)=0.1081 [95% normal 0.0326,0.1835 | Wilsonâ†‘ 0.1336] | E[qÎ»(S)]=0.0500 | Îµ=0.0500 | KL=3.046 | Î»=1.981
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 9/12 [00:01<00:00,  7.26it/s, loss=3.05, Ï€Î¸(S)=0.108, Î»=1.98][HIGH] Ï€Î¸(S)=0.1638 [95% normal 0.0394,0.2882 | Wilsonâ†‘ 0.1926] | E[qÎ»(S)]=0.0500 | Îµ=0.0500 | KL=1.676 | Î»=1.981
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 10/12 [00:01<00:00,  7.68it/s, loss=1.68, Ï€Î¸(S)=0.164, Î»=1.98][HIGH] Ï€Î¸(S)=0.5516 [95% normal 0.4367,0.6664 | Wilsonâ†‘ 0.5859] | E[qÎ»(S)]=0.0500 | Îµ=0.0500 | KL=7.851 | Î»=1.981
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/12 [00:01<00:00,  7.68it/s, loss=7.85, Ï€Î¸(S)=0.552, Î»=1.98][HIGH] Ï€Î¸(S)=0.2970 [95% normal 0.1900,0.4041 | Wilsonâ†‘ 0.3284] | E[qÎ»(S)]=0.0500 | Îµ=0.0500 | KL=8.031 | Î»=1.981
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00,  6.44it/s, loss=8.03, Ï€Î¸(S)=0.297, Î»=1.98]
=== ETU Suppression Report ===
  - Perplexity on retain: 4.94
=== Results ===
  - Ï€_base(S): 0.2570
  - Ï€_Î¸(S): 0.2570
  - Suppression ratio: 1.00 (updated/base)
  - Target Îµ: 0.0500
  - Target achieved: âœ—
  - 95% upper Ï€_base(S): 0.2872
  - 95% upper Ï€_Î¸(S): 0.2872
  - Target achieved (95% upper): âœ—
Saved suppression report to models/zephyr-7b-beta_etu_epsilon-0.05_lambda-1.9807_2025-09-02-15-17-52/suppression_report.json
Merging LoRA weights into base model...
Saved V_S to models/zephyr-7b-beta_etu_epsilon-0.05_lambda-1.9807_2025-09-02-15-17-52/V_S.ids.json
Saved ETU model to models/zephyr-7b-beta_etu_epsilon-0.05_lambda-1.9807_2025-09-02-15-17-52
Saved args to models/zephyr-7b-beta_etu_epsilon-0.05_lambda-1.9807_2025-09-02-15-17-52/args.json
Saved metrics to models/zephyr-7b-beta_etu_epsilon-0.05_lambda-1.9807_2025-09-02-15-17-52/metrics.json
âœ… ETU ì‹¤í–‰ ì™„ë£Œ!
(LLM_EvalPipeline_test) aiuser3@ai-smartlaw:~/ETU$ 
