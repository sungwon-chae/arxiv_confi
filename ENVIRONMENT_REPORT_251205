# 폐쇄망 환경 스펙 보고서

**생성 일시:** 2025-12-05  
**호스트명:** ai-smartlaw  
**OS:** Linux 6.8.0-64-generic  
**작성자:** aiuser5 (공통 정보 및 aiuser5 환경)  
**확인 필요:** aiuser3 (섹션 4, 6 작성 필요)

---

## 1. GPU 정보

### 하드웨어 사양
- **GPU 모델:** NVIDIA H200
- **GPU 개수:** 8개
- **각 GPU 메모리:** 143,771 MiB (약 140GB)
- **NVIDIA Driver:** 580.95.05
- **CUDA Version:** 13.0

### 현재 GPU 상태
```
GPU 0-3: 사용 중 (각각 약 105GB 메모리 사용, VLLM 프로세스 실행 중)
GPU 4-7: 사용 가능 (메모리 사용량 0GB)
```

### GPU 상세 정보
| GPU ID | 메모리 사용량 | 전력 사용량 | 온도 | 상태 |
|--------|-------------|------------|------|------|
| 0 | 105,288 MiB / 143,771 MiB | 122W / 700W | 30°C | 사용 중 |
| 1 | 105,286 MiB / 143,771 MiB | 126W / 700W | 27°C | 사용 중 |
| 2 | 105,286 MiB / 143,771 MiB | 115W / 700W | 26°C | 사용 중 |
| 3 | 105,286 MiB / 143,771 MiB | 122W / 700W | 30°C | 사용 중 |
| 4 | 0 MiB / 143,771 MiB | 77W / 700W | 26°C | 사용 가능 |
| 5 | 0 MiB / 143,771 MiB | 79W / 700W | 25°C | 사용 가능 |
| 6 | 0 MiB / 143,771 MiB | 76W / 700W | 24°C | 사용 가능 |
| 7 | 0 MiB / 143,771 MiB | 75W / 700W | 26°C | 사용 가능 |

### 실험 설계 권장사항
- **사용 가능한 GPU:** 4개 (GPU 4-7)
- **각 GPU 메모리:** 약 140GB
- **권장 모델 크기:**
  - 8B 모델: fp16/bf16 기준 약 16GB, **여러 개 동시 실행 가능**
  - 70B 모델: fp16/bf16 기준 약 140GB, **1개씩 실행 가능**
  - 8B 모델 4개를 각 GPU에 하나씩 배치 가능
- **Mixed Precision:** bfloat16 지원 가능 (H200은 Ampere 아키텍처 이상)
- **Tensor Parallel:** 필요 시 4개 GPU로 분산 가능

---

## 2. CPU / 메모리 / 디스크

### CPU 정보
- **모델:** Intel Xeon Platinum 8558
- **총 CPU 코어:** 192개 (논리 코어)
- **물리 코어:** 96개 (48 core × 2 socket)
- **Thread per Core:** 2 (하이퍼스레딩 활성화)
- **소켓 수:** 2개
- **NUMA 노드:** 3개 (노드별 CPU 분산)

### 메모리 (RAM)
- **총 메모리:** 1.5TB
- **사용 중:** 52GB
- **사용 가능:** 1.4TB
- **버퍼/캐시:** 59GB
- **Swap:** 127GB (사용량 0GB)

### 디스크
- **총 용량:** 46,161 GB (약 45TB)
- **사용 중:** 2,476 GB (약 2.4TB)
- **사용 가능:** 43,219 GB (약 42TB)

### 실험 설계 권장사항
- **데이터셋 로딩:** 메모리가 충분하여 대용량 데이터셋도 메모리에 올릴 수 있음
- **병렬 처리:** 192 코어로 데이터 로딩/전처리 병렬화 가능
- **결과 저장:** 디스크 여유 공간 충분 (42TB), 대용량 로그/결과 저장 가능
- **Dataloader:** `num_workers`를 높게 설정 가능 (예: 32-64)

---

## 3. aiuser5 Python 환경

> **참고:** 이 섹션은 aiuser5 계정에서 확인한 정보입니다.

### Python 버전
- **시스템 Python:** 3.12.3 (`/usr/bin/python3`)
- **컴파일러:** GCC 13.3.0
- **플랫폼:** Linux-6.8.0-64-generic-x86_64

### 가상환경 (Virtual Environment)

aiuser5 계정에서 확인한 가상환경 목록:

1. **`/data/aiuser5/.venv`**
   - Python 3.12.3
   - 주요 라이브러리 미설치 상태

2. **`/data/aiuser5/LegalAI-DataPipeline/.venv`**
   - Python 3.12.3
   - 주요 라이브러리 미설치 상태

3. **`/data/aiuser5/LegalAI-DataPipeline/BUYER_SELLER/.venv`** ⭐
   - Python 3.12.3
   - **PyTorch 2.9.0+cu128** 설치됨
   - **transformers 4.57.3** 설치됨
   - CUDA 지원 가능

### 설치된 라이브러리 (BUYER_SELLER/.venv 기준)

| 라이브러리 | 버전 | 상태 |
|-----------|------|------|
| PyTorch | 2.9.0+cu128 | ✅ **설치됨** |
| transformers | 4.57.3 | ✅ **설치됨** |
| sentencepiece | 0.2.1 | ✅ **설치됨** |
| accelerate | - | ❌ 설치 안됨 |
| datasets | - | ❌ 설치 안됨 |
| bitsandbytes | - | ❌ 설치 안됨 |
| peft | - | ❌ 설치 안됨 |
| protobuf | - | ❌ 설치 안됨 |

### CUDA 확인 (BUYER_SELLER/.venv에서)
- **CUDA available:** ✅ True
- **CUDA version:** 12.8
- **GPU 인식:** 8개 GPU 모두 정상 인식 (H200)

### 가상환경 활성화 방법

aiuser5에서 실험을 실행하기 전에 다음 명령어로 가상환경을 활성화해야 합니다:

```bash
# BUYER_SELLER venv 활성화 (PyTorch, transformers 설치됨)
source /data/aiuser5/LegalAI-DataPipeline/BUYER_SELLER/.venv/bin/activate

# 또는 entity-level-unlearning 디렉토리에서
cd /data/aiuser5/entity-level-unlearning
source ../LegalAI-DataPipeline/BUYER_SELLER/.venv/bin/activate
```

### 추가 설치가 필요한 라이브러리

BUYER_SELLER venv에 다음 라이브러리들의 설치가 필요합니다:

```bash
# venv 활성화 후
source /data/aiuser5/LegalAI-DataPipeline/BUYER_SELLER/.venv/bin/activate

# 필수 라이브러리 설치
pip install accelerate datasets bitsandbytes peft protobuf
```

**참고:** sentencepiece는 이미 설치되어 있습니다.

**주의사항:**
- PyTorch 2.9.0은 CUDA 12.8용으로 빌드됨 (CUDA 13.0과 호환 가능)
- H200 GPU 사용 가능
- bfloat16 지원됨 (PyTorch 2.9.0)

---

## 4. aiuser3 Python 환경

> **✅ 확인 완료:** 2025-12-05

### Python 버전
- **시스템 Python:** 3.12.3 (`/usr/bin/python3`)
- **컴파일러:** GCC (main, Nov 6 2025, 13:44:16)
- **플랫폼:** Linux-6.8.0-64-generic-x86_64

### 가상환경 (Virtual Environment)

aiuser3 계정에서 확인한 가상환경 목록:

1. **`/data/aiuser3/LLM_EvalPipeline_test/.venv`** ⭐
   - Python 3.12.3
   - **PyTorch 2.9.0.dev20250804+cu128** 설치됨
   - **transformers 4.57.0.dev0** 설치됨
   - **accelerate 1.10.0** 설치됨
   - **datasets 2.16.0** 설치됨
   - **peft 0.17.0** 설치됨
   - **sentencepiece 0.2.0** 설치됨
   - CUDA 지원 가능

2. **`/data/aiuser3/LegalAI-DataPipeline/.venv`**
   - Python 3.12.3
   - 주요 라이브러리 미설치 상태

3. **`/data/aiuser3/RAG-pipeline/.venv`** ⭐
   - Python 3.12.3
   - **PyTorch 2.8.0+cu128** 설치됨
   - **transformers 4.56.2** 설치됨
   - **accelerate 1.10.1** 설치됨
   - **datasets 4.2.0** 설치됨
   - **peft 0.17.1** 설치됨
   - **sentencepiece 0.2.1** 설치됨
   - CUDA 지원 가능

### 설치된 라이브러리 (LLM_EvalPipeline_test/.venv 기준)

| 라이브러리 | 버전 | 상태 |
|-----------|------|------|
| PyTorch | 2.9.0.dev20250804+cu128 | ✅ **설치됨** |
| transformers | 4.57.0.dev0 | ✅ **설치됨** |
| accelerate | 1.10.0 | ✅ **설치됨** |
| datasets | 2.16.0 | ✅ **설치됨** |
| bitsandbytes | - | ❌ 설치 안됨 |
| peft | 0.17.0 | ✅ **설치됨** |
| sentencepiece | 0.2.0 | ✅ **설치됨** |
| protobuf | - | ❌ 설치 안됨 |

### CUDA 확인 (LLM_EvalPipeline_test/.venv에서)
- **CUDA available:** ✅ True
- **CUDA version:** 12.8
- **GPU 인식:** 8개 GPU 모두 정상 인식 (H200)

### 가상환경 활성화 방법

aiuser3에서 실험을 실행하기 전에 다음 명령어로 가상환경을 활성화해야 합니다:

```bash
# LLM_EvalPipeline_test venv 활성화 (PyTorch 2.9.0.dev, transformers 4.57.0.dev0 설치됨)
source /data/aiuser3/LLM_EvalPipeline_test/.venv/bin/activate

# 또는 RAG-pipeline venv 활성화 (PyTorch 2.8.0, transformers 4.56.2 설치됨)
source /data/aiuser3/RAG-pipeline/.venv/bin/activate
```

### 추가 설치가 필요한 라이브러리

LLM_EvalPipeline_test venv에 다음 라이브러리들의 설치가 필요합니다:

```bash
# venv 활성화 후
source /data/aiuser3/LLM_EvalPipeline_test/.venv/bin/activate

# 필수 라이브러리 설치
pip install bitsandbytes protobuf
```

**참고:** 
- LLM_EvalPipeline_test venv는 대부분의 필수 라이브러리가 이미 설치되어 있어 실험에 바로 사용 가능합니다.
- RAG-pipeline venv도 PyTorch, transformers 등이 설치되어 있어 사용 가능합니다.

**주의사항:**
- PyTorch 2.9.0.dev는 CUDA 12.8용으로 빌드됨 (CUDA 13.0과 호환 가능)
- H200 GPU 사용 가능
- bfloat16 지원됨 (PyTorch 2.9.0.dev)
- bitsandbytes와 protobuf는 필요 시 추가 설치 필요

---

## 4. 모델 디렉토리 구조

### 모델 저장 위치
- **경로:** `/data/models/`
- **권한:** 다중 사용자 공유 디렉토리

### 사용 가능한 모델 목록
1. **EXAONE-3.5-32B-Instruct** (aiuser3 소유)
   - 32B 파라미터 모델
   - safetensors 형식 (27개 파일로 분할)
   - config.json, tokenizer 파일 포함

2. **EXAONE-4.0-32B** (aiuser1 소유)
   - 32B 파라미터 모델

3. **gemma-3-1b-it** (aiuser1 소유)
   - 1B 파라미터 instruction-tuned 모델

4. **gemma-3-1b-pt** (aiuser1 소유)
   - 1B 파라미터 pretrained 모델

5. **gemma-3-270m-it** (aiuser1 소유)
   - 270M 파라미터 instruction-tuned 모델

6. **gpt-oss-120b** (aiuser3 소유)
   - 120B 파라미터 모델

7. **kclaw-luxia3-pt-v1.0.0** (aiuser1 소유)

8. **KORMo-10B-base** (aiuser1 소유)
   - 10B 파라미터 base 모델

9. **luxia3-32b-base** (aiuser1 소유)
   - 32B 파라미터 base 모델

### 모델 로딩 방법
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# 예시: EXAONE 모델 로딩
model_path = "/data/models/EXAONE-3.5-32B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_path)
```

---

## 5. 실험 설계 권장사항

### 모델별 실행 전략

#### 8B 모델 (Llama-3.1-8B, zephyr-7b-alpha 등)
- **GPU당 1개 모델:** 4개 모델 동시 실행 가능 (GPU 4-7 사용)
- **메모리 여유:** 각 GPU에 약 120GB 여유
- **Batch size:** 큰 배치 크기 가능 (예: 32-64)
- **Precision:** bfloat16 권장

#### 32B 모델 (EXAONE-3.5-32B 등)
- **GPU당 1개 모델:** 4개 모델 동시 실행 가능
- **메모리 사용량:** fp16/bf16 기준 약 64GB
- **Batch size:** 중간 배치 크기 (예: 8-16)
- **Precision:** bfloat16 권장

#### 70B+ 모델
- **Tensor Parallel 필요:** 4개 GPU로 분산
- **또는:** 1개 GPU에 1개 모델 (메모리 부족 가능성)
- **Batch size:** 작은 배치 크기 (예: 1-4)

### 실험 병렬화 전략
```
GPU 4: Model A (8B) - Experiment 1
GPU 5: Model B (8B) - Experiment 2
GPU 6: Model C (8B) - Experiment 3
GPU 7: Model D (8B) - Experiment 4
```

또는

```
GPU 4-7: Large Model (70B+) - Tensor Parallel
```

### 데이터셋 처리
- **메모리 로딩:** 1.4TB 여유로 대용량 데이터셋도 메모리에 올릴 수 있음
- **Dataloader workers:** 32-64 권장
- **디스크 I/O:** 충분한 디스크 공간으로 결과 저장 문제 없음

---

## 6. 네트워크/인터넷 정책

> **✅ 확인 완료:** 2025-12-05

### 네트워크 접속 상태

다음 정보는 aiuser3 계정에서 확인한 결과입니다:

- [x] **외부 인터넷 접속:** ❌ **불가능** (Google 접속 실패)
- [x] **PyPI 접속:** ❌ **불가능** (pypi.org 접속 실패)
- [x] **Hugging Face 접속:** ✅ **가능** (huggingface.co 접속 성공, HTTP 200)
- [x] **pip install:** ✅ **작동함** (로컬 패키지 캐시 또는 내부 mirror 사용 가능)
- [ ] **git clone:** 미확인 (추가 확인 필요)
- [x] **작업 디렉토리 권한:** ✅ **쓰기 가능** (`/data/aiuser3` 디렉토리 소유권 있음)

### 확인 결과 상세

**인터넷 접속 테스트:**
```bash
# Google 접속: 실패 (HTTP 000)
# PyPI 접속: 실패 (HTTP 000)
# Hugging Face 접속: 성공 (HTTP 200)
```

**pip install 테스트:**
- `pip install --dry-run numpy` 명령어 정상 작동
- 로컬 패키지 캐시 또는 내부 PyPI mirror를 통해 설치 가능한 것으로 보임

**작업 디렉토리:**
- `/data/aiuser3`: 쓰기 권한 있음 (소유자: aiuser3)
- 홈 디렉토리: 쓰기 권한 있음

### 현재 상황 요약

**aiuser3 확인 결과:**
- ✅ **Hugging Face 접속 가능:** 모델/데이터셋 다운로드 가능 (인증 필요 시 추가 확인)
- ❌ **일반 인터넷 접속 불가:** Google, PyPI 등 일반 웹사이트 접속 불가
- ✅ **pip install 작동:** 내부 mirror 또는 로컬 캐시를 통해 패키지 설치 가능
- ✅ **작업 디렉토리 권한:** `/data/aiuser3` 디렉토리에 쓰기 권한 있음

**aiuser5에서 확인한 내용 (참고):**
- Hugging Face 모델 다운로드 시도 시 인증 문제 발생 가능
- PyPI 설치 시 SSL 오류 발생 가능성

### 권장사항

1. **라이브러리 설치:**
   - 필요한 라이브러리는 wheel 파일로 준비하여 오프라인 설치
   - 또는 내부 PyPI mirror가 있다면 해당 URL 사용

2. **모델/데이터셋 다운로드:**
   - Hugging Face 접속은 가능하므로, 인증 토큰 설정 후 다운로드 시도
   - 또는 로컬에 tar로 압축하여 전송 후 압축 해제

3. **git clone:**
   - 외부 인터넷 접속이 불가하므로, git clone은 제한적일 수 있음
   - 필요한 경우 로컬에 tar로 압축하여 전송

4. **실험 진행:**
   - 이미 설치된 라이브러리로 실험 진행 가능 (LLM_EvalPipeline_test venv 권장)
   - 추가 라이브러리 필요 시 오프라인 설치 방법 사용

---

## 7. 요약 및 다음 단계

### 현재 환경 요약

#### 공통 하드웨어 사양
✅ **강점:**
- H200 GPU 8개 (4개 사용 가능)
- 각 GPU 140GB 메모리
- 1.5TB RAM
- 42TB 디스크 여유 공간
- 192 CPU 코어

#### aiuser5 환경
✅ **설치됨:**
- PyTorch 2.9.0+cu128 (BUYER_SELLER venv)
- transformers 4.57.3 (BUYER_SELLER venv)
- CUDA 12.8 정상 작동, 8개 GPU 인식

⚠️ **개선 필요:**
- accelerate, datasets, peft 등 일부 라이브러리 추가 설치 필요

#### aiuser3 환경
✅ **설치됨:**
- PyTorch 2.9.0.dev20250804+cu128 (LLM_EvalPipeline_test venv)
- transformers 4.57.0.dev0 (LLM_EvalPipeline_test venv)
- accelerate, datasets, peft, sentencepiece 설치됨
- CUDA 12.8 정상 작동, 8개 GPU 인식

⚠️ **개선 필요:**
- bitsandbytes, protobuf 추가 설치 필요 (선택적)
- Hugging Face 인증 토큰 설정 필요 (모델 다운로드 시)

### 권장 작업 순서

#### 1. aiuser3 환경 확인
- [x] aiuser3 Python 환경 섹션 작성 (섹션 4) ✅ **완료**
- [x] 네트워크 정책 확인 및 작성 (섹션 6) ✅ **완료**
- [x] 사용할 venv 결정 및 라이브러리 확인 ✅ **완료** (LLM_EvalPipeline_test venv 권장)

#### 2. 가상환경 활성화 및 추가 라이브러리 설치

**aiuser5:**
```bash
source /data/aiuser5/LegalAI-DataPipeline/BUYER_SELLER/.venv/bin/activate
pip install accelerate datasets peft bitsandbytes protobuf
```

**aiuser3:**
```bash
# LLM_EvalPipeline_test venv 사용 (권장)
source /data/aiuser3/LLM_EvalPipeline_test/.venv/bin/activate
pip install bitsandbytes protobuf  # 선택적 설치

# 또는 RAG-pipeline venv 사용
source /data/aiuser3/RAG-pipeline/.venv/bin/activate
pip install bitsandbytes protobuf  # 선택적 설치
```

#### 3. 모델 다운로드/확인
- Llama-3.1-8B, Llama-3.1-8B-Instruct
- zephyr-7b-alpha
- 데이터셋들 (TOFU, ELUDe, RWKU, wmdp, wikitext)

#### 4. 실험 설정
- **가상환경 활성화 필수** (각 사용자별 venv)
- 4개 GPU 활용한 병렬 실험 설계
- Batch size, max_new_tokens 최적화
- Mixed precision (bfloat16) 사용

#### 5. 코드 튜닝
- 환경에 맞춘 device_map 설정
- Dataloader 최적화
- 메모리 효율적인 추론 파이프라인 구성
- **주의:** 모든 Python 스크립트 실행 시 venv 활성화 필수

---

**보고서 생성 완료**

