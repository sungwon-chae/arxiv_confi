#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©: OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ Style Transfer ì˜ˆì¸¡ ìƒì„±
vLLM ì„œë²„ ì—†ì´ OpenAI APIë¥¼ í†µí•´ ì˜ˆì¸¡ ìƒì„± (ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©)
"""

import json
import os
import time
from typing import List, Dict, Optional
from pathlib import Path
from datetime import datetime

from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
from pydantic import BaseModel, Field

from spa_prompt_utils import create_enhanced_prompt
from test_data_manager import load_test_data

# server_langgraph.pyì˜ êµ¬ì¡°í™”ëœ ì¶œë ¥ ëª¨ë¸ ì¬ì‚¬ìš©
try:
    from server_langgraph import StyleTransferOutput, determine_output_schema
except ImportError:
    # server_langgraphê°€ ì—†ìœ¼ë©´ ì§ì ‘ ì •ì˜ (server_langgraph.pyì™€ ë™ì¼)
    class StyleTransferOutput(BaseModel):
        """ë§¤ìˆ˜ì¸ â†” ë§¤ë„ì¸ ì „í™˜ ì‘ë‹µ"""
        analysis: str = Field(description="ì „í™˜ ì „ ì¡°í•­ì— ëŒ€í•œ ë¶„ì„")
        converted_sentence: str = Field(description="ì „í™˜ëœ ì¡°í•­ ë¬¸ì¥")
    
    def determine_output_schema(selected_tasks):
        """ì„ íƒëœ ì‘ì—…ì— ë”°ë¼ ì ì ˆí•œ ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ë°˜í™˜ (server_langgraph.pyì™€ ë™ì¼)"""
        if not selected_tasks:
            return None
        if "ë§¤ìˆ˜ì¸ â†” ë§¤ë„ì¸ ì „í™˜" in selected_tasks:
            return StyleTransferOutput
        return None

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), ".env"))

# OpenAI API ì„¤ì • (ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")  # ê¸°ë³¸ê°’: gpt-4o-mini (ê°€ê²© íš¨ìœ¨ì )


def get_llm(temperature: float = 0.0, max_tokens: int = 16384):
    """OpenAI APIë¥¼ ì‚¬ìš©í•˜ëŠ” ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©)"""
    if not OPENAI_API_KEY:
        raise ValueError(
            "OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
            ".env íŒŒì¼ì— OPENAI_API_KEY=your_api_keyë¥¼ ì¶”ê°€í•˜ì„¸ìš”."
        )
    
    # ëª¨ë¸ë³„ max_tokens ì œí•œ í™•ì¸ ë° ì¡°ì •
    # gpt-4o: 16384
    # gpt-4o-mini: 16000 (ì‹¤ì œ ì œí•œì€ 16000, í•˜ì§€ë§Œ 16384ë„ í—ˆìš©ë  ìˆ˜ ìˆìŒ)
    # gpt-4-turbo: ëª¨ë¸ ë²„ì „ì— ë”°ë¼ ë‹¤ë¦„
    if "gpt-4o-mini" in OPENAI_MODEL.lower():
        # GPT-4o-miniëŠ” ìµœëŒ€ 16000 í† í°ê¹Œì§€ ìƒì„± ê°€ëŠ¥
        if max_tokens > 16000:
            max_tokens = 16000
    elif "gpt-4o" in OPENAI_MODEL.lower() and "mini" not in OPENAI_MODEL.lower():
        # GPT-4oëŠ” 16384 í† í°ê¹Œì§€ ìƒì„± ê°€ëŠ¥
        if max_tokens > 16384:
            max_tokens = 16384
    elif "turbo" in OPENAI_MODEL.lower():
        # turboëŠ” ëª¨ë¸ ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ ìœ ì§€
        pass
    
    return ChatOpenAI(
        model=OPENAI_MODEL,
        api_key=OPENAI_API_KEY,
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=120.0,
    )


def get_structured_llm(temperature: float = 0.0, max_tokens: int = 16384, output_schema=None):
    """êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ìœ„í•œ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    llm = get_llm(temperature=temperature, max_tokens=max_tokens)
    if output_schema:
        return llm.with_structured_output(output_schema)
    return llm


def extract_score_from_response(response_text: str) -> Optional[float]:
    """
    LLM ì‘ë‹µì—ì„œ ì ìˆ˜ ì¶”ì¶œ
    ë‹¤ì–‘í•œ í˜•ì‹ì˜ ì‘ë‹µì—ì„œ ì ìˆ˜ë¥¼ ì°¾ì•„ëƒ„
    """
    import re
    
    # íŒ¨í„´ 1: "ì ìˆ˜: 4.0" ë˜ëŠ” "score: 4.0"
    patterns = [
        r'ì ìˆ˜[:\s]+([0-9.]+)',
        r'score[:\s]+([0-9.]+)',
        r'([0-9.]+)\s*ì ',
        r'\(([0-9.]+)\)',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, response_text, re.IGNORECASE)
        if match:
            try:
                score = float(match.group(1))
                if 0.0 <= score <= 4.0:
                    return score
            except ValueError:
                continue
    
    # íŒ¨í„´ 2: JSON í˜•ì‹ì—ì„œ ì¶”ì¶œ
    try:
        json_match = re.search(r'\{[^}]*"score"[^}]*\}', response_text)
        if json_match:
            data = json.loads(json_match.group())
            if 'score' in data:
                score = float(data['score'])
                if 0.0 <= score <= 4.0:
                    return score
    except:
        pass
    
    return None


def extract_clause_from_response(response_text: str) -> str:
    """
    LLM ì‘ë‹µì—ì„œ ë³€í™˜ëœ ì¡°í•­ ì¶”ì¶œ
    """
    import re
    
    # ```ë¡œ ê°ì‹¸ì§„ ë¶€ë¶„ ì¶”ì¶œ
    code_block_match = re.search(r'```[^\n]*\n(.*?)```', response_text, re.DOTALL)
    if code_block_match:
        return code_block_match.group(1).strip()
    
    # "ë³€í™˜ëœ ì¡°í•­:" ë˜ëŠ” "converted_sentence:" ë’¤ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    patterns = [
        r'ë³€í™˜ëœ\s*ì¡°í•­[:\s]+(.*?)(?:\n\n|\n##|$)',
        r'converted_sentence[:\s]+(.*?)(?:\n\n|\n##|$)',
        r'ì¬ì‘ì„±[:\s]+(.*?)(?:\n\n|\n##|$)',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, response_text, re.DOTALL | re.IGNORECASE)
        if match:
            clause = match.group(1).strip()
            # ë§ˆí¬ë‹¤ìš´ í¬ë§·íŒ… ì œê±°
            clause = re.sub(r'\*\*([^*]+)\*\*', r'\1', clause)
            clause = re.sub(r'`([^`]+)`', r'\1', clause)
            if clause:
                return clause
    
    # ì „ì²´ ì‘ë‹µ ë°˜í™˜ (ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ)
    return response_text.strip()


def generate_prediction_for_item(
    test_item: Dict,
    use_utils_v2: bool = True,
    use_context_v2: bool = True,
    temperature: float = 0.0
) -> Dict:
    """
    ë‹¨ì¼ test_data í•­ëª©ì— ëŒ€í•´ LLM ì˜ˆì¸¡ ìƒì„±
    
    Args:
        test_item: í…ŒìŠ¤íŠ¸ ë°ì´í„° í•­ëª©
        use_v2: spa_prompt_utils_v2 ì‚¬ìš© ì—¬ë¶€
        temperature: LLM temperature
    
    Returns:
        ì˜ˆì¸¡ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    input_clause = test_item.get("input", "")
    target_score = test_item.get("target_score", 0.0)
    spa_term = test_item.get("spa_term", "")
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    if use_utils_v2:
        from spa_prompt_utils_v2 import create_enhanced_prompt as create_enhanced_prompt_v2
        create_prompt_func = create_enhanced_prompt_v2
    else:
        create_prompt_func = create_enhanced_prompt
    
    # ëª©í‘œ ì„±í–¥ ê²°ì • (ì ìˆ˜ ê¸°ë°˜)
    if target_score <= 1.0:
        target_orientation = "buyer"  # ë§¤ìˆ˜ì¸ ì¹œí™”
    elif target_score >= 3.0:
        target_orientation = "seller"  # ë§¤ë„ì¸ ì¹œí™”
    else:
        target_orientation = "buyer"  # ê¸°ë³¸ê°’
    
    user_message = f"ìš”ì²­ ì‚¬í•­: ë‹¤ìŒ ì¡°í•­ì„ {target_score}ì  ëª©í‘œë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.\n\nê²€í†  ëŒ€ìƒ: {input_clause}"
    
    selected_options = [spa_term] if spa_term else []
    selected_tasks = ["ë§¤ìˆ˜ì¸ â†” ë§¤ë„ì¸ ì „í™˜"]
    task_params = {
        "style_target_orientation": target_orientation,
        "style_k": 2
    }
    
    enhanced_prompt = create_prompt_func(
        user_message,
        selected_options,
        selected_tasks,
        task_params,
        use_context_v2=use_context_v2
    )
    
    # LLM í˜¸ì¶œ (êµ¬ì¡°í™”ëœ ì¶œë ¥ ì‚¬ìš©)
    # streamlit_app.pyì™€ server_langgraph.pyì˜ ë¡œì§ì„ ë°˜ì˜:
    # - streamlit_app.py: ë°±ì—”ë“œ API â†’ JSON ë¬¸ìì—´ â†’ json.loads() â†’ ë”•ì…”ë„ˆë¦¬
    # - server_langgraph.py: êµ¬ì¡°í™”ëœ LLM â†’ Pydantic ëª¨ë¸ â†’ model_dump() â†’ ë”•ì…”ë„ˆë¦¬
    # - generate_predictions_local.py: OpenAI API â†’ Pydantic ëª¨ë¸ â†’ model_dump() â†’ ë”•ì…”ë„ˆë¦¬
    try:
        # server_langgraph.pyì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ êµ¬ì¡°í™”ëœ ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ê²°ì •
        # streamlit_app.pyì—ì„œëŠ” ë°±ì—”ë“œ APIê°€ ì´ ë¡œì§ì„ ìˆ˜í–‰
        output_schema = determine_output_schema(selected_tasks)
        
        if output_schema:
            # êµ¬ì¡°í™”ëœ ì¶œë ¥ ì‚¬ìš© (server_langgraph.pyì˜ get_structured_llmê³¼ ë™ì¼)
            llm = get_structured_llm(temperature=temperature, output_schema=output_schema)
            
            # server_langgraph.pyì™€ ë™ì¼í•œ ë©”ì‹œì§€ êµ¬ì„± ë°©ì‹
            # streamlit_app.pyì—ì„œëŠ” ë°±ì—”ë“œê°€ ì´ ë©”ì‹œì§€ êµ¬ì„±ì„ ìˆ˜í–‰
            system_message = SystemMessage(content="ë‹¹ì‹ ì€ M&A ì£¼ì‹ë§¤ë§¤ê³„ì•½(SPA) ì „ë¬¸ ë³€í˜¸ì‚¬ì…ë‹ˆë‹¤.")
            user_message = HumanMessage(content=enhanced_prompt)
            messages_to_send = [system_message, user_message]
            
            # ë™ê¸° í˜¸ì¶œ (streamlit_app.pyëŠ” ë°±ì—”ë“œ APIë¥¼ í†µí•´ ë¹„ë™ê¸° í˜¸ì¶œ)
            structured_output = llm.invoke(messages_to_send)
            
            # êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            # streamlit_app.py: structured_data = json.loads(full_text) (ë°±ì—”ë“œì—ì„œ JSON ë¬¸ìì—´ë¡œ ë°›ìŒ)
            # server_langgraph.py: output_dict = structured_output.model_dump() (ë™ì¼)
            # generate_predictions_local.py: ì§ì ‘ Pydantic ëª¨ë¸ì„ ë°›ìœ¼ë¯€ë¡œ model_dump() ì‚¬ìš©
            output_dict = structured_output.model_dump() if hasattr(structured_output, 'model_dump') else dict(structured_output)
            
            # êµ¬ì¡°í™”ëœ ì¶œë ¥ì—ì„œ ì¶”ì¶œ
            # streamlit_app.pyì˜ format_structured_output() í•¨ìˆ˜ì™€ ë™ì¼í•œ í•„ë“œ ì‚¬ìš©:
            # - "converted_sentence": ë³€í™˜ëœ ì¡°í•­
            # - "analysis": ë¶„ì„ í…ìŠ¤íŠ¸
            converted_sentence = output_dict.get("converted_sentence", "")
            response_text = output_dict.get("analysis", "")
            
            # ì ìˆ˜ëŠ” ë¶„ì„ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ ì‹œë„, ì‹¤íŒ¨ ì‹œ ëª©í‘œ ì ìˆ˜ ì‚¬ìš©
            score = extract_score_from_response(response_text)
            if score is None:
                score = target_score
            
            return {
                "converted_sentence": converted_sentence,
                "score": float(score),
                "raw_response": response_text,  # ë””ë²„ê¹…ìš©
                "metadata": {
                    "spa_term": spa_term,
                    "target_score": target_score,
                    "model": OPENAI_MODEL,
                    "temperature": temperature,
                    "structured_output": True,
                    "output_schema": output_schema.__name__ if output_schema else None,
                    "api_type": "openai"
                }
            }
        else:
            # êµ¬ì¡°í™”ëœ ì¶œë ¥ ìŠ¤í‚¤ë§ˆë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ì¼ë°˜ í˜¸ì¶œ
            llm = get_llm(temperature=temperature)
            messages = [
                SystemMessage(content="ë‹¹ì‹ ì€ M&A ì£¼ì‹ë§¤ë§¤ê³„ì•½(SPA) ì „ë¬¸ ë³€í˜¸ì‚¬ì…ë‹ˆë‹¤."),
                HumanMessage(content=enhanced_prompt)
            ]
            
            response = llm.invoke(messages)
            response_text = response.content if hasattr(response, 'content') else str(response)
            
            # ì¼ë°˜ ì‘ë‹µì—ì„œ ì¶”ì¶œ
            converted_sentence = extract_clause_from_response(response_text)
            score = extract_score_from_response(response_text)
            if score is None:
                score = target_score
            
            return {
                "converted_sentence": converted_sentence,
                "score": float(score),
                "raw_response": response_text,
                "metadata": {
                    "spa_term": spa_term,
                    "target_score": target_score,
                    "model": OPENAI_MODEL,
                    "temperature": temperature,
                    "structured_output": False,
                    "api_type": "openai"
                }
            }
    except Exception as e:
        print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ (í•­ëª©: {spa_term}, ì ìˆ˜: {target_score}): {e}")
        return {
            "converted_sentence": f"[ì˜¤ë¥˜: {str(e)}]",
            "score": target_score,
            "raw_response": "",
            "metadata": {
                "error": str(e),
                "spa_term": spa_term,
                "target_score": target_score,
                "api_type": "openai"
            }
        }


def generate_predictions(
    test_data_path: str,
    output_path: str,
    use_utils_v2: bool = True,
    use_context_v2: bool = True,
    temperature: float = 0.0,
    batch_size: int = 1
) -> List[Dict]:
    """
    test_dataì˜ ëª¨ë“  í•­ëª©ì— ëŒ€í•´ ì˜ˆì¸¡ ìƒì„±
    
    Args:
        test_data_path: í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        output_path: ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        use_v2: spa_prompt_utils_v2 ì‚¬ìš© ì—¬ë¶€
        temperature: LLM temperature
        batch_size: ë°°ì¹˜ í¬ê¸° (í˜„ì¬ëŠ” 1ë§Œ ì§€ì›)
    
    Returns:
        ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    print(f"ğŸ“‚ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ: {test_data_path}")
    test_data = load_test_data(test_data_path)
    print(f"   ì´ {len(test_data)}ê°œ í•­ëª©")
    
    predictions = []
    start_time = time.time()
    
    print(f"\nğŸš€ ì˜ˆì¸¡ ìƒì„± ì‹œì‘ (ëª¨ë¸: {OPENAI_MODEL}, API: OpenAI)")
    print(f"   Utils ë²„ì „: {'v2' if use_utils_v2 else 'v1'}")
    print(f"   Context ë²„ì „: {'v2' if use_context_v2 else 'v1'}")
    print(f"   Temperature: {temperature}")
    print("-" * 60)
    
    for i, item in enumerate(test_data, 1):
        spa_term = item.get("spa_term", "N/A")
        target_score = item.get("target_score", 0.0)
        
        print(f"[{i}/{len(test_data)}] {spa_term} - {target_score}ì  ëª©í‘œ...", end=" ", flush=True)
        
        pred = generate_prediction_for_item(item, use_utils_v2=use_utils_v2, use_context_v2=use_context_v2, temperature=temperature)
        predictions.append(pred)
        
        elapsed = time.time() - start_time
        avg_time = elapsed / i
        remaining = avg_time * (len(test_data) - i)
        
        print(f"âœ… ì™„ë£Œ (ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {remaining/60:.1f}ë¶„)")
        
        # ì¤‘ê°„ ì €ì¥ (ë§¤ 5ê°œë§ˆë‹¤)
        if i % 5 == 0:
            temp_output = output_path.replace('.json', f'_temp_{i}.json')
            with open(temp_output, 'w', encoding='utf-8') as f:
                json.dump(predictions, f, ensure_ascii=False, indent=2)
            print(f"   ğŸ’¾ ì¤‘ê°„ ì €ì¥: {temp_output}")
    
    # ìµœì¢… ì €ì¥
    output_data = {
        "version": "1.0",
        "created_at": datetime.now().isoformat(),
        "model": OPENAI_MODEL,
        "api_type": "openai",
        "utils_version": "v2" if use_utils_v2 else "v1",
        "context_version": "v2" if use_context_v2 else "v1",
        "temperature": temperature,
        "total_samples": len(predictions),
        "predictions": predictions
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    total_time = time.time() - start_time
    print("\n" + "=" * 60)
    print(f"âœ… ì˜ˆì¸¡ ìƒì„± ì™„ë£Œ!")
    print(f"   ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
    print(f"   í‰ê·  ì‹œê°„/í•­ëª©: {total_time/len(test_data):.1f}ì´ˆ")
    print(f"   ì €ì¥ ìœ„ì¹˜: {output_path}")
    print("=" * 60)
    
    return predictions


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='OpenAI APIë¥¼ ì‚¬ìš©í•œ Style Transfer ì˜ˆì¸¡ ìƒì„± (ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©)')
    parser.add_argument('--test_data', type=str, required=True,
                       help='í…ŒìŠ¤íŠ¸ ë°ì´í„° JSON íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--output', type=str, default=None,
                       help='ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸ê°’: predictions_local_<timestamp>.json)')
    parser.add_argument('--use_utils_v1', action='store_true',
                       help='spa_prompt_utils.py ì‚¬ìš© (ê¸°ë³¸ê°’: v2)')
    parser.add_argument('--use_context_v1', action='store_true',
                       help='spa_term_context.py ì‚¬ìš© (ê¸°ë³¸ê°’: v2)')
    parser.add_argument('--temperature', type=float, default=0.0,
                       help='LLM temperature (ê¸°ë³¸ê°’: 0.0)')
    parser.add_argument('--model', type=str, default=None,
                       help='OpenAI ëª¨ë¸ëª… (ê¸°ë³¸ê°’: .envì˜ OPENAI_MODEL ë˜ëŠ” gpt-4o)')
    
    args = parser.parse_args()
    
    # ëª¨ë¸ ì„¤ì • (ëª…ë ¹ì¤„ ì¸ìê°€ ìš°ì„ )
    global OPENAI_MODEL
    if args.model:
        OPENAI_MODEL = args.model
    
    # ì¶œë ¥ ê²½ë¡œ ê²°ì •
    if args.output is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        utils_ver = "v1" if args.use_utils_v1 else "v2"
        context_ver = "v1" if args.use_context_v1 else "v2"
        args.output = f"predictions_local_utils{utils_ver}_ctx{context_ver}_{timestamp}.json"
    
    # ë²„ì „ ê²°ì • (ê¸°ë³¸ê°’: ë‘˜ ë‹¤ v2)
    use_utils_v2 = not args.use_utils_v1
    use_context_v2 = not args.use_context_v1
    
    # API í‚¤ í™•ì¸
    if not OPENAI_API_KEY:
        print("âŒ ì˜¤ë¥˜: OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì— ë‹¤ìŒì„ ì¶”ê°€í•˜ì„¸ìš”:")
        print("   OPENAI_API_KEY=your_api_key_here")
        print("   OPENAI_MODEL=gpt-4o-mini  # ì„ íƒì‚¬í•­")
        return
    
    # ì˜ˆì¸¡ ìƒì„±
    try:
        generate_predictions(
            test_data_path=args.test_data,
            output_path=args.output,
            use_utils_v2=use_utils_v2,
            use_context_v2=use_context_v2,
            temperature=args.temperature
        )
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

