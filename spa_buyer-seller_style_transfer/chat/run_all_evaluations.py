#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
4ê°€ì§€ ì¡°í•©ì„ ìë™ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ì¡°í•©:
1. utils.py + context.py
2. utils.py + context_v2.py
3. utils_v2.py + context.py
4. utils_v2.py + context_v2.py
"""

import json
import os
import subprocess
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List

# í˜„ì¬ ë””ë ‰í† ë¦¬
SCRIPT_DIR = Path(__file__).parent


def run_command(cmd: List[str], description: str) -> bool:
    """ëª…ë ¹ì–´ ì‹¤í–‰"""
    print(f"\n{'='*60}")
    print(f"ğŸš€ {description}")
    print(f"{'='*60}")
    print(f"ëª…ë ¹ì–´: {' '.join(cmd)}")
    print("-" * 60)
    
    try:
        result = subprocess.run(cmd, check=True, capture_output=False, text=True)
        print(f"âœ… {description} ì™„ë£Œ")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ {description} ì‹¤íŒ¨: {e}")
        return False


def load_evaluation_result(result_path: str) -> Dict:
    """í‰ê°€ ê²°ê³¼ ë¡œë“œ"""
    try:
        with open(result_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return None
    except json.JSONDecodeError as e:
        print(f"âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜ ({result_path}): {e}")
        return None


def print_comparison_table(results: Dict[str, Dict]):
    """ë¹„êµ í…Œì´ë¸” ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ“Š 4ê°€ì§€ ì¡°í•© ë¹„êµ ê²°ê³¼")
    print("="*80)
    
    # í—¤ë”
    print(f"\n{'ì¡°í•©':<30} {'ROUGE-1':>10} {'ROUGE-2':>10} {'ROUGE-L':>10} {'Accuracy':>10} {'Cat.Acc':>10}")
    print("-" * 80)
    
    # ê° ì¡°í•©ë³„ ê²°ê³¼
    for combo_name, result in results.items():
        if result:
            print(f"{combo_name:<30} "
                  f"{result.get('rouge_1', 0):>10.4f} "
                  f"{result.get('rouge_2', 0):>10.4f} "
                  f"{result.get('rouge_l', 0):>10.4f} "
                  f"{result.get('accuracy', 0):>10.4f} "
                  f"{result.get('category_accuracy', 0):>10.4f}")
        else:
            print(f"{combo_name:<30} {'N/A':>10} {'N/A':>10} {'N/A':>10} {'N/A':>10} {'N/A':>10}")
    
    print("="*80)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='4ê°€ì§€ ì¡°í•©ì„ ìë™ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì¡°í•©:
  1. utils.py + context.py      (ê¸°ë³¸)
  2. utils.py + context_v2.py   (ì»¨í…ìŠ¤íŠ¸ë§Œ ê°œì„ )
  3. utils_v2.py + context.py   (í”„ë¡¬í”„íŠ¸ë§Œ ê°œì„ )
  4. utils_v2.py + context_v2.py (ë‘˜ ë‹¤ ê°œì„ )

ì˜ˆì‹œ:
  python run_all_evaluations.py --test_data test_data/test_data_template\(ì†í•´ë°°ìƒ\)_filled.json
        """
    )
    parser.add_argument('--test_data', type=str, required=True,
                       help='í…ŒìŠ¤íŠ¸ ë°ì´í„° JSON íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--use_local', action='store_true',
                       help='OpenAI API ì‚¬ìš© (ë¡œì»¬ í…ŒìŠ¤íŠ¸, generate_predictions_local.py)')
    parser.add_argument('--skip_generation', action='store_true',
                       help='ì˜ˆì¸¡ ìƒì„± ê±´ë„ˆë›°ê¸° (ì´ë¯¸ ìƒì„±ëœ íŒŒì¼ì´ ìˆëŠ” ê²½ìš°)')
    parser.add_argument('--skip_evaluation', action='store_true',
                       help='í‰ê°€ ê±´ë„ˆë›°ê¸° (ì˜ˆì¸¡ë§Œ ìƒì„±)')
    parser.add_argument('--output_dir', type=str, default='all_evaluation_results',
                       help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: all_evaluation_results)')
    
    args = parser.parse_args()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ í™•ì¸
    test_data_path = Path(args.test_data)
    if not test_data_path.exists():
        print(f"âŒ ì˜¤ë¥˜: í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_data_path}")
        sys.exit(1)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # ì¡°í•© ì •ì˜
    combinations = [
        {
            "name": "utils_v1_context_v1",
            "description": "utils.py + context.py (ê¸°ë³¸)",
            "use_utils_v2": False,
            "use_context_v2": False
        },
        {
            "name": "utils_v1_context_v2",
            "description": "utils.py + context_v2.py (ì»¨í…ìŠ¤íŠ¸ë§Œ ê°œì„ )",
            "use_utils_v2": False,
            "use_context_v2": True
        },
        {
            "name": "utils_v2_context_v1",
            "description": "utils_v2.py + context.py (í”„ë¡¬í”„íŠ¸ë§Œ ê°œì„ )",
            "use_utils_v2": True,
            "use_context_v2": False
        },
        {
            "name": "utils_v2_context_v2",
            "description": "utils_v2.py + context_v2.py (ë‘˜ ë‹¤ ê°œì„ )",
            "use_utils_v2": True,
            "use_context_v2": True
        }
    ]
    
    # ìŠ¤í¬ë¦½íŠ¸ ì„ íƒ
    if args.use_local:
        script_name = "generate_predictions_local.py"
    else:
        script_name = "generate_predictions.py"
    
    script_path = SCRIPT_DIR / script_name
    
    if not script_path.exists():
        print(f"âŒ ì˜¤ë¥˜: ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {script_path}")
        sys.exit(1)
    
    results = {}
    
    # ê° ì¡°í•©ë³„ë¡œ ì‹¤í–‰
    for combo in combinations:
        combo_name = combo["name"]
        predictions_file = output_dir / f"predictions_{combo_name}.json"
        evaluation_dir = output_dir / f"evaluation_{combo_name}"
        evaluation_file = evaluation_dir / "evaluation_results.json"
        
        # 1. ì˜ˆì¸¡ ìƒì„±
        if not args.skip_generation:
            cmd = [
                sys.executable,
                str(script_path),
                "--test_data", str(test_data_path),
                "--output", str(predictions_file)
            ]
            
            if not combo["use_utils_v2"]:
                cmd.append("--use_utils_v1")
            if not combo["use_context_v2"]:
                cmd.append("--use_context_v1")
            
            success = run_command(cmd, f"ì˜ˆì¸¡ ìƒì„±: {combo['description']}")
            if not success:
                print(f"âš ï¸ ì˜ˆì¸¡ ìƒì„± ì‹¤íŒ¨: {combo_name}")
                continue
        else:
            if not predictions_file.exists():
                print(f"âš ï¸ ì˜ˆì¸¡ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {predictions_file}")
                continue
        
        # 2. í‰ê°€ ì‹¤í–‰
        if not args.skip_evaluation:
            cmd = [
                sys.executable,
                str(SCRIPT_DIR / "evaluate.py"),
                "--test_data", str(test_data_path),
                "--predictions", str(predictions_file),
                "--output_dir", str(evaluation_dir)
            ]
            
            success = run_command(cmd, f"í‰ê°€ ì‹¤í–‰: {combo['description']}")
            if not success:
                print(f"âš ï¸ í‰ê°€ ì‹¤íŒ¨: {combo_name}")
                continue
        
        # 3. ê²°ê³¼ ë¡œë“œ
        result = load_evaluation_result(evaluation_file)
        results[combo["description"]] = result
    
    # 4. ë¹„êµ í…Œì´ë¸” ì¶œë ¥
    if not args.skip_evaluation:
        print_comparison_table(results)
        
        # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        comparison_file = output_dir / "comparison_results.json"
        with open(comparison_file, 'w', encoding='utf-8') as f:
            json.dump({
                "generated_at": datetime.now().isoformat(),
                "test_data": str(test_data_path),
                "results": results
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ë¹„êµ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {comparison_file}")
    
    print("\n" + "="*80)
    print("âœ… ëª¨ë“  í‰ê°€ ì™„ë£Œ!")
    print("="*80)


if __name__ == "__main__":
    main()

