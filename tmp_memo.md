aiuser3@ai-smartlaw:~$ ls -la ~/LLM_EvalPipeline_test/.venv/
total 36
drwxrwxr-x 6 aiuser3 aiuser3 4096 Aug 14 10:16 .
drwxrwxr-x 9 aiuser3 aiuser3 4096 Aug 21 08:51 ..
drwxrwxr-x 2 aiuser3 aiuser3 4096 Aug 14 10:16 bin
-rw-rw-r-- 1 aiuser3 aiuser3   43 Aug 13 14:40 CACHEDIR.TAG
-rw-rw-r-- 1 aiuser3 aiuser3    1 Aug 13 14:40 .gitignore
drwxrwxr-x 3 aiuser3 aiuser3 4096 Aug 14 10:16 include
drwxrwxr-x 3 aiuser3 aiuser3 4096 Aug 13 14:40 lib
lrwxrwxrwx 1 aiuser3 aiuser3    3 Aug 13 14:40 lib64 -> lib
-rwxrwxrwx 1 aiuser3 aiuser3    0 Aug 13 14:40 .lock
-rw-rw-r-- 1 aiuser3 aiuser3  154 Aug 13 14:40 pyvenv.cfg
drwxrwxr-x 3 aiuser3 aiuser3 4096 Aug 13 14:41 share
aiuser3@ai-smartlaw:~$ source ~/LLM_EvalPipeline_test/.venv/bin/activate
(LLM_EvalPipeline_test) aiuser3@ai-smartlaw:~$ python --version
Python 3.12.3
(LLM_EvalPipeline_test) aiuser3@ai-smartlaw:~$ pip list | head -10
Package                            Version
---------------------------------- ------------------------
absl-py                            2.3.1
accelerate                         1.10.0
aiohappyeyeballs                   2.6.1
aiohttp                            3.12.15
aiosignal                          1.4.0
annotated-types                    0.7.0
anyio                              4.10.0
astor                              0.8.1
ERROR: Pipe to stdout was broken
Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>
BrokenPipeError: [Errno 32] Broken pipe
(LLM_EvalPipeline_test) aiuser3@ai-smartlaw:~$ python -c "
import sys
print('=== venv í™˜ê²½ íŒ¨í‚¤ì§€ í™•ì¸ ===')
packages = ['torch', 'transformers', 'datasets', 'peft', 'numpy', 'yaml']
for pkg in packages:
    try:
        module = __import__(pkg)
        version = getattr(module, '__version__', 'unknown')
        print(f'âœ… {pkg} {version}')
    except ImportError:
        print(f'âŒ {pkg} ì—†ìŒ')
"
=== venv í™˜ê²½ íŒ¨í‚¤ì§€ í™•ì¸ ===
âœ… torch 2.9.0.dev20250804+cu128
âœ… transformers 4.55.0
âœ… datasets 2.16.0
âœ… peft 0.17.0
âœ… numpy 2.2.6
âœ… yaml 6.0.2
(LLM_EvalPipeline_test) aiuser3@ai-smartlaw:~$ python -c "
import torch
print(f'âœ… PyTorch {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
print(f'GPU count: {torch.cuda.device_count()}')
if torch.cuda.is_available():
    for i in range(torch.cuda.device_count()):
        print(f'GPU {i}: {torch.cuda.get_device_name(i)}')
"
âœ… PyTorch 2.9.0.dev20250804+cu128
CUDA available: True
GPU count: 8
GPU 0: NVIDIA H200
GPU 1: NVIDIA H200
GPU 2: NVIDIA H200
GPU 3: NVIDIA H200
GPU 4: NVIDIA H200
GPU 5: NVIDIA H200
GPU 6: NVIDIA H200
GPU 7: NVIDIA H200
(LLM_EvalPipeline_test) aiuser3@ai-smartlaw:~$ nvidia -smi
WARNING:root:could not open file '/etc/apt/sources.list.d/sdcss.list': [Errno 13] Permission denied: '/etc/apt/sources.list.d/sdcss.list'

nvidia: command not found
(LLM_EvalPipeline_test) aiuser3@ai-smartlaw:~$ nvidia-smi
Mon Sep  1 19:00:39 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    Off |   00000000:0A:00.0 Off |                    0 |
| N/A   31C    P0            120W /  700W |       0MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    Off |   00000000:18:00.0 Off |                    0 |
| N/A   29C    P0            122W /  700W |       0MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H200                    Off |   00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0            113W /  700W |       0MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H200                    Off |   00000000:44:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |       0MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H200                    Off |   00000000:87:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |       0MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H200                    Off |   00000000:90:00.0 Off |                    0 |
| N/A   29C    P0            121W /  700W |       0MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H200                    Off |   00000000:B9:00.0 Off |                    0 |
| N/A   28C    P0            117W /  700W |       0MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H200                    Off |   00000000:C2:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |       0MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
(LLM_EvalPipeline_test) aiuser3@ai-smartlaw:~$ cd ETU
(LLM_EvalPipeline_test) aiuser3@ai-smartlaw:~/ETU$ python benchmark_8gpu.py
=== ETU 8ëŒ€ H200 GPU ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ===
ğŸš€ 8ëŒ€ H200 GPU ë²¤ì¹˜ë§ˆí¬ ì‹œì‘
============================================================
GPU 0: NVIDIA H200 (139.8 GB)
GPU 1: NVIDIA H200 (139.8 GB)
GPU 2: NVIDIA H200 (139.8 GB)
GPU 3: NVIDIA H200 (139.8 GB)
GPU 4: NVIDIA H200 (139.8 GB)
GPU 5: NVIDIA H200 (139.8 GB)
GPU 6: NVIDIA H200 (139.8 GB)
GPU 7: NVIDIA H200 (139.8 GB)
ğŸ’¾ ì´ GPU ë©”ëª¨ë¦¬: 1118.5 GB
============================================================
ğŸš€ ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...

ğŸ“Š ë©”ëª¨ë¦¬ ëŒ€ì—­í­ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...
GPU 0 ë©”ëª¨ë¦¬ ëŒ€ì—­í­: 1999.65 GB/s
GPU 1 ë©”ëª¨ë¦¬ ëŒ€ì—­í­: 2532.67 GB/s
GPU 2 ë©”ëª¨ë¦¬ ëŒ€ì—­í­: 2533.27 GB/s
GPU 3 ë©”ëª¨ë¦¬ ëŒ€ì—­í­: 2532.78 GB/s
GPU 4 ë©”ëª¨ë¦¬ ëŒ€ì—­í­: 2533.37 GB/s
GPU 5 ë©”ëª¨ë¦¬ ëŒ€ì—­í­: 2533.49 GB/s
GPU 6 ë©”ëª¨ë¦¬ ëŒ€ì—­í­: 2536.84 GB/s
GPU 7 ë©”ëª¨ë¦¬ ëŒ€ì—­í­: 2534.68 GB/s

ğŸ”¢ ì—°ì‚° ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...
GPU 0 ì—°ì‚° ì„±ëŠ¥: 51206.64 GFLOPS
GPU 1 ì—°ì‚° ì„±ëŠ¥: 51177.54 GFLOPS
GPU 2 ì—°ì‚° ì„±ëŠ¥: 51184.68 GFLOPS
GPU 3 ì—°ì‚° ì„±ëŠ¥: 51187.45 GFLOPS
GPU 4 ì—°ì‚° ì„±ëŠ¥: 51180.68 GFLOPS
GPU 5 ì—°ì‚° ì„±ëŠ¥: 51184.31 GFLOPS
GPU 6 ì—°ì‚° ì„±ëŠ¥: 51206.73 GFLOPS
GPU 7 ì—°ì‚° ì„±ëŠ¥: 50964.30 GFLOPS

ğŸ”„ ë©€í‹° GPU ìŠ¤ì¼€ì¼ë§ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...
  1ëŒ€ GPU í…ŒìŠ¤íŠ¸...
    1ëŒ€ GPU íš¨ìœ¨ì„±: 281.517
  2ëŒ€ GPU í…ŒìŠ¤íŠ¸...
    2ëŒ€ GPU íš¨ìœ¨ì„±: 12.207
  3ëŒ€ GPU í…ŒìŠ¤íŠ¸...
    3ëŒ€ GPU íš¨ìœ¨ì„±: 6.429
  4ëŒ€ GPU í…ŒìŠ¤íŠ¸...
    4ëŒ€ GPU íš¨ìœ¨ì„±: 3.832
  5ëŒ€ GPU í…ŒìŠ¤íŠ¸...
    5ëŒ€ GPU íš¨ìœ¨ì„±: 2.621
  6ëŒ€ GPU í…ŒìŠ¤íŠ¸...
    6ëŒ€ GPU íš¨ìœ¨ì„±: 1.967
  7ëŒ€ GPU í…ŒìŠ¤íŠ¸...
    7ëŒ€ GPU íš¨ìœ¨ì„±: 1.502
  8ëŒ€ GPU í…ŒìŠ¤íŠ¸...
    8ëŒ€ GPU íš¨ìœ¨ì„±: 1.192

ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...
GPU 0 ìµœëŒ€ ë©”ëª¨ë¦¬: 18.39 GB
GPU 1 ìµœëŒ€ ë©”ëª¨ë¦¬: 18.39 GB
GPU 2 ìµœëŒ€ ë©”ëª¨ë¦¬: 18.39 GB
GPU 3 ìµœëŒ€ ë©”ëª¨ë¦¬: 18.39 GB
GPU 4 ìµœëŒ€ ë©”ëª¨ë¦¬: 18.39 GB
GPU 5 ìµœëŒ€ ë©”ëª¨ë¦¬: 18.39 GB
GPU 6 ìµœëŒ€ ë©”ëª¨ë¦¬: 18.39 GB
GPU 7 ìµœëŒ€ ë©”ëª¨ë¦¬: 18.39 GB

============================================================
ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½
============================================================
í‰ê·  ë©”ëª¨ë¦¬ ëŒ€ì—­í­: 2467.09 GB/s
í‰ê·  ì—°ì‚° ì„±ëŠ¥: 51161.54 GFLOPS
í‰ê·  ë©€í‹° GPU íš¨ìœ¨ì„±: 38.908
============================================================
ğŸ“ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥ë¨: h200_benchmark_20250901_190127.json
(LLM_EvalPipeline_test) aiuser3@ai-smartlaw:~/ETU$ ^C
